{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN's and other cool neural networks stuff\n",
    "```During this exercise you will train, examine and visualize convolutional neural networks. Images are a very unique example of high dimensional data. There are, indeed, many dimensions - even for low-resolution images (let's say, 32x32 pixels), while regarding each pixel as a feature, you can end up with hundreds of features. Such large vectors are, generally speaking, hard to visualize, but images make it all much easier, you can just draw the image. We will use this nice property to explore our networks. ```\n",
    "\n",
    "~```Ittai Haran```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Input, Conv2D, Flatten, MaxPool2D, Dropout, Softmax\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```As you recall, you used before the MNIST dataset. Now we will use a much harder dataset - CIFAR10, which contains low resolution images of 10 different objects - airplace, automobile, bird, cat, deer, dog, frog, horse, ship and truck. Start by loading the dataset.\n",
    "Notice: the values given to pixels are in the range [0,255]. You might want to move them to the range [0,1] for later use in your neural networks.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "num_to_words = {0:'airplane',1:'automobile',2:'bird',3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Draw a random picture using plt.imshow function.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Create a fully connected neural network that predicts the right class. after compiling your neural network (model.compile) you can use model.summary to print the architecture of your network. Use it, and make sure you have less than 2,000,000 parameters. Use any activations you'ld like.\n",
    "Notice: Youl'd probably like to use the softmax activation on your last layer. Your network will have to get flattened vectors (or get matrices and flatten them on her own, using the Flatten layer) - what shape should your input have?\n",
    "Use the adam optimizer. What should be the loss in this classification problem?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train your network for 40 epochs (or 10 minutes, whichever comes faster) and get train accuracy of more than 0.6. What is the accuracy computed on the test? Would you say you are in overfit situation? explain your answer.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will now attack the same problem using a simple CNN. Build a CNN model as follows:```\n",
    "- ```Input layer```\n",
    "- ```Convolutional layer of 32 filters of size (3,3) with relu activation```\n",
    "- ```Convolutional layer of 32 filters of size (3,3) with relu activation```\n",
    "- ```Max pooling layer```\n",
    "- ```Dropout layer of p=0.25```\n",
    "- ```Convolutional layer of 64 filters of size (3,3) with relu activation```\n",
    "- ```Convolutional layer of 64 filters of size (3,3) with relu activation```\n",
    "- ```Max pooling layer```\n",
    "- ```Dropout layer of p=0.25```\n",
    "- ```Flattning layer (Flatten in keras)```\n",
    "- ```Fully connected layer of 200 hidden units```\n",
    "- ```Dropout layer of p=0.5```\n",
    "- ```Output layer```\n",
    "\n",
    "```Use the same loss as with the fully connected model and the same optimizer. How many weights are there in your network? Train it for 40 epochs. What train loss did you get? What test loss did you get? How is it similar to the fully connected case?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will now experience with visualization and interpretation of the network. Given a single image, we would like to know what parts of the image contribute most to the prediction of the network. In order to do, each time we will black out part of the image, to get all the possible images with blacked out part:```\n",
    "\n",
    "![title](resources/image_for_notebook.png)\n",
    "\n",
    "```In total you will have, for a 32x32 image, 1,024 blacked images. Write a function that, given an image (32x32x3 matrix) and a parameter``` $a$ ```, creates a new tensor (of shape 1024x32x32x3) so that tensor[num] is the original image with a blacked out square of edge size``` $a$ ```, concentrated around the (num//32,num%32)-pixel. We will call the output tensor the \"variation tensor\".```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use``` $a=4$ ```and choose a random image from the test segment that got labeled correctly using your model. Create, using the function you recently wrote, the variation tensor. Create the model's predictions for every blacked image in the tensor (using model.predict(tensor)).\n",
    "You will get a matrix of shape 1024x10. Take only the column that matches the image's label, so you get a 1024-dimensions vector. Reshape it to be a 32x32 image. Now every pixel tells the probability of the model to label the image correctly, while there is a blacked out square concentrated around that pixel. Show the original image and the image you got. Normalize the scale of the image you got to the range [0,1] using plt.imshow(..., vmin=0, vmax=1).\n",
    "Take your time to examine the procedure you just created on different images.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```You can get a concept of \"network focus\" using the visualization you created.\n",
    "Given the heatmap you created, find the 80 pixels with smallest values. That means, the pixels that, when removing a square around them, we get the maximum damage to the network prediction. Mark these pixels in the original image (actually, in a copy of the original image - you don't want to destroy your data) by adding 0.5 to the red component of the rgb.\n",
    "Show the original image, the heatmap image and the marked image.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We've talked in class about transfer learning. Here you will experience with a simple task of transfer learning. We will take a VGGFace model, trained to predict faces among 2,622 classes, and use it to discriminate between the faces of following celebrities: Alicia Vikander, Amy Adams and Andy Serkis. We will do it by cutting out the last layers and replacing them with layers of our own.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface import VGGFace, utils # an extra directory supplied with the exercise (https://drive.google.com/open?id=1ax6V-FVfXKci3ZdybWHK8AK11A2yjH5m)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#This next import will help you with augmentation - generating augmented photos from your originals.\n",
    "#Read about this general teqnique, and also about ImageDataGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Start by loading the data in images.pkl and in images_labels.pkl. Show an image of Alicia, Amy and Andy, and make sure you can tell the difference ;)```\n",
    "\n",
    "(can be found in: https://drive.google.com/open?id=1go4U-nI4H5kBz6hstI4_PYtzS35jHEDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```In transfer learning we take a trained model, cut off its end and replace it with some layers of our own. Hence we will have to preprocess our data the same way it was preprocessed when training VGGFace. Use the following line to do so:```\n",
    "```python\n",
    "X_processed = utils.preprocess_input(X.copy().astype(float), version=1)\n",
    "```\n",
    "```Transform the values of Y to be [1 0] or [0 1]. Split your data to 80% train data and 20% test data.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Let's look at the VGGFace model we are about to use. Load it:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGGFace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The model is fitted to predict the name of the celebrity in the pictures it gets. Read the example picture given to you in face_example.pkl. Show it. Transform it into a tensor of shape (1, 224, 224, 3) and use utils.preprocess_input as seen before to get it ready to enter to your network. Use```\n",
    "```python\n",
    "utils.decode_predictions(model.predict(x))\n",
    "```\n",
    "```To get 5 most probable classes. Who is the man in the picture?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Let's return to our dataset. As you could impress, it's a pretty small one. Hence we can try to make it bigger using augmentations of the data. We will do it here using a mechanism supplied by keras. Create an instance of keras.preprocessing.image.ImageDataGenerator, which will define how you will create augmentations of each original image you've created. Choose all the parameters on your own (consider, for example, rotation_range, zoom_range, width_shift_range, horizontal_flip and so on).```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Follow the following instructions:```\n",
    "- ```Examine the architecture of the VGGFace model using .summary(). Understand what it means to replace the last 2 dense layers (including the final softmax layer). How many weights are there in the model?```\n",
    "- ```Use .get_layer() to retrieve the last layer which you want to keep in your new network```\n",
    "- ```Create 2 new Dense layers which continue the previous pretrained layers (the last layer should have a softmax activation, for the first one try tanh)```\n",
    "- ```Create a new model with the input of the original model as input (vgg_model.input), and which outputs the new dense-softmax layer```\n",
    "- ```Freeze all of the layers except the last 2 using .layers on the new model, and .trainable = False. This will stop you from training those layers```\n",
    "- ```Compile the model with sgd optimizer and with metric=['accuracy'] (what does it do?)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now you're ready to train the model:```\n",
    "- ```Use .fit_generator() and not .fit(), since you'll be using the augmentor you created```\n",
    "- ```Use .flow() on the instance of ImageDataGenerator as the first input```\n",
    "- ```Choose a combination of batch_size(within .flow) and steps_per_epoch which will create a total number of images that you want per each epoch. You will have to use small batches (~20) so you won't get memory error```\n",
    "- ```Use the test segment to validate your results - you can compute your score on the validation if you add validation_data=(X_test, Y_test) in your .fit_generator function```\n",
    "\n",
    "```Could you transfer the VGGFace model to your new task?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
