{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction exercise\n",
    "```This exercise is purely about features extraction. We will learn how to do it quick and efficiently.\n",
    "We will be working on a kaggle dataset of quora questions, where each record is composed of a pair of questions, while the target is to determine whether the questions have the same meaning. (the label \"is_duplicate\")\n",
    "We will extract features for each question and for each pair of questions and will train a simple model (default xgboost) using those features.```\n",
    "\n",
    "```The purpose of this exercise is to acquire good practices, so please read the instructions carefully and do as it says. You are also encouraged to look at the solution when after you are finished. In addition, when solving the exercise, try to write\n",
    "as efficient and as clean code as you can.```\n",
    "\n",
    "```Note: We are about to do some kaggle cheats, that is, we will compute features by mixing the train and the test.\n",
    "Please notice exactly where we did so. In addition, every time you meet a question in the instructions (you can identify a question by '?'), please answer it in a comment block. ```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some modules you might find useful\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenizer for future use\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "twt_tokenizer = TweetTokenizer()\n",
    "\n",
    "# word2vec model for future use\n",
    "\n",
    "## can be found in: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('resources/word_2_vec/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be found in: # can be found in: https://drive.google.com/open?id=1G6rXwTw0bOBbkSaw96bW76BFnxM1x7H5\n",
    "data = pd.read_csv('resources/data/train.csv')\n",
    "data = data.iloc[:2000]\n",
    "data = data[data.apply(lambda x: not (type(x['question1']) == float or type(x['question2']) == float), axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```First we would like to extract features regarding a single question. In order to do so, first create a dataset containing  all the questions (and their id. why should we remember the id?), without duplicates. Name it 'questions'.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions (4000, 3)\n"
     ]
    }
   ],
   "source": [
    "q1_dataset = data[['id', 'qid1', 'question1']].rename(columns={'qid1': \"qid\", \"question1\": \"question\"}, inplace=False)\n",
    "q2_dataset = data[['id', 'qid2', 'question2']].rename(columns={'qid2': \"qid\", \"question2\": \"question\"}, inplace=False)\n",
    "\n",
    "questions = pd.concat([q1_dataset, q2_dataset])\n",
    "questions = questions.drop_duplicates(keep=\"first\")\n",
    "\n",
    "print(\"questions\", questions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Add a column containing the questions, tokenized using twt_tokenizer, the TweetTokenizer object we created earlier. Name it 'question_sep'. Make sure that you treat the questions in lower case.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>question_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>[what, is, the, step, by, step, guide, to, inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>[what, is, the, story, of, kohinoor, (, koh-i-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>[how, can, i, increase, the, speed, of, my, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>[why, am, i, mentally, very, lonely, ?, how, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>[which, one, dissolve, in, water, quikly, suga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid                                           question  \\\n",
       "0   0    1  What is the step by step guide to invest in sh...   \n",
       "1   1    3  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2    5  How can I increase the speed of my internet co...   \n",
       "3   3    7  Why am I mentally very lonely? How can I solve...   \n",
       "4   4    9  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                       question_step  \n",
       "0  [what, is, the, step, by, step, guide, to, inv...  \n",
       "1  [what, is, the, story, of, kohinoor, (, koh-i-...  \n",
       "2  [how, can, i, increase, the, speed, of, my, in...  \n",
       "3  [why, am, i, mentally, very, lonely, ?, how, c...  \n",
       "4  [which, one, dissolve, in, water, quikly, suga...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions['question_step'] = questions['question'].apply(lambda x: twt_tokenizer.tokenize(x.lower()))\n",
    "\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Create an empty list called 'question_features_for_future_use'. We are going to befoul the questions dataframe, so we will want to remember which of its columns are important to us and which are just columns helping us to create other columns.```\n",
    "```Next, I will ask you to create some features. Whenever I use this sign: (*), know that you have to add the feature name to the list.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_features_for_future_use = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Before we start computing features, write a function that gets a column name and saves a csv file with 2 columns: qid and the column chosen. Name it 'save_feature' and make sure you use it after every feature computed, since it might be very very important for later parts of the exercise and your life.```\n",
    "\n",
    "```Save the features in the resources/features/<col_name>.csv .```\n",
    "\n",
    "```use os.path and os.getcwd().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature(df, column_name):\n",
    "    df_to_save = df[['qid', column_name]]\n",
    "    df_to_save.to_csv('resources/features/' + column_name + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Compute the following:```\n",
    "- ```Counter of the word part of speech (use collections.Counter and pos_tag, which we imported earlier. do it using one line). (*)```\n",
    "- ```number of different numbers appearing in the question\n",
    "(numbers, not digits. use regex. don't count words like 'one') (one line). (*)```\n",
    "- ```number of words in a question (one line). (*)```\n",
    "- ```length of longest word (one line). (*)```\n",
    "- ```word2vec mean of the question. (*)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RONENAH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-e766176a499a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_specific\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_specific\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# questions['number_count'] = questions['question'].apply(lambda x: len(re.findall('(\\d+)', x)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# questions['number_of_words'] = questions['question_step'].apply(len)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# questions['pos_tag_count'] = questions['question_step'].apply(lambda x_1: Counter(map(lambda x_2: x_2[1], pos_tag(tuple(x_1)))))\n",
    "\n",
    "\n",
    "questions['number_count'] = questions['question'].apply(lambda x: sum(c.isdigit() for c in s))\n",
    "# questions['number_of_words'] = questions['question_step'].apply(len)\n",
    "# questions['length_of_longest_word'] = questions['question_step'].apply(lambda x: max(map(len, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```Counter of the question_words (one line). (*)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_words = ['why', 'how', 'where', 'who', 'what', 'which', 'when', 'wheather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```We will now use tf-idf grade (if you aren't familiar with the concept, read about it;) ``` https://en.wikipedia.org/wiki/Tf%E2%80%93idf ```).\n",
    "do the following:```\n",
    "- ```initialize a TfidfVectorizer object. use norm = None, use English stop words and twt_tokenizer we used before. Name it tfidf.```\n",
    "- ```create the tf-idf matrix of all the questions (look again at the note in the beginning of the exercise).```\n",
    "- ```look at tfidf.vocabulary_.```\n",
    "- ```create a reversed vocabulary (given an index returns a word. do it in one line, using list comprehension).```\n",
    "- ```create a column, such that every question has a list of its words and the word's tf-idf grades. do it without transferring the tf-idf matrix into a dense matrix (keep it sparse).```\n",
    "- ```for each question, find the third biggest tf-idf grade. take the list of the words with bigger grades the the third biggest tf-idf grade, and create a column with the \"mean word2vec vector\" of these words. (*)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```We now move to features concerning both questions, and not just one of them. But first, run the following cell, known as the evil cell.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(''.join(map(lambda x: chr(ord(x)-1), 'jnqpsu!nbuqmpumjc/qzqmpu!bt!qmu\\x0bgspn!nbuqmpumjc/jnbhf!jnqpsu!jnsfbe\\x0bjnb'+\\\n",
    "                      'hf!>!jnsfbe)#sftpvsdft0wjtvbmj{bujpo!ifmqfst0T'+\\\n",
    "                      'njmjoh`Efwjm`Fnpkj/qoh#*\\x0bqmu/jntipx)jnbhf*\\x0bqmu/tipx)*')) +'\\n' + ''.join(map(lambda x: chr(ord(x)-1), 'fyju)*'))+'\\n'\\\n",
    "     +''.join(map(lambda x: chr(ord(x)-1), 'qsjou)#Ibibibib!J!fyjufe!zpvs!lfsofm/!Dpoujovf!gspn!ifsf'+\\\n",
    "                  '!xjuipvu!sfsvoojoh!uif!qsfwjpvt!dfmmt!)cftjeft!uif!jnqpsu!dfmmt-!boe!mpbe!uif!ebub!bhbjo*#*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Understand how the evil cell works.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now we will add the features we computed earlier to the data dataframe. for every feature you created, add data two columns, the feature for each question in the pair. Use DataFrame.merge and the qid columns you saved every time you saved a feature. Use also os.listdir and DataFrame.rename, and do it in 7 lines of code at top.\n",
    "Use the following converter (in the pd.read_csv syntax): converters = {feature_name:lambda x: eval(x)}. Why is it needed? Hint: open pos_tag_count.csv. If you aren't familiar with the amazing eval function, read about it:)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Now we would like to find a way to take a feature for each question separately and make it one. Remember our question features are of 3 kinds:```\n",
    "- ```number```\n",
    "- ```Counter```\n",
    "- ```vector```\n",
    "\n",
    "```For each kind we will write a method taking both features and producing one feature:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_two_features_to_1_number(number_1, number_2):\n",
    "    # the function returns |number_1-number_2|\n",
    "    pass\n",
    "\n",
    "def from_two_features_to_1_vector(vector_1, vector_2):\n",
    "    # the function returns the cosine similarity between the vectors\n",
    "    pass\n",
    "\n",
    "def from_two_features_to_1_counter(counter_1, counter_2):\n",
    "    # the Counter objects represent distribution. return the sim of all absolute values of differences between them.\n",
    "    # Remember- Counter have some great properties.\n",
    "    pass\n",
    "\n",
    "def from_two_features_to_1(feature_1, feature_2):\n",
    "    # return the right one feature based on the feature's types\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I suspect you know what that's for:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features_for_future_use = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use the methods you wrote to get one feature from every pair of features you have, while running over the features in question_features_for_future_use. give it meaningful names.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Add the following features:```\n",
    "- ```number of common words between the two questions. (one line) (*)```\n",
    "- ```number of common words between the two questions, not including stop words. (one line) (*)```\n",
    "\n",
    "```You might have to use twt_tokenizer again. note that we could save the tikenized questions.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:21:47.684219Z",
     "start_time": "2019-09-05T14:21:47.670220Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now think of a feature of your own and implement it.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I'm not going to use the evil cell again, but I'll remind you to save your features.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```That's it! take your features and train a RandomForestRegressor using them. Don't forget to split to train and test sections. What score did you get?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
