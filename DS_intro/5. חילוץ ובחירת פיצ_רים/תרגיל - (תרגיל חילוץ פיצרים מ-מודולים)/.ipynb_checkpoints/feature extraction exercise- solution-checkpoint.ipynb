{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction exercise\n",
    "```This exercise is purely about features extraction. We will learn how to do it quick and efficiently.\n",
    "We will be working on a kaggle dataset of quora questions, where each record is composed of a pair of questions, while the target is to determine whether the questions have the same meaning.\n",
    "We will extract features for each question and for each pair of questions and will train a simple model (default xgboost) using those features.```\n",
    "\n",
    "```The purpose of this exercise is to acquire good practices, so please read the instructions carefully and do as it says. You are also encouraged to look at the solution when after you are finished. In addition, when solving the exercise, try to write\n",
    "as efficient and as clean code as you can.```\n",
    "\n",
    "```Note: We are about to do some kaggle cheats, that is, we will compute features by mixing the train and the test.\n",
    "Please notice exactly where we did so. In addition, every time you meet a question in the instructions (you can identify a question by '?'), please answer it in a comment block. ```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some modules you might find useful\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenizer for future use\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "twt_tokenizer = TweetTokenizer()\n",
    "\n",
    "# word2vec model for future use\n",
    "\n",
    "## can be found in: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('resources/word_2_vec/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The data for the excercise can be found in: ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be found in: https://drive.google.com/open?id=1G6rXwTw0bOBbkSaw96bW76BFnxM1x7H5\n",
    "data = pd.read_csv('resources/data/train.csv')\n",
    "data.head()\n",
    "data = data.iloc[:2000]\n",
    "data = data[data.apply(lambda x: not (type(x['question1']) == float or type(x['question2']) == float), axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```First we would like to extract features regarding a single question. In order to do so, first create a dataset containing  all the questions (and their id. why should we remember the id?), without duplicates. Name it 'questions'.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>3969</td>\n",
       "      <td>3970</td>\n",
       "      <td>I am visiting Sri Lanka soonfor 9 days, how ca...</td>\n",
       "      <td>Do Indians hate Sri Lankans?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>3971</td>\n",
       "      <td>3972</td>\n",
       "      <td>What are some good examples of 4 stanza poems?</td>\n",
       "      <td>What are some good Ilocano poems?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>3973</td>\n",
       "      <td>3974</td>\n",
       "      <td>Which CPU is better I3 4th Gen or 6th Gen?</td>\n",
       "      <td>Which is better intel i5 (6th gen) or i7 (5th ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>3975</td>\n",
       "      <td>3976</td>\n",
       "      <td>What are some of the best tourist places to vi...</td>\n",
       "      <td>Where are the foremost tourist places in Chhat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>1999</td>\n",
       "      <td>3977</td>\n",
       "      <td>3978</td>\n",
       "      <td>What are the differences between a love marria...</td>\n",
       "      <td>Which is better: an arranged marriage or a lov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  qid1  qid2                                          question1  \\\n",
       "0        0     1     2  What is the step by step guide to invest in sh...   \n",
       "1        1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2        2     5     6  How can I increase the speed of my internet co...   \n",
       "3        3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4        4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "...    ...   ...   ...                                                ...   \n",
       "1995  1995  3969  3970  I am visiting Sri Lanka soonfor 9 days, how ca...   \n",
       "1996  1996  3971  3972     What are some good examples of 4 stanza poems?   \n",
       "1997  1997  3973  3974         Which CPU is better I3 4th Gen or 6th Gen?   \n",
       "1998  1998  3975  3976  What are some of the best tourist places to vi...   \n",
       "1999  1999  3977  3978  What are the differences between a love marria...   \n",
       "\n",
       "                                              question2  is_duplicate  \n",
       "0     What is the step by step guide to invest in sh...             0  \n",
       "1     What would happen if the Indian government sto...             0  \n",
       "2     How can Internet speed be increased by hacking...             0  \n",
       "3     Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4               Which fish would survive in salt water?             0  \n",
       "...                                                 ...           ...  \n",
       "1995                       Do Indians hate Sri Lankans?             0  \n",
       "1996                  What are some good Ilocano poems?             0  \n",
       "1997  Which is better intel i5 (6th gen) or i7 (5th ...             0  \n",
       "1998  Where are the foremost tourist places in Chhat...             1  \n",
       "1999  Which is better: an arranged marriage or a lov...             0  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid                                           question\n",
       "0   0    1  What is the step by step guide to invest in sh...\n",
       "1   1    3  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2   2    5  How can I increase the speed of my internet co...\n",
       "3   3    7  Why am I mentally very lonely? How can I solve...\n",
       "4   4    9  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = pd.concat( [data[['id', 'qid1', 'question1']].rename(columns={'qid1':'qid', 'question1':'question'}),\n",
    "                        data[['id', 'qid2', 'question2']].rename(columns={'qid2':'qid', 'question2':'question'})])\n",
    "questions = questions.drop_duplicates()\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Add a column containing the questions, tokenized using twt_tokenizer, the TweetTokenizer object we created earlier. Name it 'question_sep'. Make sure that you treat the questions in lower case.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['question_sep'] = questions['question'].apply(lambda x: tuple(twt_tokenizer.tokenize(x.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Create an empty list called 'question_features_for_future_use'. We are going to befoul the questions dataframe, so we will want to remember which of its columns are important to us and which are just columns helping us to create other columns.```\n",
    "```Next, I will ask you to create some features. Whenever I use this sign: (*), know that you have to add the feature name to the list.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_features_for_future_use = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Before we start computing features, write a function that gets a column name and saves a csv file with 2 columns: qid and the column chosen. Name it 'save_feature' and make sure you use it after every feature computed, since it might be very very important for later parts of the exercise and your life.```\n",
    "\n",
    "```Save the features in the resources/features/<col_name>.csv .```\n",
    "\n",
    "```use os.path and os.getcwd().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature(col_name):\n",
    "    if not os.path.exists(os.getcwd()+'/resources/features'):\n",
    "        os.mkdir(os.getcwd()+'/resources/features')\n",
    "    df_to_save = questions[['qid', col_name]]\n",
    "    df_to_save.to_csv('resources/features/'+col_name+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Compute the following:```\n",
    "- ```Counter of the word part of speech (use collections.Counter and pos_tag, which we imported earlier. do it using one line). (*)```\n",
    "- ```number of different numbers appearing in the question\n",
    "(numbers, not digits. use regex. don't count words like 'one') (one line). (*)```\n",
    "- ```number of words in a question (one line). (*)```\n",
    "- ```length of longest word (one line). (*)```\n",
    "- ```word2vec mean of the question. (*)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['pos_tag_count'] = questions['question_sep'].apply(lambda x: Counter(map(lambda y: y[1], pos_tag(x))))\n",
    "questions['number_count'] = questions['question'].apply(lambda x: len(re.findall('(\\d+)', x)))\n",
    "questions['number_of_words'] = questions['question_sep'].apply(len)\n",
    "questions['length_of_longest_word'] = questions['question_sep'].apply(lambda x: max(map(len, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_word_2_vec(lister):\n",
    "    lister = list(filter(lambda x: x in word2vec.vocab, lister))\n",
    "    if len(lister) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.mean(np.array(list(map(lambda x: word2vec[x], lister))), axis = 0)\n",
    "\n",
    "questions['word_2_vec_mean'] = questions['question_sep'].apply(get_mean_word_2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```Counter of the question_words (one line). (*)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_words = ['why', 'how', 'where', 'who', 'what', 'which', 'when', 'wheather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['questions_words'] = questions['question_sep'].apply(lambda x: Counter(filter(lambda y: y in question_words, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will now use tf-idf grade (if you aren't familiar with the concept, read about it;) ``` https://en.wikipedia.org/wiki/Tf%E2%80%93idf ```).\n",
    "do the following:```\n",
    "- ```initialize a TfidfVectorizer object. use norm = None, use english stop words and twt_tokenizer we used before. Name it tfidf.```\n",
    "- ```create the tf-idf matrix of all the questions (look again at the note in the beggining of the exercise).```\n",
    "- ```look at tfidf.vocabulary_.```\n",
    "- ```create a reversed vocabulary (given an index returns a word. do it in one line, using list comprehension).```\n",
    "- ```create a column, such that every question has a list of its words and the word's tf-idf grades. do it without transfering the tf-idf matrix into a dense matrix (keep it sparse).```\n",
    "- ```for each question, find the third biggest tf-idf grade. create a column such that every question have a list of the words with bigger grades the the third biggest tf-idf grade. (*)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will do it again, this time more general, by completing the following functions.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_words_n(x, n = 3):\n",
    "    # the function gets a list of tuples (word, tf-idf grade) and n\n",
    "    # it returns the words which have grades bigger than the n'th biggest grade\n",
    "    pass\n",
    "\n",
    "def get_most_important_words_total(x, n = 3):\n",
    "    # the function gets a list of tuples (word, tf-idf grade) and n\n",
    "    # it returns the n words that have the biggest grades\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_words_n(x, n = 3):\n",
    "    sorted_x = sorted(x, key = lambda y: -y[1])\n",
    "    threshold = sorted_x[min(len(sorted_x)-1, 2)][1]\n",
    "    sorted_x = filter(lambda y: y[1]>=threshold, sorted_x)\n",
    "    return list(map(lambda y: y[0], sorted_x))\n",
    "\n",
    "def get_most_important_words_total(x, n = 3):\n",
    "    sorted_x = sorted(x, key = lambda y: -y[1])[:n]\n",
    "    return list(map(lambda y: y[0], sorted_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_features(n_gram, most_important_words_method):\n",
    "    # the function gets the size of n for the n_grams in the tf-idf computation and\n",
    "    # the result method, which will be one of the two functions above\n",
    "    # it adds the tf-idf features you computed before\n",
    "    # when n_gram = 1, make the function add another feature: the mean word2vec vector of the most important words\n",
    "    # features created will be added to question_features_for_future_use (!)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_features(n_gram, most_important_words_method):\n",
    "    tfidf = TfidfVectorizer(input = 'content', stop_words='english', norm=None, tokenizer=twt_tokenizer.tokenize,\n",
    "                           ngram_range = (n_gram,n_gram))\n",
    "    tfidf_table = tfidf.fit_transform(list(questions['question']))\n",
    "    tfidf_vocab_reverse = {v:k for k,v in tfidf.vocabulary_.items()}\n",
    "\n",
    "    #tfidf_words = map(lambda row: list(zip(map(lambda y: tfidf_vocab_reverse[y], row.nonzero()[1]), np.array(row[row.nonzero()]).reshape(-1))), tfidf_table)\n",
    "    tfidf_words = [[(tfidf_vocab_reverse[j], i[0,j]) for j in i.nonzero()[1]] for i in tfidf_table]\n",
    "    #questions['tfidf_words_againts_all_'+str(n_gram)] = tfidf_words\n",
    "    #questions['most_important_words_3_'+str(n_gram)] = questions['tfidf_words_againts_all_'+str(n_gram)].\\\n",
    "    #                                                    apply(lambda x: most_important_words_method(x))\n",
    "    most_important_words = [most_important_words_method(x) for x in tfidf_words]\n",
    "    \n",
    "    if n_gram == 1:\n",
    "        #questions['most_important_words_3_word2vec_'+str(n_gram)] = questions['most_important_words_3_'+str(n_gram)]\\\n",
    "        #.apply(get_mean_word_2_vec)\n",
    "        questions['most_important_words_3_word2vec_'+str(n_gram)] = [get_mean_word_2_vec(x) for x in most_important_words]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(input = 'content', stop_words='english', norm=None, tokenizer=twt_tokenizer.tokenize,ngram_range = (1,1))\n",
    "# tfidf_table = tfidf.fit_transform(list(questions['question']))\n",
    "# inv_vocab = {v:k for k,v in tfidf.vocabulary_.items()}\n",
    "# tfidf_words = [[(inv_vocab[j], i[0,j]) for j in i.nonzero()[1]] for i in tfidf_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tfidf_features(1,get_most_important_words_n)\n",
    "#get_tfidf_features(1,partial(get_most_important_words_total, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions.sort_values('qid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm=None, preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x000001C8C3737978>>,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer(input = 'content', stop_words='english', norm=None, tokenizer=twt_tokenizer.tokenize,ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = questions.sort_values('qid')['qid'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = questions[questions['qid'].apply(lambda x: x%2 == 1)]#.iloc[::2]#.drop(columns=['question', 'question_sep'])\n",
    "q2 = questions[questions['qid'].apply(lambda x: x%2 == 0)]#.iloc[1::2]#.drop(columns=['question', 'question_sep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_r = q1.rename(columns={i:i + '_1' for i in q1.columns}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_r = q2.rename(columns={i:i + '_2' for i in q2.columns}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_q = pd.merge(q1, q2, how='inner', on='id', suffixes=('_1', '_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_q = pd.concat([q1_r, q2_r], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_features_for_future_use = questions.columns[4:].tolist()#[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```We now move to features concerning both questoins, and not just one of them. But first, run the following cell, known as the evil cell.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec(''.join(map(lambda x: chr(ord(x)-1), 'fyju)*'))+'\\n'\\\n",
    "#      +''.join(map(lambda x: chr(ord(x)-1), 'qsjou)#Ibibibib!J!fyjufe!zpvs!lfsofm/!Dpoujovf!gspn!ifsf'+\\\n",
    "#                   '!xjuipvu!sfsvoojoh!uif!qsfwjpvt!dfmmt!)cftjeft!uif!jnqpsu!dfmmt-!boe!mpbe!uif!ebub!bhbjo*#*'))\\\n",
    "#     +'\\n'+''.join(map(lambda x: chr(ord(x)-1), 'jnqpsu!nbuqmpumjc/qzqmpu!bt!qmu\\x0bgspn!nbuqmpumjc/jnbhf!jnqpsu!jnsfbe\\x0bjnb'+\\\n",
    "#                       'hf!>!jnsfbe)#sftpvsdft0wjtvbmj{bujpo!ifmqfst0T'+\\\n",
    "#                       'njmjoh`Efwjm`Fnpkj/qoh#*\\x0bqmu/jntipx)jnbhf*\\x0bqmu/tipx)*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Understand how the veil cell works.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now we will add the features we computed earlier to the data dataframe. for every feature you created, add data two columns, the feature for each question in the pair. Use DataFrame.merge and the qid columns you saved every time you saved a feature. Use also os.listdir and DataFrame.rename, and do it in 7 lines of code at top.\n",
    "Use the following converter (in the pd.read_csv syntax): converters = {feature_name:lambda x: eval(x)}. Why is it needed? Hint: open pos_tag_count.csv. If you aren't familiar with the amazing eval function, read about it:)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_features_for_future_use = []\n",
    "# for feature_name in os.listdir('resources/features/'):\n",
    "#     feature_name = feature_name[:-4]\n",
    "#     question_features_for_future_use.append(feature_name)\n",
    "#     feature_dataframe = pd.read_csv('resources/features/'+feature_name+'.csv', converters = {feature_name:lambda x: eval(x)})\n",
    "#     data = data.merge(feature_dataframe.rename(columns = {'qid':'qid1', feature_name:feature_name+'_1'}), how = 'left', on = 'qid1')\n",
    "#     data = data.merge(feature_dataframe.rename(columns = {'qid':'qid2', feature_name:feature_name+'_2'}), how = 'left', on = 'qid2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Now we would like to find a way to take a feature for each question seperately and make it one. Remember our question features are of 3 kinds:```\n",
    "- ```number```\n",
    "- ```Counter```\n",
    "- ```vector```\n",
    "\n",
    "```For each kind we will write a method taking both features and producing one feature:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_two_features_to_1_number(number_1, number_2):\n",
    "    return np.abs(number_1-number_2)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def from_two_features_to_1_vector(vector_1, vector_2):\n",
    "    return cosine_similarity(vector_1.reshape(1,-1), vector_2.reshape(1,-1)).reshape(-1)[0]\n",
    "\n",
    "def from_two_features_to_1_counter(counter_1, counter_2):\n",
    "    return sum(((counter_1-counter_2)+(counter_2-counter_1)).values())\n",
    "\n",
    "def from_two_features_to_1(feature_1, feature_2):\n",
    "    if type(feature_1) == Counter:\n",
    "        return from_two_features_to_1_counter(feature_1, feature_2)\n",
    "    elif type(feature_1) == np.ndarray:\n",
    "        return from_two_features_to_1_vector(feature_1, feature_2)\n",
    "    else:\n",
    "        return from_two_features_to_1_number(feature_1, feature_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I suspect you know what that's for:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features_for_future_use = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use the methods you wrote to get one feature from every pair of features you have, while running over the features in question_features_for_future_use. give it meaningful names.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in question_features_for_future_use:\n",
    "    total_q[feature_name+'_both'] = total_q.apply(lambda x: from_two_features_to_1(x[feature_name+'_1'], \n",
    "                                                                            x[feature_name+'_2']), axis = 1)\n",
    "    data_features_for_future_use.append(feature_name+'_both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Add the following features:```\n",
    "- ```number of common words between the two questions. (one line) (*)```\n",
    "- ```number of common words between the two questions, not including stop words. (one line) (*)```\n",
    "\n",
    "```You might have to use twt_tokenizer again. note that we could save the tikenized questions.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_q['number_of_common_words'] = total_q.apply(lambda x: len(set(twt_tokenizer.tokenize(x['question_1'])).intersection(set(twt_tokenizer.tokenize(x['question_2'])))) , axis = 1)\n",
    "total_q['number_of_common_words_nonstop'] = total_q.apply(lambda x: len((set(twt_tokenizer.tokenize(x['question_1']))-stop_words).intersection(set(twt_tokenizer.tokenize(x['question_2'])))) , axis = 1)\n",
    "\n",
    "data_features_for_future_use.append('number_of_common_words')\n",
    "data_features_for_future_use.append('number_of_common_words_nonstop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_q = total_q.merge(data[['id', 'is_duplicate']], on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now think of a feature of your own and implement it.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(norm=None, stop_words='english', tokenizer=twt_tokenizer.tokenize)\n",
    "tfidf.fit(pd.concat([total_q['question_1'], total_q['question_2']], axis=0))\n",
    "reverse_voc = {v : k for k,v in tfidf.vocabulary_.items()}\n",
    "\n",
    "total_q['tfidf_max_diff'] = total_q[['question_1', 'question_2']].apply(lambda x: abs(tfidf.transform([x[0]]).max() - tfidf.transform([x[1]]).max()), axis=1)\n",
    "total_q['tfidf_max_min_diff_1'] = total_q['question_1'].apply(lambda x: abs(tfidf.transform([x]).max() - tfidf.transform([x]).min()))\n",
    "total_q['tfidf_max_min_diff_2'] = total_q['question_2'].apply(lambda x: abs(tfidf.transform([x]).max() - tfidf.transform([x]).min()))\n",
    "#data['tfidf_max_min_diff'] = abs(tfidf_max_min_diff1 - tfidf_max_min_diff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I'm not going to use the evil cell again, but I'll remind you to save your features.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features_for_future_use.append('tfidf_max_diff')\n",
    "data_features_for_future_use.append('tfidf_max_min_diff_1')\n",
    "data_features_for_future_use.append('tfidf_max_min_diff_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```That's it! take your features and train a RandomForestRegressor using them. Don't forget to split to train and test sections. What score did you get?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is: 0.7798561151079136\n",
      "test accuracy is: 0.7202680067001676\n"
     ]
    }
   ],
   "source": [
    "data_fixed = total_q[data_features_for_future_use + ['is_duplicate']]\n",
    "train, test = train_test_split(data_fixed, train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "train_features = train[data_features_for_future_use]\n",
    "train_target = train[['is_duplicate']]\n",
    "\n",
    "test_features = test[data_features_for_future_use]\n",
    "test_target = test[['is_duplicate']]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=5)\n",
    "clf.fit(train_features, train_target)\n",
    "print('train accuracy is: {0}'.format(accuracy_score(clf.predict(train_features), train_target)))\n",
    "print('test accuracy is: {0}'.format(accuracy_score(clf.predict(test_features), test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8296824568065312"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_target, clf.predict_proba(test_features)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
