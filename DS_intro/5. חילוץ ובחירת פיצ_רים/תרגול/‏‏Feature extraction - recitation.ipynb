{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW & TF-IDF - recitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``\n",
    "In this recitation, you will experience with implementing \"tf-idf document vectorizer\", and see how it can be use to solve practical problem.\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```What we going to do here is to solve the next problem:\n",
    "Given a set of articles writen by different authors, we want to decide the \"authorship\" of a new article writen by one of the authors.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can be found in: https://drive.google.com/open?id=1Ni5Rb_q9gdCIGPW2i1oyH2a5G823fOio\n",
    "df = pd.read_csv('data/Gungor_2018_VictorianAuthorAttribution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have articles of 10 different authors\n",
    "df.groupby('author').agg(article_count=pd.NamedAgg(column=\"text\", aggfunc=\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"bag of words\" (BOW)\n",
    "\n",
    "wikipedia link: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "```Bag of words is a way to represent a document (or a sentance) as the \"counts\" of the words in it.```\n",
    "\n",
    "``That way, if we have the vocabulary of the language (all the words in the language) and we agree on some order of thses words (say [\"television\", \"mouse\", ...]), we can represent any socument as a vector that each value in it is the count of the corresponding word in the document.``\n",
    "\n",
    "example:\n",
    "\n",
    "```doc       = \"John likes to watch movies. Mary likes movies too.\n",
    "vocabulary = [\"John, \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"book\"].\n",
    "=> bow = [1, 2, 1, 1, 2, 1, 1, 0].```\n",
    "\n",
    "With this representation we can measure \"distance\" between documents.\n",
    "\n",
    "```The most way to do it is \"cosine similaruty\". For vector v1, and vector v2, we define \"cosine similaruty\" as:```\n",
    "\n",
    "$$similarity(v1, v2) = \\frac{<v1, w2>}{\\lVert v1 \\rVert \\cdot \\lVert v2 \\rVert}$$\n",
    "\n",
    "```And it is the value of``` $cos(\\theta)$, ```where``` $\\theta$ ```is the angle between v1 and v2. (big similarity(v1, v2) means v1 and v2 are close to each other)```\n",
    "\n",
    "(Explanation can be found in: https://en.wikipedia.org/wiki/Cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Lets make our articles to be in BOW shape:``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(document):\n",
    "    \n",
    "    doc_strings = document.split(' ')\n",
    "    doc_strings = [i.strip() for i in doc_strings]\n",
    "    doc_strings = [i for i in doc_strings if i != '']\n",
    "    \n",
    "    return doc_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['text'].apply(clean_words).tolist()\n",
    "authors = df['author'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Lets spit the data into train and test. We remember that the meaning of the train and test is: data we have (train), and data we will see in the future (test).``\n",
    "\n",
    "``Because we have only the train data when we \"learn\", the words we consider for the model are only the words we see in the train.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, test_docs, train_authors, test_authors = train_test_split(documents, authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = reduce(operator.iconcat, train_docs, [])\n",
    "vocabulary = np.array(list(set(all_words))) # the vocabulary composed only from the words in the train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create vectorizer that given the vocabulary, can turn documents to BOW vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_vectorizer(words):\n",
    "    \n",
    "    indexer = {w:i for i,w in enumerate(words)}\n",
    "    \n",
    "    def vectorizer(document):\n",
    "        N, vocab_size = len(documents), len(words)\n",
    "        vec = np.zeros(vocab_size)\n",
    "        for w in document:\n",
    "            if w in indexer: # if we dont have the word in the vocabulary we not counting it\n",
    "                vec[indexer[w]] += 1\n",
    "        return vec\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = create_document_vectorizer(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_vectors = np.vstack([vectorizer(doc) for doc in train_docs])\n",
    "test_bow_vectors = np.vstack([vectorizer(doc) for doc in test_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_cosine_similarity(vecs1, vecs2):\n",
    "    \"\"\"\n",
    "    vecs1: N1 x M dim matrix (N1 vectors with dim M)\n",
    "    vecs2: N2 x M dim matrix (N2 vectors with dim M)\n",
    "    \n",
    "    return: N1 x N2 dim matrix \"scores\", where scores[i,j] = cosine_similarity(vecs1[i], vecs2[j])\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = # answer (can check it against \"sklearn.metrics.pairwise.cosine_similarity\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```In order to decise the author of every document in the test, we will calculate the cosine similarity between every doc in the test to every doc in the train.\n",
    "We will say that the author of a document in the test is the author of its \"most similar document in the train\". (highest score)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pairwise_cosine_similarity(test_bow_vectors, train_bow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_authors = train_authors[scores.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: ', np.mean(predicted_authors == test_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('confision matrix:')\n",
    "confusion_matrix(test_authors, predicted_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Not bad hah? Lets see how can we improve it.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf (term frequencyâ€“inverse document frequency)\n",
    "\n",
    "wikipedia link: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "```Main problem of \"bag of words\" approach (in addition to the lost of \"words order\"), is that we give significant wight to \"stop words.```\n",
    "\n",
    "<b>stop words:</b> ```are the most common words in the language (i.e. it, I, was, to, ...), and they can be found in almost all documents. Therfore, they cannot help us \"distinguish\" between documents, and usualy their value in the BOW vector is very high compared to other words.```\n",
    "\n",
    "```tf-idf come to solve this problem by giving \"score\" to each word that represent: how much this word \"distinguish\" the documents we have.```\n",
    "\n",
    "Say we $N$ documents $\\{doc_1, ..., doc_N\\}$, and vocabulary $\\{w_1, ..., w_M\\}$.\n",
    "\n",
    "We define $n_w$ = \"in how many documents w appears\".\n",
    "\n",
    "Lets define the \"idf\" (inverse document frequency) score for a word $w$ as: $idf(w) = log(\\frac{N}{n_w + 1})$\n",
    "\n",
    "```(low idf value means \"stop word\", and high as distinguishing word)```\n",
    "\n",
    "\n",
    "```We now can improve our BOW representation of some new document by multiplying every value in the BOW vector with the corresponting idf score\n",
    "(td-idf[i] = bow[i]*idf[i]), and get tf-idf representation.```\n",
    "\n",
    "\n",
    "\n",
    "<b>Remark:</b>\n",
    "\n",
    "```The training process of this tf-idf \"model\" is the calculation of the idf scores.\n",
    "(And there are more ways to calculate idf scores)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def culc_idf_scores(bow_vectors):\n",
    "    \"\"\"\n",
    "    bow_vectors: matrix of shape (num documents x vocabulary size)\n",
    "    \n",
    "    return: idf score for every word in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    idf = # answer\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = culc_idf_scores(train_bow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Lets see which words got low scores:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[np.argsort(idf)[:50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Lets see which words got high scores:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[np.argsort(idf)[::-1][:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_vectors = train_bow_vectors * idf\n",
    "test_tfidf_vectors  = test_bow_vectors * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pairwise_cosine_similarity(test_tfidf_vectors, train_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_authors = train_authors[scores.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: ', np.mean(predicted_authors == test_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('confision matrix:')\n",
    "confusion_matrix(test_authors, predicted_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Make a vector for every author, as the mean of all his tf-idf vectors in the train.\n",
    "Try to compare the test tf-idf vectors only to those vectors now instead of all the vectors.\n",
    "How the accuracy now? how is the computation time?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now do the same, but instead of the one \"mean\" vector for each author, find 3 vectors for each author as the cluster centers of KMeans(k=3) on his train tf-idf vectors. (try low n_init to reduce run time)\n",
    "Is it more helpful?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
