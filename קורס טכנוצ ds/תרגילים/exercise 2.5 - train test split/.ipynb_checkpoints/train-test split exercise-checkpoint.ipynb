{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Train-Test Split\n",
    "```In this exercise you will experience with an importent and often neglected issue in the data scienstist work - the train-test split. For a specific dataset we will examine different ways to split it and will understand the limitations and constraints we have to take when creating a good train-test split.```\n",
    "\n",
    "```~ Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```First, load the dataset. Notice that the dataset is made out of pairs of objects, where each row has the id of each object and the features related to it. How many different objects are there?\n",
    "We would like to describe the objects and the data using a specific data structure. What structure can best describe the objects and the relations between them (what two objects happen to be in the same sample)?\n",
    "You are also given a separated dataset, that we will save for later testing.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use networkx to create a graph describing the objects and the relations between them. How many connected components (use networkx) the graph has? Draw a histogram of their sizes. Are there any edges that aren't between left objects and right objects? That kind of graph is called a bipartite graph. For any graph computations, networkx is your friend, and it should be very easy.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```In order to get a baseline model we will try to have predictions using only one object from each sample. Create a dataset containig only the left objects. Drop duplicates, so every object will appear only once.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Split your data randomly with ratio 0.7-0.3 to train and validation segments. Train a simple model (a random forest, maybe?) to predict the target. Make sure your model isn't overfitted, and try to get the best score you can (on the validation segment). Compare your results to a simple baseline - the mean of the target computed on the train segment.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use the model you got to compute loss on the test dataset given to you. Did you get similar results for the validation and test segments?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Repeat that process, only this time use all of the sample, and not just the left object. Accordingly, you don't have to drop any duplicates. Use the naive train-test split. Did you get a good score on both train and validation? why (or why not)? Do you think the score you got on the test corresponds to the \"real\" generalizing error? Compute the loss on the separated test segment to get the \"real\" error. Is there any gap between validation-error and test-error? Why?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I should supply them with \"real\" test data, and show them that they fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will now create a new train-test split, so that every connected component is contained either in the train segment or in the test segment. To do so, implement the following algorithm:```\n",
    "\n",
    "```while length(train_segment)<0.7*length(data):\n",
    "    choose randomly a sample s from the data (that is not in train_segment)\n",
    "    add the connected component containing s to the train_segment\n",
    "test_segment = data - train_segment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train a good model using your train segment. What is the best score you can get on your validation segment? Compare it to the test segment. What is the problem with the train-test split method we used? Hint: How many connected components are there in the train segment, and how many are in the validation segment? Examine also the distribution of the target, both in the train and in the test. Do they look the same? Can we safely use the validation to estimate the generalization error on the test segment?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Do the train-test split again, only this time make sure you have ~0.7 of the connected components in your train segment. That is, implement the following algorithm:```\n",
    "\n",
    "```while length(train_segment)<0.7*length(data):\n",
    "    choose randomly a connected component c from the graph\n",
    "    add c to the train_segment\n",
    "test_segment = data-train_segment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```What part of the connected components you have in your train segment this time? Try alse look again at the distribution of the target in the two segments.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train a good model using you train segment. What is the best score you can get on your validation and test segments? Did you get a better score? Can you now use the validation segment to estimate the generalization error? ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Bonus: the data for this exercise was uniquely generated, using MNIST (what? how???). Can you generate a similar dataset? What parameters control this problem? Exaplaine how it can be done.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
