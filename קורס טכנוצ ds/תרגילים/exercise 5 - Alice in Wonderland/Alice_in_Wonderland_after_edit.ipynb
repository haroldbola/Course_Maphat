{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alice_in_Wonderland.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aS2vcNEFQAhY"
      },
      "source": [
        "# Text Generating using RNN\n",
        "```In this exercise you will use a recurrent neural network architecture. It's main purpose if for you to gain confidence when working with networks, while having fun with an interesting and simple application of them.```\n",
        "\n",
        "\n",
        "```~Ittai Haran.```\n",
        "\n",
        "```comments by: Roy Amir```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LqI1wsHkQAhZ",
        "outputId": "a69e45e2-e511-44c3-d377-de864842c2fc",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, LSTM, Input, GRU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vYrP5cCpQAhg"
      },
      "source": [
        "## Part I\n",
        "```Generating text by generating letters.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uq_YXPeKQAhh"
      },
      "source": [
        "```1) Start by loading the text of Alice in Wonderland by Lewis Carroll. Cut away the header and transform the entire text into lower case. Finish when you have lower cased string, containing the story. (header ends at ~ 700. check the exact spot)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Cw8W2o9QAhi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dIz7OHpjQAhm"
      },
      "source": [
        "```2) Create a mapping between the unique characters in the text and integers. Create the reverse mapping.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_zPvAkvQAhn",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kc9BO6QDQAhs"
      },
      "source": [
        "```3) Create the dataset: your network is about to get vectors with 20 characters (or, to be precised, the integers replacing those characters), and predict the next character. Save your results in dataX and dataY.```\n",
        "\n",
        "```\n",
        "(The idea is to slide a \"window\" with a length of 21 words over the story's string, each time saving the 20 first words as x, and the 21st word as y)```\n",
        "\n",
        "```Make sure you do have integer vectors rather than vectors of characters. Transform the integer vectors of dataX to matrices of (number of vectors (20)) X (number of different letters) using 1-hot encoding. Do the same to dataY.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MM7XB8FuQAht",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-M_OhJv9QAhx"
      },
      "source": [
        "```4) Create a simple RNN model with one hidden LSTM layer with 256 units and dropout with rate of 0.2. use categorical crossentropy as loss.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iM68uiEzQAhy",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4W0QY1J8QAh2"
      },
      "source": [
        "```5) Train your model. Use a callback to save your model after every epoch.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-GEcJC1SQAh3",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KOThzqF6QAh8"
      },
      "source": [
        "```6) Now we will use the model to generate text. Start by a random seed. that is, a random sequence you used when training the model. Do the following:```\n",
        " ```Predict the next letter.```\n",
        "- ```Save the letter you got.```\n",
        "- ```Add the predicted letter to the train (concatenate from the right).```\n",
        "- ```Drop the left most letter in you sentence.```\n",
        "- ```Repeat 1000 times.```\n",
        "- ```Print the predicted sentences your model created :)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wx9PGk-3QAh9",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SJeO6KDtQAiB"
      },
      "source": [
        "```7) What can you say of the generated text? is it readable? Did you get any real english words? Any real English sentences?```\n",
        "```Try adding another LSTM+Dropout layer to your model.Are the results somehow better? ```\n",
        "\n",
        "```(remember return_sequences = True if and only if the LTSM layer comes before another LTSM layer)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HoksZXetQAiC",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "heGG8Xa0QAiG"
      },
      "source": [
        "## Part II\n",
        "```Generating text by generating words using Word2Vec.```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8T8zpmGiQAiI"
      },
      "source": [
        "```8) Start by loading a word2vec model and a word tokenizer (using nltk).```\n",
        "\n",
        "```(you are supposed to use the huge file we once told you to download: \"GoogleNews-vectors-negative300.bin\" and use gensim.models.KeyedVectors.load_word2vec_format to load the model from it. word_tokenize from nltk is a good tokenizer)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vUkm780JQAiJ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6DzHDuSOQAiN"
      },
      "source": [
        "```9) Tokenize the text's words to get a list of the words of the story. What words your word2vec model doesn't recognize? Try filtering out such words, or fixing other words, while maintining minimal impact over the original text.```\n",
        "\n",
        "```(do not  blindly delete all the words that don't exist in the model!! Focus on **technical** reasons causing misrecognition, and fix them)```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w5mM2h8YQAiO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NfVKJCpsQAiR"
      },
      "source": [
        "```10) What about the other unrecognised words? Some words are misrecognised because of language differences:```\n",
        "\n",
        "```The book is written by a british author, but word2vec is trained after the american style. Luckily, you are provided with a british-to-american dictionary, to help you translate the british style to american style. Use it to clean your text.```\n",
        "\n",
        "```(use the file that was attached with this exercise, british_to_american.pkl\", and the pickle module. after dictionary check, you may delete any remaining unrecognised words)```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjK8y8Yg8tCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KgEjeWLzQAiU"
      },
      "source": [
        "```11) Create the word_to_num and num_to_word dictionaries as you did earlier with the characters.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_E_LXuuTQAiV",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "24JDXKhWQAiY"
      },
      "source": [
        "```12) Create a dataset. This time we will not use a 1-hot encoding, but an Embedding layer. Hence, each sample would be made of 10 numbers between 0 and the size of your word_to_int dictionary. We would like our model to predict probability over all the words that appeared in our tokenized text. Build your target that way.```\n",
        "\n",
        "``` (do the same window-thing we did before, this time with a window 10 units long, when you translate every word to its corresponding number. the target (11-th word) should be hot encoded)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bVT4pfW-QAiY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GL-HJKdIQAib"
      },
      "source": [
        "```13) Create a matrix of the size (number of different words)X(dimension of word2vec vectors), the i'th row is the vector of int_to_word[i].```\n",
        "\n",
        "```(Word2Vec vector. np.zeros could be handy in creating an empty matrix in the desired size)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tB9qe9KHQAib",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vpHbNdHHQAig"
      },
      "source": [
        "```14) Build the model. Use an embedding layer and initialize it by specifying weights = [matrix] in its builder. Besides that, use the same architecture you used earlier. Train your model. Try 2 different attitudes: training the embeding layer, or freezing it.```\n",
        "\n",
        "```(embedding layer: input_dim = Size of the vocabulary,output_dim = Size of transformed vectors. to \"freeze\" a layer use layer.trainable = False/True)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCfg9T9NQAii",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KL0HSlyNQAil"
      },
      "source": [
        "```15) Time for predicting! Do as you did with the characters to generate text by generating words.```\n",
        "\n",
        "``` (It takes some epochs to get to results. achieving over 90% accuracy is possible, and recommended. Let your computer run while you do other stuff)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQ5jRONuQAin",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}