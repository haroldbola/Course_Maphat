{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Alice in Wonderland.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"}},"cells":[{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"aS2vcNEFQAhY","colab_type":"text"},"source":["# Text Generating using RNN\n","```In this exercise you will use a recurrent neural network architecture. Its main purpose is for you to gain confidence when working with networks, while having fun with an interesting and simple application of them.```\n","\n","\n","```~Ittai Haran```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"LqI1wsHkQAhZ","colab_type":"code","outputId":"a69e45e2-e511-44c3-d377-de864842c2fc","colab":{}},"source":["import sys\n","import numpy as np\n","from keras.models import Model\n","from keras.layers import Dense, Dropout, LSTM, Input, GRU\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils"],"execution_count":0,"outputs":[{"output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n","  from ._conv import register_converters as _register_converters\n","Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"vYrP5cCpQAhg","colab_type":"text"},"source":["## Part I\n","```Generating text letter by letter.```"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Uq_YXPeKQAhh","colab_type":"text"},"source":["```1) Start by loading the text of Alice in Wonderland by Lewis Carroll (data/wonderland.txt). Cut away the header and transform the entire text into lower case. Finish when you have lower cased string, containing the story.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"0Cw8W2o9QAhi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"dIz7OHpjQAhm","colab_type":"text"},"source":["```2) Create a mapping between the unique characters in the text and integers. Create the reverse mapping.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"u_zPvAkvQAhn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Kc9BO6QDQAhs","colab_type":"text"},"source":["```3) Create the dataset: the model (RNN) you will soon build will get as input a vector of 20 characters (or, to be precise, the integers representing those characters, given by the mapping you just created). From that, it will predict the next character. Save your results as dataX and dataY. Make sure you have integer vectors rather than vectors of characters. After that, transform the integer vectors of dataX to matrices of the shape: (number of vectors (20)) X (number of different letters) using 1-hot encoding. Do the same to dataY.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"MM7XB8FuQAht","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"-M_OhJv9QAhx","colab_type":"text"},"source":["```4) Create a simple RNN model with one hidden LSTM layer with 256 units and dropout with rate of 0.2```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"iM68uiEzQAhy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"4W0QY1J8QAh2","colab_type":"text"},"source":["```5) Train your model. Use a callback to save your model after every epoch.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"-GEcJC1SQAh3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"KOThzqF6QAh8","colab_type":"text"},"source":["```6) Now we will use the model to generate text. Start with a random seed as an input to the model. That is, choose a random sequence of 20 letters you used when training the model. Then, do the following:```\n","- ```Predict the next letter.```\n","- ```Save the letter you got.```\n","- ```Add the predicted letter to the input (concatenate from the right).```\n","- ```Drop the left most letter in your input.```\n","- ```Repeat 1000 times.```\n","- ```Print the predicted sentences your model created :)```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Wx9PGk-3QAh9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"SJeO6KDtQAiB","colab_type":"text"},"source":["```7) What can you say about the generated text? Is it readable? Did you get any real english words? Any real English sentences?```\n","\n","```Try adding another LSTM+Dropout layer to your model. Are the results somehow better?```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"HoksZXetQAiC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"heGG8Xa0QAiG","colab_type":"text"},"source":["## Part II\n","```Generating text by generating words using Word2Vec.```"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"8T8zpmGiQAiI","colab_type":"text"},"source":["```8) Start by loading a word2vec model and a word tokenizer (using nltk).```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"vUkm780JQAiJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"6DzHDuSOQAiN","colab_type":"text"},"source":["```9) Tokenize the text's words to get a list of the words of the story. What words your word2vec model doesn't recognize? Try filtering out such words, or fixing other words, while maintining minimal impact over the original text.```"]},{"cell_type":"markdown","metadata":{"id":"VdZIRsckHcWy","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"w5mM2h8YQAiO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NfVKJCpsQAiR","colab_type":"text"},"source":["```10) The book is written by a british author, but word2vec was trained with american texts. Luckily, you are provided with a british-to-american dictionary, to help you translate the british words to american words. Use it to clean your text.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"t6AnX4y8QAiR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"KgEjeWLzQAiU","colab_type":"text"},"source":["```11) Create the word_to_num and num_to_word dictionaries as you did earlier with the characters.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"_E_LXuuTQAiV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"24JDXKhWQAiY","colab_type":"text"},"source":["```12) Create a dataset. This time we will not use a 1-hot encoding, but an Embedding layer. Hence, each sample will be made out of 10 numbers between 0 and the size of your word_to_int dictionary. We would like our model to predict probabilities for all the words that appeared in our tokenized text. Build your target in the appropriate way.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"bVT4pfW-QAiY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"GL-HJKdIQAib","colab_type":"text"},"source":["```13) Create a matrix of the size (number of different words)X(dimension of word2vec vectors), where the i'th row is the vector of int_to_word[i].```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"tB9qe9KHQAib","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"vpHbNdHHQAig","colab_type":"text"},"source":["```14) Build the model. Use an embeding layer and initialize it by specifying weights = [matrix] in its builder. Besides that, use the same architecture you used earlier. Train your model. Try 2 different attitudes: training the embeding layer, or freezing it.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"qCfg9T9NQAii","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"KL0HSlyNQAil","colab_type":"text"},"source":["```15) Time for predicting! Do as you did with the characters to generate text by generating words.```"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"xQ5jRONuQAin","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}