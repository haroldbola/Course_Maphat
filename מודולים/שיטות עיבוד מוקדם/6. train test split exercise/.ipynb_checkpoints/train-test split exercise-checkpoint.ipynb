{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "```In this exercise you will experience with an important and often neglected issue in the data scientist work - the train-test split. For a specific dataset we will examine different ways to split it and will understand the limitations and constraints we have to take when creating a good train-test split.```\n",
    "\n",
    "```~ Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def maps(func, lister): # if you hate python 3 as much as I do you might want to use this\n",
    "    return list(map(func, lister))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```First, load the dataset. Notice that the dataset is made out of pairs of objects, where each row has the id of each object and the features related to it. How many different objects are there?\n",
    "We would like to describe the objects and the data using a specific data structure. What structure can best describe the objects and the relations between them (what two objects happen to be in the same sample)?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_left</th>\n",
       "      <th>index_right</th>\n",
       "      <th>feature_1_left</th>\n",
       "      <th>feature_2_left</th>\n",
       "      <th>feature_3_left</th>\n",
       "      <th>feature_4_left</th>\n",
       "      <th>feature_5_left</th>\n",
       "      <th>feature_6_left</th>\n",
       "      <th>feature_7_left</th>\n",
       "      <th>feature_8_left</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_92_right</th>\n",
       "      <th>feature_93_right</th>\n",
       "      <th>feature_94_right</th>\n",
       "      <th>feature_95_right</th>\n",
       "      <th>feature_96_right</th>\n",
       "      <th>feature_97_right</th>\n",
       "      <th>feature_98_right</th>\n",
       "      <th>feature_99_right</th>\n",
       "      <th>feature_100_right</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>926_1</td>\n",
       "      <td>407_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>798_1</td>\n",
       "      <td>431_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5761_1</td>\n",
       "      <td>2602_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6847_1</td>\n",
       "      <td>3227_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9163_1</td>\n",
       "      <td>4377_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  index_left index_right  feature_1_left  feature_2_left  feature_3_left  \\\n",
       "0      926_1       407_2               0               0               0   \n",
       "1      798_1       431_2               0               0               0   \n",
       "2     5761_1      2602_2               0               0               0   \n",
       "3     6847_1      3227_2               0               0               0   \n",
       "4     9163_1      4377_2               0               0               0   \n",
       "\n",
       "   feature_4_left  feature_5_left  feature_6_left  feature_7_left  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   feature_8_left  ...  feature_92_right  feature_93_right  feature_94_right  \\\n",
       "0               0  ...                 0                 0                 0   \n",
       "1               0  ...                 0                 0                 0   \n",
       "2               0  ...                 0                 0                 0   \n",
       "3               0  ...                 0                 0                 0   \n",
       "4               0  ...                 0                 0                 0   \n",
       "\n",
       "   feature_95_right  feature_96_right  feature_97_right  feature_98_right  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   feature_99_right  feature_100_right  target  \n",
       "0                 0                  0    10.8  \n",
       "1                 0                  0    11.8  \n",
       "2                 0                  0     1.6  \n",
       "3                 0                  0     4.4  \n",
       "4                 0                  0     3.0  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use networkx to create a graph describing the objects and the relations between them. How many connected components the graph has? Draw a histogram of their sizes. Are there any edges between left objects and right objects? That kind of graph is called a bipartite graph. For any graph computations, networkx is your friend, and it should be very easy.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of connected compnents:  213\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of objects seen both as left and right objects:  0\n"
     ]
    }
   ],
   "source": [
    "list_indices = list(zip(df['index_left'], df['index_right']))\n",
    "graph = nx.Graph(list_indices)\n",
    "\n",
    "connected_components = list(nx.connected_components(graph))\n",
    "print('number of connected compnents: ', str(len(connected_components)))\n",
    "plt.hist(maps(len, connected_components), bins=50)\n",
    "plt.show()\n",
    "\n",
    "print('number of objects seen both as left and right objects: ', \n",
    "      str(len(set(df['index_left']).intersection(df['index_right']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```In order to get a baseline model we will try to have predictions using only one object from each sample. Create a dataset containing only the left objects. Drop duplicates, so every object will appear only once.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10768, 102)\n",
      "(166451, 203)\n"
     ]
    }
   ],
   "source": [
    "left_df = df.drop(columns=['index_right']+list(map(lambda x: 'feature_'+str(x)+'_right', range(1, 101))))\n",
    "left_df = left_df.drop_duplicates(subset=['index_left'])\n",
    "\n",
    "print(left_df.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Split your data randomly with ratio 0.7-0.3. Train a simple model (a random forest, maybe?) to predict the target. Make sure your model isn't overfitted, and try to get the best score you can (on the test segment). Compare your results to a simple baseline - the mean of the target computed on the train segment.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(left_df.drop(columns=['target']), left_df['target'], train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline mse on train: 10.82211121221284\n",
      "baseline mse on test: 10.691807073178115\n",
      "mse on train: 0.20239780127155876\n",
      "mse on test: 2.7820901424458344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=32)\n",
    "\n",
    "max_features = ['sqrt', 'log2']\n",
    "min_samples_split = [2, 5, 10, 15, ]\n",
    "min_samples_leaf = [1, 2, 4, 5, 10]\n",
    "bootstrap = [True, False]\n",
    "n_estimators = [40, 90, 100, 110, 120, 130, 140, 150, 200, 210, 220, 250]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               }\n",
    "\n",
    "random_model = RandomizedSearchCV(estimator=rf_reg, param_distributions=random_grid,\n",
    "                                  n_iter=50, verbose=2, random_state=32, n_jobs=-1,\n",
    "                                  return_train_score=True, cv=3)\n",
    "\n",
    "random_model.fit(X_train, Y_train)\n",
    "\n",
    "print('baseline mse on train: {0}'.format(mean_squared_error(Y_train,np.mean(Y_train)*np.ones(len(Y_train)))))\n",
    "print('baseline mse on test: {0}'.format(mean_squared_error(Y_test,np.mean(Y_train)*np.ones(len(Y_test)))))\n",
    "print('mse on train: {0}'.format(mean_squared_error(Y_train, random_model.predict(X_train))))\n",
    "print('mse on test: {0}'.format(mean_squared_error(Y_test, random_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Repeat that process, only this use all of the sample, and not just the left object. Accordingly, you don't have to drop any duplicates. Use the naive train-test split. Did you get a good score on both train and test? why (or why not)? Do you think the score you got on the test corresponds to the \"real\" generalization error? why, or why not?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 40.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline mse on train: 9.1000140147194\n",
      "baseline mse on test: 9.110322662135223\n",
      "mse on train: 0.03586328117040951\n",
      "mse on test: 0.4559457747153847\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop(columns=['target']), df['target'], train_size = 0.7)\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=32)\n",
    "\n",
    "max_features = ['sqrt', 'log2']\n",
    "min_samples_split = [2, 5, 10, 15, ]\n",
    "min_samples_leaf = [1, 2, 4, 5, 10]\n",
    "bootstrap = [True, False]\n",
    "n_estimators = [40, 90, 100, 110, 120, 130, 140, 150, 200, 210, 220, 250]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               }\n",
    "\n",
    "random_model = RandomizedSearchCV(estimator=rf_reg, param_distributions=random_grid,\n",
    "                                  n_iter=50, verbose=2, random_state=32, n_jobs=-1,\n",
    "                                  return_train_score=True, cv=3)\n",
    "\n",
    "random_model.fit(X_train, Y_train)\n",
    "\n",
    "print('baseline mse on train: {0}'.format(mean_squared_error(Y_train,np.mean(Y_train)*np.ones(len(Y_train)))))\n",
    "print('baseline mse on test: {0}'.format(mean_squared_error(Y_test,np.mean(Y_train)*np.ones(len(Y_test)))))\n",
    "print('mse on train: {0}'.format(mean_squared_error(Y_train, random_model.predict(X_train))))\n",
    "print('mse on test: {0}'.format(mean_squared_error(Y_test, random_model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will now create a new train-test split, so that every connected component is contained either in the train segment or in the test segment. To do so, implement the following algorithm:```\n",
    "\n",
    "```while length(train_segment)<0.7*length(data):\n",
    "    choose randomly a sample s from the data (that is not in train_segment)\n",
    "    add the connected component containing s to the train_segment\n",
    "test_segment = data - train_segment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_indexes = dict(zip(list(zip(df['index_left'], df['index_right'])), df.index))\n",
    "nx.set_edge_attributes(graph, edges_to_indexes, name='edge_index')\n",
    "\n",
    "all_connected_subgraphs = list(graph.subgraph(c) for c in nx.connected_components(graph))\n",
    "\n",
    "train_index = []\n",
    "indexes_to_choose_from = list(df.index)\n",
    "while len(train_index)<=0.7*len(df):\n",
    "    place = np.random.choice(indexes_to_choose_from)\n",
    "    sample = df.iloc[place]\n",
    "\n",
    "    for graph_co in all_connected_subgraphs:\n",
    "        nodes_connected = graph_co.nodes()\n",
    "        if sample['index_left'] in nodes_connected:\n",
    "            break\n",
    " \n",
    "    df_connected = df['index_left'].apply(lambda x: x in nodes_connected)\n",
    "    list_indices = list(set(df_connected.index[df_connected].tolist()))\n",
    "    \n",
    "\n",
    "    train_index.extend(list_indices)\n",
    "    train_index = list(set(train_index))\n",
    "    indexes_to_choose_from = list(set(indexes_to_choose_from)-set(train_index)) \n",
    "\n",
    "test_index = indexes_to_choose_from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train a good model using your train segment. What is the best score you can get on your test? What is the problem with the train-test split method we used? Hint: How many connected components are there in the train segment, and how many are in the test segment? Examine also the distribution of the target, both in the train and in the test. Do they look the same?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = (df.drop(columns=['target']).iloc[train_index],\n",
    "                                    df.drop(columns=['target']).iloc[test_index],\n",
    "                                    df['target'].iloc[train_index],\n",
    "                                    df['target'].iloc[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Do the train-test split again, only this time make sure you have ~0.7 of the connected components in your train segment, using a different algorithm.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```What part of the connected components you have in your train segment this time? Try also look again at the distribution of the target in the two segments.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train a good model using you train segment. What is the best score you can get on your test? Did you get a better score? why?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Bonus: the data for this exercise was uniquely generated, using MNIST (what? how???). Can you generate a similar dataset? What parameters control this problem? Explain how it can be done.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
