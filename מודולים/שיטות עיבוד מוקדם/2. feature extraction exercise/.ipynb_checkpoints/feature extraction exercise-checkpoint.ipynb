{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction exercise\n",
    "```This exercise is purely about features extraction. We will learn how to do it quick and efficiently.\n",
    "We will be working on a kaggle dataset of quora questions, where each record is composed of a pair of questions, while the target is to determine whether the questions have the same meaning.\n",
    "We will extract features for each question and for each pair of questions and will train a simple model (default xgboost) using those features.```\n",
    "\n",
    "```The purpose of this exercise is to acquire good practices, so please read the instructions carefully and do as it says. You are also encouraged to look at the solution when after you are finished. In addition, when solving the exercise, try to write\n",
    "as efficient and as clean code as you can.```\n",
    "\n",
    "```Note: We are about to do some kaggle cheats, that is, we will compute features by mixing the train and the test.\n",
    "Please notice exactly where we did so and tell it to your tutor. In addition, every time you meet a question in the instructions (you can identify a question by '?'), please answer it in a comment block for future discussing with you tutor. ```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some modules you might find useful\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence tokenizer for future use\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "twt_tokenizer = TweetTokenizer()\n",
    "\n",
    "# word2vec model for future use\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('resources/word_2_vec/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('resources/data/train.csv')\n",
    "data.head()\n",
    "data = data.iloc[:2000]\n",
    "data = data[data.apply(lambda x: not (type(x['question1']) == float or type(x['question2']) == float), axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```First we would like to extract features regarding a single question. In order to do so, first create a dataset containing  all the questions (and their id. why should we remember the id?), without duplicates. Name it 'questions'.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Add a column containing the questions, tokenized using twt_tokenizer, the TweetTokenizer object we created earlier. Name it 'question_sep'. Make sure that you treat the questions in lower case.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Create an empty list called 'question_features_for_future_use'. We are going to befoul the questions dataframe, so we will want to remember which of its columns are important to us and which are just columns helping us to create other columns.```\n",
    "```Next, I will ask you to create some features. Whenever I use this sign: (*), know that you have to add the feature name to the list.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Before we start computing features, write a function that gets a column name and saves a csv file with 2 columns: qid and the column chosen. Name it 'save_feature' and make sure you use it after every feature computed, since it might be very very important for later parts of the exercise and your life.```\n",
    "\n",
    "```Save the features in the resources/features/<col_name>.csv .```\n",
    "\n",
    "```use os.path and os.getcwd().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Compute the following:```\n",
    "- ```Counter of the word part of speech (use collections.Counter, which we imported earlier. do it using one line). (*)```\n",
    "- ```number of different numbers appearing in the question\n",
    "(numbers, not digits. use regex. don't count words like 'one') (one line). (*)```\n",
    "- ```number of words in a question (one line). (*)```\n",
    "- ```length of longest word (one line). (*)```\n",
    "- ```word2vec mean of the question. (*)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```Counter of the question_words (one line). (*)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_words = ['why', 'how', 'where', 'who', 'what', 'which', 'when', 'wheather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will now use tf-idf grade (if you aren't familiar with the concept, read about it, and be ready to\n",
    "talk about it with your tutor ;) ``` https://en.wikipedia.org/wiki/Tf%E2%80%93idf ```).\n",
    "do the following:```\n",
    "- ```initialize a TfidfVectorizer object. use norm = None, use English stop words and twt_tokenizer we used before. Name it tfidf.```\n",
    "- ```create the tf-idf matrix of all the questions (look again at the note in the beginning of the exercise).```\n",
    "- ```look at tfidf.vocabulary_.```\n",
    "- ```create a reversed vocabulary (given an index returns a word. do it in one line, using list comprehension).```\n",
    "- ```create a column, such that every question has a list of its words and the word's tf-idf grades. do it without transferring the tf-idf matrix into a dense matrix (keep it sparse).```\n",
    "- ```for each question, find the third biggest tf-idf grade. create a column such that every question have a list of the words with bigger grades the the third biggest tf-idf grade. (*)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will do it again, this time more general, by completing the following functions.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T09:06:23.696008Z",
     "start_time": "2019-09-05T09:06:23.689988Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_important_words_n(x, n = 3):\n",
    "    # the function gets a list of tuples (word, tf-idf grade) and n\n",
    "    # it returns the words which have grades bigger than the n'th biggest grade. Note that several words may have the same grade.\n",
    "    pass\n",
    "\n",
    "def get_most_important_words_total(x, n = 3):\n",
    "    # the function gets a list of tuples (word, tf-idf grade) and n\n",
    "    # it returns the n words that have the biggest grades\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_features(n_gram, most_important_words_method):\n",
    "    # the function gets the size of n for the n_grams in the tf-idf computation and\n",
    "    # the result method, which will be one of the two functions above\n",
    "    # it adds the tf-idf features you computed before\n",
    "    # when n_gram = 1, make the function add another feature: the mean word2vec vector of the most important words\n",
    "    # features created will be added to question_features_for_future_use (!)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_tfidf_features(1,get_most_important_words_n)\n",
    "get_tfidf_features(2,partial(get_most_important_words_total, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```We now move to features concerning both questions, and not just one of them. But first, run the following cell, known as the evil cell.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(''.join(map(lambda x: chr(ord(x)-1), 'fyju)*'))+'\\n'\\\n",
    "     +''.join(map(lambda x: chr(ord(x)-1), 'qsjou)#Ibibibib!J!fyjufe!zpvs!lfsofm/!Dpoujovf!gspn!ifsf'+\\\n",
    "                  '!xjuipvu!sfsvoojoh!uif!qsfwjpvt!dfmmt!)cftjeft!uif!jnqpsu!dfmmt-!boe!mpbe!uif!ebub!bhbjo*#*'))\\\n",
    "    +'\\n'+''.join(map(lambda x: chr(ord(x)-1), 'jnqpsu!nbuqmpumjc/qzqmpu!bt!qmu\\x0bgspn!nbuqmpumjc/jnbhf!jnqpsu!jnsfbe\\x0bjnb'+\\\n",
    "                      'hf!>!jnsfbe)#sftpvsdft0wjtvbmj{bujpo!ifmqfst0T'+\\\n",
    "                      'njmjoh`Efwjm`Fnpkj/qoh#*\\x0bqmu/jntipx)jnbhf*\\x0bqmu/tipx)*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Understand how the evil cell works.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now we will add the features we computed earlier to the data dataframe. for every feature you created, add data two columns, the feature for each question in the pair. Use DataFrame.merge and the qid columns you saved every time you saved a feature. Use also os.listdir and DataFrame.rename, and do it in 7 lines of code at top.\n",
    "Use the following converter (in the pd.read_csv syntax): converters = {feature_name:lambda x: eval(x)}. Why is it needed? Hint: open pos_tag_count.csv. If you aren't familiar with the amazing eval function, read about it and be ready to discuss it with your tutor :)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Now we would like to find a way to take a feature for each question separately and make it one. Remember our question features are of 3 kinds:```\n",
    "- ```number```\n",
    "- ```Counter```\n",
    "- ```vector```\n",
    "\n",
    "```For each kind we will write a method taking both features and producing one feature:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_two_features_to_1_number(number_1, number_2):\n",
    "    # the function returns |number_1-number_2|\n",
    "    pass\n",
    "\n",
    "def from_two_features_to_1_vector(vector_1, vector_2):\n",
    "    # the function returns the cosine similarity between the vectors\n",
    "    pass\n",
    "\n",
    "def from_two_features_to_1_counter(counter_1, counter_2):\n",
    "    # the Counter objects represent distribution. return the sim of all absolute values of differences between them.\n",
    "    # Remember- Counter have some great properties.\n",
    "    pass\n",
    "\n",
    "def from_two_features_to_1(feature_1, feature_2):\n",
    "    # return the right one feature based on the feature's types\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I suspect you know what that's for:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_features_for_future_use = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use the methods you wrote to get one feature from every pair of features you have, while running over the features in question_features_for_future_use. give it meaningful names.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Add the following features:```\n",
    "- ```number of common words between the two questions. (one line) (*)```\n",
    "- ```number of common words between the two questions, not including stop words. (one line) (*)```\n",
    "\n",
    "```You might have to use twt_tokenizer again. note that we could save the tikenized questions.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:21:47.684219Z",
     "start_time": "2019-09-05T14:21:47.670220Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now think of a feature of your own and implement it.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I'm not going to use the evil cell again, but I'll remind you to save your features.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```That's it! take your features and train a RandomForestRegressor using them. Don't forget to split to train and test sections. What score did you get?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
