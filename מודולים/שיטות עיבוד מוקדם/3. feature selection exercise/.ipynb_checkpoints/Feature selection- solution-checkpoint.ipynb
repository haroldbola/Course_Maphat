{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "```In this exercise you will experience a very under rated aspect of the data scientist job: feature selection.\n",
    "We will look at the most common algorithms, examine them and will develop new feature selection algorithms.\n",
    "The data you will be working on is pretty hard: you will soon find out why :)```\n",
    "\n",
    "```Note: When questions are asked (you can identify a question by '?'), answer it in another cell and be prepared to talk about it with your tutor.```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Load data.csv. The data is made up from some measurements of soil in Africa, and was derived from Kaggle. SOC is the target.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train a simple Random Forest model on the data. Use your knowledge to make it as good a model as you can. Don't forget to split the data to train and test segments.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_train = []\n",
    "error_test = []\n",
    "for depth in range(2,12):\n",
    "    clf = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=depth)\n",
    "    clf.fit(df_train, target_train)\n",
    "    \n",
    "    error_train.append(mean_squared_error(clf.predict(df_train), target_train))\n",
    "    error_test.append(mean_squared_error(clf.predict(df_test), target_test))\n",
    "    \n",
    "    print(depth)\n",
    "    \n",
    "plt.plot(error_train)\n",
    "plt.plot(error_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf.fit(df_train, target_train)\n",
    "print(mean_squared_error(clf.predict(df_train), target_train))\n",
    "print(mean_squared_error(clf.predict(df_test), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use model.feature_importances_ to select the 100 most important features. Use these features to train your model. Did you get better results on the train segment? what are results on the test segment?\n",
    "How does model.feature_importances_ work? Some features have 0 importance. Why?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x[0] for x in sorted(zip(df.columns, clf.feature_importances_), key = lambda x: -x[1])][:100]\n",
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[features], target_train['SOC'])\n",
    "print(mean_squared_error(clf_selected.predict(df_train[features]), target_train['SOC']))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[features]), target_test['SOC']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Well, that didn't work quite so good. Let's try a different approach. Now select the 100 features that are most correlated to the data. What results did you get now? You got so far two groups of feature selected. Are they similar to one another? Why or why not?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = np.corrcoef(df_train.transpose(), target_train)\n",
    "corr_features = sorted(list(zip(list(df.columns)+['SOC'], corrs[-1]), key = lambda x: -x[1]))[1:]\n",
    "corr_features = [x[0] for x in corr_features[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.200874887434\n",
      "0.401442933818\n"
     ]
    }
   ],
   "source": [
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[corr_features], target_train)\n",
    "print(mean_squared_error(clf_selected.predict(df_train[corr_features]), target_train))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[corr_features]), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now cluster the features by their correlation to one another. Make sure you have at least ~30 clusters. Pick from each cluster the feature most correlated to the target. What are your results now? What clustering algorithm did you use?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clusters = DBSCAN(eps = 0.001, metric=\"precomputed\").fit_predict(1-abs(corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(x):\n",
    "    return max(x, key = lambda x: -x[0])[1]\n",
    "\n",
    "choose_features = pd.DataFrame(zip(clusters, zip(corrs[-1], list(df.columns)+['SOC']))).iloc[:-1]\n",
    "features_selected_corr = choose_features.groupby(0).agg(choose)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0594274307251\n",
      "0.174055082353\n"
     ]
    }
   ],
   "source": [
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[features_selected_corr], target_train['SOC'])\n",
    "print(mean_squared_error(clf_selected.predict(df_train[features_selected_corr]), target_train['SOC']))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[features_selected_corr]), target_test['SOC']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Before we continue, read the documentation of sklearn.feature_selection. What other feature selection algorithms did you find there? try one of them on your data.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```You are about to be very surprised. select 100 features randomly and evaluate the model trained with them. Repeat it 100 times. What is the best set of feature you got? How is it compared to the other subsets you got so far? Can you explain it?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0589292326065\n",
      "0.220988208427\n",
      "-----------------------------------\n",
      "0.130027641885\n",
      "0.363043766131\n",
      "-----------------------------------\n",
      "0.0626344792349\n",
      "0.205456919831\n",
      "-----------------------------------\n",
      "0.0677479410859\n",
      "0.21747579557\n",
      "-----------------------------------\n",
      "0.100113620463\n",
      "0.322853339086\n",
      "-----------------------------------\n",
      "0.0469965640281\n",
      "0.193533815239\n",
      "-----------------------------------\n",
      "0.0649124212461\n",
      "0.322598669285\n",
      "-----------------------------------\n",
      "0.0618093761526\n",
      "0.279370533775\n",
      "-----------------------------------\n",
      "0.0796151484123\n",
      "0.185439996149\n",
      "-----------------------------------\n",
      "0.0964645818662\n",
      "0.283625330334\n",
      "-----------------------------------\n",
      "0.0748653445205\n",
      "0.16183771335\n",
      "-----------------------------------\n",
      "0.111090943036\n",
      "0.307932037551\n",
      "-----------------------------------\n",
      "0.0710708284713\n",
      "0.281532717236\n",
      "-----------------------------------\n",
      "0.0682780111316\n",
      "0.174660986238\n",
      "-----------------------------------\n",
      "0.0511028452941\n",
      "0.233219539664\n",
      "-----------------------------------\n",
      "0.0683026594404\n",
      "0.209631285254\n",
      "-----------------------------------\n",
      "0.0876335899975\n",
      "0.348721954209\n",
      "-----------------------------------\n",
      "0.0989548322443\n",
      "0.25932963095\n",
      "-----------------------------------\n",
      "0.0510780437139\n",
      "0.218421375315\n",
      "-----------------------------------\n",
      "0.09501703349\n",
      "0.249026005175\n",
      "-----------------------------------\n",
      "0.115881040369\n",
      "0.248474557567\n",
      "-----------------------------------\n",
      "0.12884680777\n",
      "0.263006444077\n",
      "-----------------------------------\n",
      "0.0769024594579\n",
      "0.217117121489\n",
      "-----------------------------------\n",
      "0.116463673882\n",
      "0.303885084253\n",
      "-----------------------------------\n",
      "0.116566111129\n",
      "0.301326112651\n",
      "-----------------------------------\n",
      "0.125622380048\n",
      "0.314373405157\n",
      "-----------------------------------\n",
      "0.199828878345\n",
      "0.43734944037\n",
      "-----------------------------------\n",
      "0.118318569731\n",
      "0.29607043647\n",
      "-----------------------------------\n",
      "0.0534009645017\n",
      "0.144437243207\n",
      "-----------------------------------\n",
      "0.110283040007\n",
      "0.410273033341\n",
      "-----------------------------------\n",
      "0.0589441638975\n",
      "0.259207634592\n",
      "-----------------------------------\n",
      "0.0875007502386\n",
      "0.290324185668\n",
      "-----------------------------------\n",
      "0.0593634823608\n",
      "0.186507793721\n",
      "-----------------------------------\n",
      "0.105758582637\n",
      "0.292991525581\n",
      "-----------------------------------\n",
      "0.0737200202684\n",
      "0.20829798995\n",
      "-----------------------------------\n",
      "0.0979717701631\n",
      "0.247034967908\n",
      "-----------------------------------\n",
      "0.082838305202\n",
      "0.201558108438\n",
      "-----------------------------------\n",
      "0.114240223859\n",
      "0.278851572076\n",
      "-----------------------------------\n",
      "0.0751862225919\n",
      "0.271947151465\n",
      "-----------------------------------\n",
      "0.108123819191\n",
      "0.266344756717\n",
      "-----------------------------------\n",
      "0.0603080151147\n",
      "0.198749448814\n",
      "-----------------------------------\n",
      "0.10711889256\n",
      "0.272437598503\n",
      "-----------------------------------\n",
      "0.114973914822\n",
      "0.336592221247\n",
      "-----------------------------------\n",
      "0.093069132212\n",
      "0.275650616078\n",
      "-----------------------------------\n",
      "0.0700606650064\n",
      "0.222615348258\n",
      "-----------------------------------\n",
      "0.0844546565691\n",
      "0.211429882107\n",
      "-----------------------------------\n",
      "0.0626482701076\n",
      "0.205405123432\n",
      "-----------------------------------\n",
      "0.057091372451\n",
      "0.205606176983\n",
      "-----------------------------------\n",
      "0.0950587070955\n",
      "0.269256483526\n",
      "-----------------------------------\n",
      "0.0967877355749\n",
      "0.248409759206\n",
      "-----------------------------------\n",
      "0.0897187949306\n",
      "0.329396896401\n",
      "-----------------------------------\n",
      "0.0889211300617\n",
      "0.237556589593\n",
      "-----------------------------------\n",
      "0.0881886751672\n",
      "0.223601255314\n",
      "-----------------------------------\n",
      "0.0828791400617\n",
      "0.274650578783\n",
      "-----------------------------------\n",
      "0.113651425916\n",
      "0.30093035472\n",
      "-----------------------------------\n",
      "0.0846252062471\n",
      "0.255775927064\n",
      "-----------------------------------\n",
      "0.0519524825097\n",
      "0.205104848157\n",
      "-----------------------------------\n",
      "0.066516349008\n",
      "0.191646641757\n",
      "-----------------------------------\n",
      "0.0869500097912\n",
      "0.214027875639\n",
      "-----------------------------------\n",
      "0.137350457015\n",
      "0.333887197197\n",
      "-----------------------------------\n",
      "0.0971919635078\n",
      "0.255382077107\n",
      "-----------------------------------\n",
      "0.0716501446632\n",
      "0.376723283674\n",
      "-----------------------------------\n",
      "0.0653487158828\n",
      "0.186126669123\n",
      "-----------------------------------\n",
      "0.120341627822\n",
      "0.301988949229\n",
      "-----------------------------------\n",
      "0.0491300111333\n",
      "0.171909281957\n",
      "-----------------------------------\n",
      "0.0804678719979\n",
      "0.217009767188\n",
      "-----------------------------------\n",
      "0.0618956362736\n",
      "0.217484734292\n",
      "-----------------------------------\n",
      "0.0976529541486\n",
      "0.227237152231\n",
      "-----------------------------------\n",
      "0.0695156312183\n",
      "0.199073629652\n",
      "-----------------------------------\n",
      "0.119317819672\n",
      "0.274051499958\n",
      "-----------------------------------\n",
      "0.0821710330926\n",
      "0.224913884888\n",
      "-----------------------------------\n",
      "0.070956237708\n",
      "0.224959914388\n",
      "-----------------------------------\n",
      "0.102903151698\n",
      "0.254795096422\n",
      "-----------------------------------\n",
      "0.099238705584\n",
      "0.216747882479\n",
      "-----------------------------------\n",
      "0.0523035193979\n",
      "0.209723180028\n",
      "-----------------------------------\n",
      "0.0816660470481\n",
      "0.359889135786\n",
      "-----------------------------------\n",
      "0.0509861011439\n",
      "0.18494110341\n",
      "-----------------------------------\n",
      "0.0624488725895\n",
      "0.251028138274\n",
      "-----------------------------------\n",
      "0.111874853107\n",
      "0.207029380937\n",
      "-----------------------------------\n",
      "0.0648645662073\n",
      "0.18767565913\n",
      "-----------------------------------\n",
      "0.126907986594\n",
      "0.332338810633\n",
      "-----------------------------------\n",
      "0.0539028090037\n",
      "0.16446235263\n",
      "-----------------------------------\n",
      "0.0901964994092\n",
      "0.263944366795\n",
      "-----------------------------------\n",
      "0.0682100471421\n",
      "0.228841119908\n",
      "-----------------------------------\n",
      "0.0709957038442\n",
      "0.198651140481\n",
      "-----------------------------------\n",
      "0.120284333683\n",
      "0.271514801088\n",
      "-----------------------------------\n",
      "0.0599509857056\n",
      "0.21735411347\n",
      "-----------------------------------\n",
      "0.164285790846\n",
      "0.366977984566\n",
      "-----------------------------------\n",
      "0.0661899066854\n",
      "0.163305405253\n",
      "-----------------------------------\n",
      "0.110065057823\n",
      "0.223300526224\n",
      "-----------------------------------\n",
      "0.0630203560074\n",
      "0.237380212062\n",
      "-----------------------------------\n",
      "0.0938083635279\n",
      "0.241511792719\n",
      "-----------------------------------\n",
      "0.113064709802\n",
      "0.289148705217\n",
      "-----------------------------------\n",
      "0.0508618454837\n",
      "0.216327211127\n",
      "-----------------------------------\n",
      "0.107955764512\n",
      "0.362426207086\n",
      "-----------------------------------\n",
      "0.0743935402852\n",
      "0.180665301746\n",
      "-----------------------------------\n",
      "0.0699966064604\n",
      "0.184728908289\n",
      "-----------------------------------\n",
      "0.0602168384329\n",
      "0.21636276173\n",
      "-----------------------------------\n",
      "0.064116223743\n",
      "0.218189440414\n",
      "-----------------------------------\n",
      "0.0644307711384\n",
      "0.202905012347\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "for i in range(100):\n",
    "    features_random = list(set(np.random.choice(df.columns, size = 30)))\n",
    "    clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "    clf_selected.fit(df_train[features_random], target_train['SOC'])\n",
    "    print(mean_squared_error(clf_selected.predict(df_train[features_random]), target_train['SOC']))\n",
    "    print(mean_squared_error(clf_selected.predict(df_test[features_random]), target_test['SOC']))\n",
    "    print('-----------------------------------')\n",
    "    features_list.append((features_random,\n",
    "                        mean_squared_error(clf_selected.predict(df_train[features_random]), target_train['SOC']),\n",
    "                        mean_squared_error(clf_selected.predict(df_test[features_random]), target_test['SOC'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0552761666787\n",
      "0.151489479898\n"
     ]
    }
   ],
   "source": [
    "features_rand_selected = min(features_list, key = lambda x: x[2])[0]\n",
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[features_rand_selected], target_train['SOC'])\n",
    "print(mean_squared_error(clf_selected.predict(df_train[features_rand_selected]), target_train['SOC']))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[features_rand_selected]), target_test['SOC']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now it's your time to enter the office's competition for feature selection. Invent your own feature selection algorithm. Make it a good one (because if it isn't, your tutor might ask you to implement one of the other algorithms proposed by the office's members, and that might be awful ;) ) Before you act, talk about your idea with your tutor.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forum4",
   "language": "python",
   "name": "forum4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
