{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "```In this exercise you will experience a very under rated aspect of the data scientist job: feature selection.\n",
    "We will look at the most common algorithms, examine them and will develop new feature selection algorithms.\n",
    "The data you will be working on is pretty hard: you will soon find out why :)```\n",
    "\n",
    "```Note: When questions are asked (you can identify a question by '?'), answer it in another cell and be prepared to talk about it with your tutor.```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Load data.csv. The data is made up from some measurements of soil in Africa, and was derived from Kaggle. SOC is the target.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train a simple Random Forest model on the data. Use your knowledge to make it as good a model as you can. Don't forget to split the data to train and test segments.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MMM\\AppData\\Local\\conda\\conda\\envs\\python3_ittai\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[      m7497.96  m7496.04  m7494.11  m7492.18  m7490.25  m7488.32  m7486.39  \\\n",
       " 0     0.302553  0.301137  0.299748  0.300354  0.302679  0.303799  0.301702   \n",
       " 1     0.270192  0.268555  0.266964  0.267938  0.271013  0.272346  0.269870   \n",
       " 2     0.317433  0.316265  0.314948  0.315224  0.316942  0.317764  0.316067   \n",
       " 3     0.261116  0.259767  0.258384  0.259001  0.261310  0.262417  0.260534   \n",
       " 4     0.260038  0.258425  0.256544  0.257030  0.259602  0.260786  0.258717   \n",
       " 5     0.172350  0.170766  0.169383  0.170148  0.172570  0.173675  0.171809   \n",
       " 6     0.354537  0.352801  0.351282  0.352225  0.355192  0.356416  0.354078   \n",
       " 7     0.312328  0.310354  0.308660  0.309540  0.312404  0.313769  0.311575   \n",
       " 8     0.351604  0.349781  0.348140  0.349015  0.351900  0.353247  0.351208   \n",
       " 9     0.321137  0.319281  0.317571  0.318293  0.321233  0.322900  0.320903   \n",
       " 10    0.318528  0.316745  0.315244  0.316218  0.319089  0.320168  0.317681   \n",
       " 11    0.271869  0.270120  0.268421  0.269073  0.271838  0.273209  0.271064   \n",
       " 12    0.221892  0.221089  0.220112  0.220277  0.221695  0.222519  0.221480   \n",
       " 13    0.203378  0.202519  0.201698  0.202206  0.203815  0.204559  0.203126   \n",
       " 14    0.274983  0.273829  0.272986  0.273775  0.275718  0.276473  0.274814   \n",
       " 15    0.272229  0.271181  0.270077  0.270300  0.271859  0.272776  0.271480   \n",
       " 16    0.237877  0.236226  0.234775  0.235328  0.237268  0.238284  0.236964   \n",
       " 17    0.335613  0.333533  0.332115  0.333135  0.335598  0.336475  0.334277   \n",
       " 18    0.324745  0.322865  0.321144  0.321765  0.324287  0.325310  0.323040   \n",
       " 19    0.270775  0.269260  0.267889  0.268762  0.271207  0.272157  0.269933   \n",
       " 20    0.320766  0.319487  0.317959  0.317944  0.319547  0.320570  0.319381   \n",
       " 21    0.258633  0.257315  0.256076  0.256491  0.258349  0.259416  0.258099   \n",
       " 22    0.300252  0.299175  0.297988  0.298154  0.299673  0.300696  0.299473   \n",
       " 23    0.299191  0.298114  0.297176  0.297704  0.299133  0.299488  0.297775   \n",
       " 24    0.356295  0.355390  0.354542  0.355230  0.357081  0.357585  0.355484   \n",
       " 25    0.307664  0.306018  0.304630  0.305312  0.307496  0.308421  0.306535   \n",
       " 26    0.270177  0.268649  0.267156  0.267592  0.269798  0.270813  0.268780   \n",
       " 27    0.255908  0.254085  0.252350  0.252952  0.255385  0.256369  0.254147   \n",
       " 28    0.354739  0.353233  0.351735  0.351979  0.353762  0.354687  0.352968   \n",
       " 29    0.285674  0.284119  0.282684  0.283247  0.285421  0.286384  0.284549   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 1127  0.094602  0.087023  0.080219  0.084170  0.095920  0.100595  0.090438   \n",
       " 1128  0.226131  0.218800  0.212682  0.216687  0.227658  0.231624  0.221710   \n",
       " 1129  0.224161  0.217137  0.211037  0.214845  0.225742  0.229653  0.219454   \n",
       " 1130  0.405402  0.395589  0.386539  0.391073  0.405613  0.411221  0.397937   \n",
       " 1131  0.400397  0.390273  0.381414  0.386375  0.401221  0.406985  0.393708   \n",
       " 1132  0.339866  0.331163  0.323747  0.328162  0.340658  0.344998  0.333165   \n",
       " 1133  0.376833  0.368153  0.360613  0.365155  0.377888  0.382477  0.370851   \n",
       " 1134  0.435423  0.426570  0.419411  0.424144  0.436600  0.440774  0.429069   \n",
       " 1135  0.425122  0.416775  0.409667  0.413843  0.425782  0.430050  0.418686   \n",
       " 1136  0.416605  0.407067  0.398403  0.403337  0.418530  0.424857  0.412076   \n",
       " 1137  0.408461  0.398368  0.389665  0.395572  0.411681  0.417930  0.404336   \n",
       " 1138  0.377061  0.367975  0.359669  0.363811  0.377257  0.382558  0.370274   \n",
       " 1139  0.345212  0.336018  0.327918  0.332308  0.345565  0.350436  0.338186   \n",
       " 1140  0.442111  0.432687  0.424147  0.428438  0.442133  0.447552  0.435347   \n",
       " 1141  0.416194  0.408525  0.402229  0.406415  0.417629  0.421262  0.410285   \n",
       " 1142  0.337945  0.329746  0.323050  0.327035  0.338025  0.341843  0.331473   \n",
       " 1143  0.344330  0.336493  0.329975  0.334302  0.345772  0.349749  0.339284   \n",
       " 1144  0.265288  0.258436  0.253018  0.256757  0.266497  0.269804  0.260471   \n",
       " 1145  0.227154  0.220173  0.214197  0.217716  0.227807  0.231318  0.221953   \n",
       " 1146  0.303041  0.294231  0.286534  0.290921  0.303706  0.308471  0.296804   \n",
       " 1147  0.295997  0.286903  0.279037  0.283461  0.296405  0.301119  0.289158   \n",
       " 1148  0.388349  0.379921  0.372633  0.376626  0.388245  0.392036  0.380572   \n",
       " 1149  0.402530  0.393798  0.386271  0.390508  0.402794  0.406973  0.395420   \n",
       " 1150  0.238301  0.231176  0.225217  0.228925  0.239310  0.242905  0.233048   \n",
       " 1151  0.240342  0.233151  0.226961  0.230455  0.240552  0.244018  0.234277   \n",
       " 1152  0.207530  0.199996  0.193196  0.196532  0.207288  0.211360  0.201480   \n",
       " 1153  0.182376  0.174975  0.168315  0.171872  0.182498  0.186223  0.176172   \n",
       " 1154  0.146829  0.139909  0.133845  0.137163  0.147417  0.151469  0.141935   \n",
       " 1155  0.091910  0.084445  0.077992  0.081678  0.092455  0.096463  0.086509   \n",
       " 1156  0.471388  0.464764  0.459496  0.463215  0.472801  0.475735  0.466201   \n",
       " \n",
       "       m7484.46  m7482.54  m7480.61    ...          EVI      LSTD      LSTN  \\\n",
       " 0     0.298936  0.298126  0.298120    ...     1.062682 -0.716713 -0.090016   \n",
       " 1     0.266976  0.266544  0.266766    ...     1.062682 -0.716713 -0.090016   \n",
       " 2     0.313874  0.313301  0.313296    ...     1.156705 -1.282552 -0.088336   \n",
       " 3     0.258039  0.257246  0.257124    ...     1.156705 -1.282552 -0.088336   \n",
       " 4     0.256352  0.255902  0.255822    ...     1.191691 -1.206971  0.011420   \n",
       " 5     0.169488  0.168869  0.168796    ...     1.191691 -1.206971  0.011420   \n",
       " 6     0.351400  0.350607  0.350315    ...     1.129738 -1.193144  0.028141   \n",
       " 7     0.308812  0.308033  0.307816    ...     1.129738 -1.193144  0.028141   \n",
       " 8     0.348580  0.347629  0.347163    ...     0.874636 -0.828330 -0.019305   \n",
       " 9     0.317956  0.316825  0.316501    ...     0.874636 -0.828330 -0.019305   \n",
       " 10    0.314855  0.314170  0.314155    ...     1.397959 -1.105609 -0.025249   \n",
       " 11    0.268470  0.267727  0.267194    ...     1.397959 -1.105609 -0.025249   \n",
       " 12    0.220111  0.219783  0.219643    ...     1.069242 -1.079342  0.051365   \n",
       " 13    0.201111  0.200333  0.200200    ...     1.069242 -1.079342  0.051365   \n",
       " 14    0.272887  0.272391  0.272135    ...     1.029883 -0.868677  0.045235   \n",
       " 15    0.269632  0.269072  0.268997    ...     1.029883 -0.868677  0.045235   \n",
       " 16    0.234923  0.234024  0.233674    ...     1.053936 -0.797599  0.057227   \n",
       " 17    0.331626  0.330780  0.330553    ...     1.053936 -0.797599  0.057227   \n",
       " 18    0.320409  0.319814  0.319700    ...     1.000000 -0.768306  0.135227   \n",
       " 19    0.267154  0.266290  0.266112    ...     1.000000 -0.768306  0.135227   \n",
       " 20    0.317426  0.316751  0.316810    ...     1.005831 -0.967695  0.119224   \n",
       " 21    0.256134  0.255522  0.255446    ...     1.005831 -0.967695  0.119224   \n",
       " 22    0.297372  0.296602  0.296550    ...     1.173469 -1.070390  0.075691   \n",
       " 23    0.295769  0.295212  0.295228    ...     1.173469 -1.070390  0.075691   \n",
       " 24    0.353068  0.352334  0.352243    ...     0.978134 -0.894958  0.067069   \n",
       " 25    0.304160  0.303642  0.303687    ...     0.978134 -0.894958  0.067069   \n",
       " 26    0.266300  0.265648  0.265480    ...     0.920554 -0.770461  0.140329   \n",
       " 27    0.251511  0.250983  0.251042    ...     0.920554 -0.770461  0.140329   \n",
       " 28    0.350581  0.349779  0.349617    ...     1.172012 -1.017554  0.170128   \n",
       " 29    0.282122  0.281180  0.280829    ...     1.172012 -1.017554  0.170128   \n",
       " ...        ...       ...       ...    ...          ...       ...       ...   \n",
       " 1127  0.078488  0.075776  0.075346    ...     0.387755 -0.459732 -0.382869   \n",
       " 1128  0.210492  0.208152  0.207646    ...    -0.107872 -0.474842 -0.207403   \n",
       " 1129  0.208024  0.205904  0.205654    ...    -0.107872 -0.474842 -0.207403   \n",
       " 1130  0.382231  0.378799  0.378414    ...     0.042274 -0.434049 -0.153522   \n",
       " 1131  0.378100  0.374986  0.374798    ...     0.042274 -0.434049 -0.153522   \n",
       " 1132  0.319850  0.317467  0.317509    ...    -0.133382 -0.354837 -0.128458   \n",
       " 1133  0.357460  0.354676  0.354437    ...    -0.133382 -0.354837 -0.128458   \n",
       " 1134  0.416002  0.413603  0.413582    ...    -0.033528 -0.309427 -0.257212   \n",
       " 1135  0.406004  0.403938  0.403750    ...    -0.033528 -0.309427 -0.257212   \n",
       " 1136  0.396847  0.393659  0.393316    ...    -0.237609 -0.345885 -0.207197   \n",
       " 1137  0.388465  0.385263  0.385085    ...    -0.237609 -0.345885 -0.207197   \n",
       " 1138  0.355698  0.352722  0.352467    ...     0.167638 -0.234877 -0.410962   \n",
       " 1139  0.323900  0.320873  0.320653    ...     0.167638 -0.234877 -0.410962   \n",
       " 1140  0.420908  0.418102  0.417805    ...    -0.177114 -0.332217 -0.163183   \n",
       " 1141  0.398324  0.396276  0.396158    ...    -0.177114 -0.332217 -0.163183   \n",
       " 1142  0.319850  0.317817  0.317902    ...     0.145773 -0.411993 -0.311067   \n",
       " 1143  0.327480  0.325076  0.324882    ...     0.145773 -0.411993 -0.311067   \n",
       " 1144  0.249985  0.247863  0.247509    ...     0.395773 -0.381748 -0.338928   \n",
       " 1145  0.211678  0.209794  0.209431    ...     0.395773 -0.381748 -0.338928   \n",
       " 1146  0.283242  0.280595  0.280298    ...     0.353499 -0.337854 -0.482806   \n",
       " 1147  0.275436  0.272899  0.272825    ...     0.353499 -0.337854 -0.482806   \n",
       " 1148  0.367922  0.365704  0.365603    ...     0.584548 -0.309710 -0.219049   \n",
       " 1149  0.382794  0.380533  0.380033    ...     0.584548 -0.309710 -0.219049   \n",
       " 1150  0.221954  0.219725  0.219423    ...     0.292274 -0.550442 -0.554737   \n",
       " 1151  0.223327  0.221121  0.220787    ...     0.292274 -0.550442 -0.554737   \n",
       " 1152  0.190100  0.187901  0.187649    ...     0.306851 -0.511141 -0.481023   \n",
       " 1153  0.164717  0.162330  0.161882    ...     0.306851 -0.511141 -0.481023   \n",
       " 1154  0.130603  0.128283  0.128194    ...     0.659621 -0.549317 -0.646528   \n",
       " 1155  0.074924  0.072652  0.072640    ...     0.659621 -0.549317 -0.646528   \n",
       " 1156  0.456083  0.454609  0.454631    ...     0.271137 -0.245935 -0.444938   \n",
       " \n",
       "           REF1      REF2      REF3      REF7      RELI      TMAP      TMFI  \n",
       " 0    -0.861091 -0.537106 -0.722567 -0.646673  1.687734  0.190708  0.056843  \n",
       " 1    -0.861091 -0.537106 -0.722567 -0.646673  1.687734  0.190708  0.056843  \n",
       " 2    -0.935273 -0.631725 -0.832298 -0.814516  1.806660  0.190708  0.056843  \n",
       " 3    -0.935273 -0.631725 -0.832298 -0.814516  1.806660  0.190708  0.056843  \n",
       " 4    -0.906182 -0.528757 -0.795031 -0.780242  0.430513  0.190708  0.056843  \n",
       " 5    -0.906182 -0.528757 -0.795031 -0.780242  0.430513  0.190708  0.056843  \n",
       " 6    -0.902545 -0.586271 -0.780538 -0.774698  0.039755  0.190708  0.056843  \n",
       " 7    -0.902545 -0.586271 -0.780538 -0.774698  0.039755  0.190708  0.056843  \n",
       " 8    -0.842182 -0.683673 -0.689441 -0.732863  0.226639  0.190708  0.056843  \n",
       " 9    -0.842182 -0.683673 -0.689441 -0.732863  0.226639  0.190708  0.056843  \n",
       " 10   -0.937455 -0.485158 -0.780538 -0.862399  2.690112  0.190708  0.056843  \n",
       " 11   -0.937455 -0.485158 -0.780538 -0.862399  2.690112  0.190708  0.056843  \n",
       " 12   -0.901818 -0.614100 -0.836439 -0.801411  0.787292  0.190708  0.056843  \n",
       " 13   -0.901818 -0.614100 -0.836439 -0.801411  0.787292  0.190708  0.056843  \n",
       " 14   -0.880727 -0.618738 -0.815735 -0.752520  0.209650  0.190708  0.056843  \n",
       " 15   -0.880727 -0.618738 -0.815735 -0.752520  0.209650  0.190708  0.056843  \n",
       " 16   -0.898909 -0.561224 -0.832298 -0.794859  0.226639  0.190708  0.056843  \n",
       " 17   -0.898909 -0.561224 -0.832298 -0.794859  0.226639  0.190708  0.056843  \n",
       " 18   -0.854545 -0.511132 -0.830228 -0.698589  0.073734  0.190708  0.056843  \n",
       " 19   -0.854545 -0.511132 -0.830228 -0.698589  0.073734  0.190708  0.056843  \n",
       " 20   -0.900364 -0.623377 -0.892340 -0.794859  0.175671  0.190708  0.056843  \n",
       " 21   -0.900364 -0.623377 -0.892340 -0.794859  0.175671  0.190708  0.056843  \n",
       " 22   -0.940364 -0.604824 -0.927536 -0.865927 -0.062181  0.190708  0.056843  \n",
       " 23   -0.940364 -0.604824 -0.927536 -0.865927 -0.062181  0.190708  0.056843  \n",
       " 24   -0.925091 -0.733766 -0.805383 -0.837702  0.821271  0.190708  0.056843  \n",
       " 25   -0.925091 -0.733766 -0.805383 -0.837702  0.821271  0.190708  0.056843  \n",
       " 26   -0.813818 -0.526902 -0.788820 -0.662298  0.583418  0.190708  0.056843  \n",
       " 27   -0.813818 -0.526902 -0.788820 -0.662298  0.583418  0.190708  0.056843  \n",
       " 28   -0.891636 -0.552876 -0.778468 -0.799395  0.617397  0.190708  0.056843  \n",
       " 29   -0.891636 -0.552876 -0.778468 -0.799395  0.617397  0.190708  0.056843  \n",
       " ...        ...       ...       ...       ...       ...       ...       ...  \n",
       " 1127 -0.648000 -0.561224 -0.817805 -0.817540  1.840639  1.539208  1.618022  \n",
       " 1128 -0.674909 -1.024119 -0.815735 -0.767641  0.243629  1.539208  1.618022  \n",
       " 1129 -0.674909 -1.024119 -0.815735 -0.767641  0.243629  1.539208  1.618022  \n",
       " 1130 -0.770182 -1.080705 -0.900621 -0.871976  1.398913  1.539208  1.618022  \n",
       " 1131 -0.770182 -1.080705 -0.900621 -0.871976  1.398913  1.539208  1.618022  \n",
       " 1132 -0.741818 -1.165121 -0.904762 -0.868952  1.025144  1.539208  1.618022  \n",
       " 1133 -0.741818 -1.165121 -0.904762 -0.868952  1.025144  1.539208  1.618022  \n",
       " 1134 -0.712727 -1.045455 -0.834369 -0.847782  1.840639  1.539208  1.618022  \n",
       " 1135 -0.712727 -1.045455 -0.834369 -0.847782  1.840639  1.539208  1.618022  \n",
       " 1136 -0.733818 -1.237477 -0.867495 -0.907762  1.144071  1.539208  1.618022  \n",
       " 1137 -0.733818 -1.237477 -0.867495 -0.907762  1.144071  1.539208  1.618022  \n",
       " 1138 -0.715636 -0.886827 -0.834369 -0.857863  0.906218  1.539208  1.618022  \n",
       " 1139 -0.715636 -0.886827 -0.834369 -0.857863  0.906218  1.539208  1.618022  \n",
       " 1140 -0.674182 -1.072356 -0.844720 -0.839718  3.335712  1.539208  1.618022  \n",
       " 1141 -0.674182 -1.072356 -0.844720 -0.839718  3.335712  1.539208  1.618022  \n",
       " 1142 -0.743273 -0.964750 -0.850932 -0.826109  1.008155  1.539208  1.618022  \n",
       " 1143 -0.743273 -0.964750 -0.850932 -0.826109  1.008155  1.539208  1.618022  \n",
       " 1144 -0.774545 -0.818182 -0.890269 -0.737903  2.995923  1.539208  1.618022  \n",
       " 1145 -0.774545 -0.818182 -0.890269 -0.737903  2.995923  1.539208  1.618022  \n",
       " 1146 -0.756364 -0.793135 -0.896480 -0.808468  0.362555  1.539208  1.618022  \n",
       " 1147 -0.756364 -0.793135 -0.896480 -0.808468  0.362555  1.539208  1.618022  \n",
       " 1148 -0.808000 -0.715213 -0.910973 -0.863407  0.821271  1.539208  1.618022  \n",
       " 1149 -0.808000 -0.715213 -0.910973 -0.863407  0.821271  1.539208  1.618022  \n",
       " 1150 -0.778182 -0.889610 -0.917184 -0.744456  1.313965  1.539208  1.618022  \n",
       " 1151 -0.778182 -0.889610 -0.917184 -0.744456  1.313965  1.539208  1.618022  \n",
       " 1152 -0.814545 -0.940631 -0.966874 -0.831653  1.772681  1.539208  1.618022  \n",
       " 1153 -0.814545 -0.940631 -0.966874 -0.831653  1.772681  1.539208  1.618022  \n",
       " 1154 -0.754182 -0.515770 -0.913043 -0.740423  2.944954  1.539208  1.618022  \n",
       " 1155 -0.754182 -0.515770 -0.913043 -0.740423  2.944954  1.539208  1.618022  \n",
       " 1156 -0.789091 -0.914657 -0.956522 -0.804435  1.178050  1.539208  1.618022  \n",
       " \n",
       " [1157 rows x 3593 columns]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split([df.drop('SOC', axis=1), df['SOC']], train_size=0.7)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MMM\\AppData\\Local\\conda\\conda\\envs\\python3_ittai\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(df.drop('SOC', axis=1), df['SOC'], train_size=0.7)\n",
    "error_train = []\n",
    "error_test = []\n",
    "for depth in range(2,12):\n",
    "    clf = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=depth)\n",
    "    clf.fit(df_train, target_train)\n",
    "    \n",
    "    error_train.append(mean_squared_error(clf.predict(df_train), target_train))\n",
    "    error_test.append(mean_squared_error(clf.predict(df_test), target_test))\n",
    "    \n",
    "    print(depth)\n",
    "    \n",
    "plt.plot(error_train)\n",
    "plt.plot(error_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04823649883608305\n",
      "0.27609763746808547\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf.fit(df_train, target_train)\n",
    "print(mean_squared_error(clf.predict(df_train), target_train))\n",
    "print(mean_squared_error(clf.predict(df_test), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Use model.feature_importances_ to select the 100 most important features. Use these features to train your model. Did you get better results on the train segment? what are results on the test segment?\n",
    "How does model.feature_importances_ work? Some features have 0 importance. Why?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04902182738595473\n",
      "0.2620029368672982\n"
     ]
    }
   ],
   "source": [
    "features = [x[0] for x in sorted(zip(df.columns, clf.feature_importances_), key = lambda x: -x[1])][:100]\n",
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[features], target_train)\n",
    "print(mean_squared_error(clf_selected.predict(df_train[features]), target_train))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[features]), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Well, that didn't work quite so good. Let's try a different approach. Now select the 100 features that are most correlated to the data. What results did you get now? You got so far two groups of feature selected. Are they similar to one another? Why or why not?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = np.corrcoef(df_train.transpose(), target_train)\n",
    "corr_features = sorted(list(zip(list(df.columns)+['SOC'], corrs[-1])), key = lambda x: -x[1])[1:]\n",
    "corr_features = [x[0] for x in corr_features[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17848483823383735\n",
      "0.6757140666780279\n"
     ]
    }
   ],
   "source": [
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[corr_features], target_train)\n",
    "print(mean_squared_error(clf_selected.predict(df_train[corr_features]), target_train))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[corr_features]), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now cluster the features by their correlation to one another. Make sure you have at least ~30 clusters. Pick from each cluster the feature most correlated to the target. What are your results now? What clustering algorithm did you use?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clusters = DBSCAN(eps = 0.001, metric=\"precomputed\").fit_predict(1-abs(corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(x):\n",
    "    return max(x, key = lambda x: -x[0])[1]\n",
    "\n",
    "choose_features = pd.DataFrame(list(zip(clusters, zip(corrs[-1], list(df.columns)+['SOC'])))).iloc[:-1]\n",
    "features_selected_corr = choose_features.groupby(0).agg(choose)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058263596784614964\n",
      "0.2595219874849178\n"
     ]
    }
   ],
   "source": [
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[features_selected_corr], target_train)\n",
    "print(mean_squared_error(clf_selected.predict(df_train[features_selected_corr]), target_train))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[features_selected_corr]), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Before we continue, read the documentation of sklearn.feature_selection. What other feature selection algorithms did you find there? try one of them on your data.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```You are about to be very surprised. select 100 features randomly and evaluate the model trained with them. Repeat it 100 times. What is the best set of feature you got? How is it compared to the other subsets you got so far? Can you explain it?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05651201537665083\n",
      "0.2698342353126435\n",
      "-----------------------------------\n",
      "0.06652265540084935\n",
      "0.2666013022298534\n",
      "-----------------------------------\n",
      "0.10332783741087836\n",
      "0.47793577733448994\n",
      "-----------------------------------\n",
      "0.11216290703690901\n",
      "0.5686818617113927\n",
      "-----------------------------------\n",
      "0.09932412562193454\n",
      "0.49135344273392534\n",
      "-----------------------------------\n",
      "0.08133461822503883\n",
      "0.3487035946388528\n",
      "-----------------------------------\n",
      "0.0723437052826911\n",
      "0.29926554381640874\n",
      "-----------------------------------\n",
      "0.10946440971540161\n",
      "0.4258561770824264\n",
      "-----------------------------------\n",
      "0.10787523452059077\n",
      "0.42030279794374464\n",
      "-----------------------------------\n",
      "0.05375622421113419\n",
      "0.2319850945474299\n",
      "-----------------------------------\n",
      "0.08447351736095826\n",
      "0.3557712773508264\n",
      "-----------------------------------\n",
      "0.1252398108785685\n",
      "0.5470485854819532\n",
      "-----------------------------------\n",
      "0.0635738885662912\n",
      "0.2451021495574238\n",
      "-----------------------------------\n",
      "0.08012142393384909\n",
      "0.3152004803611018\n",
      "-----------------------------------\n",
      "0.09367648343899798\n",
      "0.4946858820777773\n",
      "-----------------------------------\n",
      "0.07118078603681985\n",
      "0.33781847878779714\n",
      "-----------------------------------\n",
      "0.08358505476672133\n",
      "0.3615873833843326\n",
      "-----------------------------------\n",
      "0.11582879680174447\n",
      "0.6077981545893587\n",
      "-----------------------------------\n",
      "0.06462979018584084\n",
      "0.2545043185177138\n",
      "-----------------------------------\n",
      "0.11729238556946346\n",
      "0.4815635413366203\n",
      "-----------------------------------\n",
      "0.06932852516983917\n",
      "0.2758069878053279\n",
      "-----------------------------------\n",
      "0.04945871761133959\n",
      "0.22591933437471842\n",
      "-----------------------------------\n",
      "0.07944788777294268\n",
      "0.3674093454128088\n",
      "-----------------------------------\n",
      "0.08704531066462946\n",
      "0.3879652293473526\n",
      "-----------------------------------\n",
      "0.13683299830229922\n",
      "0.5024748530129989\n",
      "-----------------------------------\n",
      "0.060374914251465885\n",
      "0.25745548602873153\n",
      "-----------------------------------\n",
      "0.09693176820697913\n",
      "0.4501005732180906\n",
      "-----------------------------------\n",
      "0.12080016394771576\n",
      "0.6024432925070247\n",
      "-----------------------------------\n",
      "0.13001584738071303\n",
      "0.493359657595701\n",
      "-----------------------------------\n",
      "0.0830988257289471\n",
      "0.37132911787570716\n",
      "-----------------------------------\n",
      "0.06690435810804182\n",
      "0.27933634643525623\n",
      "-----------------------------------\n",
      "0.08480650077352865\n",
      "0.36976377571836394\n",
      "-----------------------------------\n",
      "0.07694064460691699\n",
      "0.384067098702139\n",
      "-----------------------------------\n",
      "0.060557257109304666\n",
      "0.2698948606833084\n",
      "-----------------------------------\n",
      "0.06125687862148037\n",
      "0.28581985710246255\n",
      "-----------------------------------\n",
      "0.10760651652954338\n",
      "0.4523156876682521\n",
      "-----------------------------------\n",
      "0.08053677391153269\n",
      "0.3735934597769926\n",
      "-----------------------------------\n",
      "0.0704077119200256\n",
      "0.33902245083897137\n",
      "-----------------------------------\n",
      "0.08376986405793174\n",
      "0.3496266687353486\n",
      "-----------------------------------\n",
      "0.1066339338925421\n",
      "0.5792113953865922\n",
      "-----------------------------------\n",
      "0.10652289339591373\n",
      "0.4850182370233064\n",
      "-----------------------------------\n",
      "0.06203526752068253\n",
      "0.2351314307565073\n",
      "-----------------------------------\n",
      "0.06395778906486041\n",
      "0.32426746039703286\n",
      "-----------------------------------\n",
      "0.10533717147876678\n",
      "0.5298665418796782\n",
      "-----------------------------------\n",
      "0.076172341752183\n",
      "0.29872398588576926\n",
      "-----------------------------------\n",
      "0.05406709059941195\n",
      "0.21183411553355636\n",
      "-----------------------------------\n",
      "0.06829428667289232\n",
      "0.2913206768166426\n",
      "-----------------------------------\n",
      "0.08337734962614197\n",
      "0.38374424110083716\n",
      "-----------------------------------\n",
      "0.0999762602008383\n",
      "0.44892799504118847\n",
      "-----------------------------------\n",
      "0.0682192941868657\n",
      "0.30081228520176256\n",
      "-----------------------------------\n",
      "0.08988240635744611\n",
      "0.37404748160964285\n",
      "-----------------------------------\n",
      "0.114750234490799\n",
      "0.44620015692189224\n",
      "-----------------------------------\n",
      "0.07234510240033201\n",
      "0.3703432460309\n",
      "-----------------------------------\n",
      "0.056781714169861316\n",
      "0.27816224983487914\n",
      "-----------------------------------\n",
      "0.06299582788838605\n",
      "0.2707484651779029\n",
      "-----------------------------------\n",
      "0.09514796183690324\n",
      "0.4231193461027271\n",
      "-----------------------------------\n",
      "0.09090794367991611\n",
      "0.4248539427944272\n",
      "-----------------------------------\n",
      "0.07243675377284135\n",
      "0.32159197798555395\n",
      "-----------------------------------\n",
      "0.11580452333680465\n",
      "0.41825315202326047\n",
      "-----------------------------------\n",
      "0.05983658819177032\n",
      "0.27910403591864275\n",
      "-----------------------------------\n",
      "0.0777208619913438\n",
      "0.3283488968807353\n",
      "-----------------------------------\n",
      "0.0617661978052004\n",
      "0.2734720727016348\n",
      "-----------------------------------\n",
      "0.07211915918873535\n",
      "0.2948931787459552\n",
      "-----------------------------------\n",
      "0.09599377707932662\n",
      "0.4225760313479238\n",
      "-----------------------------------\n",
      "0.07572892833389472\n",
      "0.35210386446198205\n",
      "-----------------------------------\n",
      "0.06676896695200618\n",
      "0.30791970239258776\n",
      "-----------------------------------\n",
      "0.09971604386338728\n",
      "0.44017133822181964\n",
      "-----------------------------------\n",
      "0.06690460363856608\n",
      "0.3137522506329635\n",
      "-----------------------------------\n",
      "0.05718288676075285\n",
      "0.26531822995129023\n",
      "-----------------------------------\n",
      "0.06953311559691537\n",
      "0.2885298169051439\n",
      "-----------------------------------\n",
      "0.06276644013548242\n",
      "0.31166918969508806\n",
      "-----------------------------------\n",
      "0.07619305955171359\n",
      "0.3299177068743035\n",
      "-----------------------------------\n",
      "0.06958191748671126\n",
      "0.3000666719795695\n",
      "-----------------------------------\n",
      "0.08065716059698085\n",
      "0.3465393196277889\n",
      "-----------------------------------\n",
      "0.06383949416653291\n",
      "0.2601993362540208\n",
      "-----------------------------------\n",
      "0.07054765090942784\n",
      "0.2993437127290092\n",
      "-----------------------------------\n",
      "0.0861524178049634\n",
      "0.33173883841165996\n",
      "-----------------------------------\n",
      "0.06339288858984479\n",
      "0.31925373280244634\n",
      "-----------------------------------\n",
      "0.09693005772666213\n",
      "0.4513014883528657\n",
      "-----------------------------------\n",
      "0.0978338649677409\n",
      "0.41448492452459124\n",
      "-----------------------------------\n",
      "0.14139329632471498\n",
      "0.49100761825337025\n",
      "-----------------------------------\n",
      "0.058781006417879436\n",
      "0.32436177407645506\n",
      "-----------------------------------\n",
      "0.06551089418661361\n",
      "0.33245770959175497\n",
      "-----------------------------------\n",
      "0.12684298755044093\n",
      "0.4731737351729969\n",
      "-----------------------------------\n",
      "0.05661905222297133\n",
      "0.25631976271285817\n",
      "-----------------------------------\n",
      "0.06108678049949009\n",
      "0.24222747963433366\n",
      "-----------------------------------\n",
      "0.09116770044133696\n",
      "0.3544499307851531\n",
      "-----------------------------------\n",
      "0.0873430970311486\n",
      "0.3878297709337726\n",
      "-----------------------------------\n",
      "0.1164741843521473\n",
      "0.5219943849646839\n",
      "-----------------------------------\n",
      "0.11607491544919396\n",
      "0.4437125441676963\n",
      "-----------------------------------\n",
      "0.05419549790572726\n",
      "0.23556641625473937\n",
      "-----------------------------------\n",
      "0.06911932285773534\n",
      "0.3046412207725009\n",
      "-----------------------------------\n",
      "0.057319279911725106\n",
      "0.24283213195682563\n",
      "-----------------------------------\n",
      "0.11614973352602449\n",
      "0.5912355967883028\n",
      "-----------------------------------\n",
      "0.06655397046180826\n",
      "0.38462691628332274\n",
      "-----------------------------------\n",
      "0.067914546981651\n",
      "0.3104429249837125\n",
      "-----------------------------------\n",
      "0.12567846157138618\n",
      "0.48351324187780653\n",
      "-----------------------------------\n",
      "0.0753016713274264\n",
      "0.33560683248078443\n",
      "-----------------------------------\n",
      "0.0638074427598677\n",
      "0.272266782686066\n",
      "-----------------------------------\n",
      "0.07425085369567751\n",
      "0.3479504924515637\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "for i in range(100):\n",
    "    features_random = list(set(np.random.choice(df_train.columns, size = 30)))\n",
    "    clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "    clf_selected.fit(df_train[features_random], target_train)\n",
    "    print(mean_squared_error(clf_selected.predict(df_train[features_random]), target_train))\n",
    "    print(mean_squared_error(clf_selected.predict(df_test[features_random]), target_test))\n",
    "    print('-----------------------------------')\n",
    "    features_list.append((features_random,\n",
    "                        mean_squared_error(clf_selected.predict(df_train[features_random]), target_train),\n",
    "                        mean_squared_error(clf_selected.predict(df_test[features_random]), target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05723467980698782\n",
      "0.22831512116944946\n"
     ]
    }
   ],
   "source": [
    "features_rand_selected = min(features_list, key = lambda x: x[2])[0]\n",
    "clf_selected = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=7)\n",
    "clf_selected.fit(df_train[features_rand_selected], target_train)\n",
    "print(mean_squared_error(clf_selected.predict(df_train[features_rand_selected]), target_train))\n",
    "print(mean_squared_error(clf_selected.predict(df_test[features_rand_selected]), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now it's your time to enter the office's competition for feature selection. Invent your own feature selection algorithm. Make it a good one (because if it isn't, your tutor might ask you to implement one of the other algorithms proposed by the office's members, and that might be awful ;) ) Before you act, talk about your idea with your tutor.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
