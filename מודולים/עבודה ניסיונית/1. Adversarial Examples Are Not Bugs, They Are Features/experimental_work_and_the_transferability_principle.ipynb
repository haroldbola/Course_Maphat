{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "experimental work and the transferability principle.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxAra_wOqpBQ"
      },
      "source": [
        "## Experimental Work and The Transferability Principle\n",
        "```In this exercise you will experience with real experimental work and will meet a very interesting issue - adversarial AI and the transferability principle. You will define experiments and measures for success, and will execute those experiments. It is of great importance that you will discuss this exercise with your tutor, even during the work on the exercise.```\n",
        "\n",
        "```~Ohad Amosi & Ittai Haran```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsROUXTNqpBV"
      },
      "source": [
        "```Read the paper \"Adversarial Examples Are Not Bugs, They Are Features\", which you can find in this exercise directory. Read it thoroughly. Make sure you understand how the datasets of experiments #1 and #2 were generated. As you might tell, the paper reports very interesting results.```\n",
        "\n",
        "```If you are not familiar with the concept of model distillation, read about it. There arise the question, how can we be sure that the \"non-robust\" features described in the paper are real? Could it be that the effect the paper measures is a model distillation? Think about it: In both the experiments the authors used a trained robust network to label images, on which another network was trained normally. Could it be that the robust network was somehow \"leaked\"? Think about this possibility, and how it could have happen in experiment #1 and experiment #2.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmNWt916qpBW"
      },
      "source": [
        "## Answer\n",
        "\n",
        "In distillation training, one model is trained to predict the output probabilities of another model that was trained on an earlier, baseline standard to emphasize accuracy.\n",
        "\n",
        "In the case of the first experiment, robust model is used to create a robust dataset.\n",
        "We want to replace each sample x in the original dataset by a sample x_r. x and x_r must be very close in the latent space of a robust model.\n",
        "We use only the representation layer of the robust model, not the output probabilities and then it is not a distillation training. Furthermore, the new sample x_r does not lie in the latent sapce, but in the input space\n",
        "\n",
        "Experiment #2 does not use a robust model, but creates advresarial example to show that good classification can be achieved only based on non-robust features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMNkNZMiqpBW"
      },
      "source": [
        "```All in all, we will examine two conjectures:```\n",
        "1. ```The paper is great, non-robust features are the real thing and different networks use the same features.```\n",
        "2. ```The paper is wrong, it's only a fancy way of network distilling.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrjOUvtPqpBX"
      },
      "source": [
        "```Let's first explore the concept of distilling networks. Can we do it in any case? Can you distill a network using only, for example, white noise? Can you do it using only the predictions? Or do you maybe need to use the logits of the network? Answer this question. MNIST might help you with that.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_98iH7IqpBX"
      },
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Layer\n",
        "import tensorflow as tf"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf4uG7lzqpBY"
      },
      "source": [
        "class DivideLayer(Layer):\n",
        "    def __init__(self, temperature=1, **kwargs):\n",
        "        self.temperature = temperature\n",
        "        super(DivideLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            return  inputs / self.temperature\n",
        "        return inputs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf2le7lGqpBY"
      },
      "source": [
        "def train_model(x_train, y_train, input_shape, num_classes, temperature=1):\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=input_shape),\n",
        "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Flatten(),\n",
        "            layers.Dropout(0.5),\n",
        "            DivideLayer(temperature),\n",
        "            layers.Dense(num_classes, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=3)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "#     model.summary()\n",
        "    model.fit(x_train, y_train, batch_size=256, epochs=15, validation_split=0.1, callbacks=[es])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPDFh26XqyDS"
      },
      "source": [
        "def TestAttack(model, adv_images, orig_images, true_labels, target_labels=None, targeted=False):\n",
        "    adv_images = adv_images.numpy()\n",
        "    score = model.evaluate(adv_images, true_labels, verbose=0)\n",
        "    \n",
        "    print('Test loss: {:.2f}'.format(score[0]))\n",
        "    print('Successfully moved out of source class: {:.2f}'.format( 1 - score[1]))\n",
        "    \n",
        "    if targeted:\n",
        "        score = model.evaluate(adv_images, target, verbose=0)\n",
        "        print('Test loss: {:.2f}'.format(score[0]))\n",
        "        print('Successfully perturbed to target class: {:.2f}'.format(score[1]))\n",
        "    \n",
        "    dist = np.mean(np.sqrt(np.mean(np.square(adv_images - orig_images), axis=(1,2,3))))\n",
        "    print('Mean perturbation distance: {:.2f}'.format(dist))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnoBkO4Pq0xC"
      },
      "source": [
        "def FastGradientSignMethod(model, input_image, input_label, eps=0.3):\n",
        "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(input_image)\n",
        "        prediction = model(input_image)\n",
        "        loss = loss_object(input_label, prediction)\n",
        "\n",
        "    gradient = tape.gradient(loss, input_image)\n",
        "    signed_grad = tf.sign(gradient)\n",
        "    adv_x = input_image + eps*signed_grad\n",
        "    \n",
        "    return adv_x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK5lG8riqvKL"
      },
      "source": [
        "def attack_model(test_images, test_labels, model, eps=0.3):\n",
        "  test_images = tf.convert_to_tensor(test_images)\n",
        "\n",
        "  adv_images = FastGradientSignMethod(model, test_images, test_labels, eps=eps)\n",
        "  TestAttack(model, adv_images, test_images.numpy(), test_labels, targeted=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atp3ixO5rQvJ"
      },
      "source": [
        "def train_distillation(temperature=1, use_proba=True, white_noise_input=False):\n",
        "    num_classes = 10\n",
        "    input_shape = (28, 28, 1)\n",
        "    \n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    x_train = x_train.astype(\"float32\") / 255\n",
        "    x_test = x_test.astype(\"float32\") / 255\n",
        "    x_train = np.expand_dims(x_train, -1)\n",
        "    x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "    print(\"Training teacher model\")\n",
        "    teacher = train_model(x_train, y_train, input_shape, num_classes, temperature=temperature)\n",
        "    \n",
        "    _, acc = teacher.evaluate(x_test, y_test, batch_size=512)\n",
        "    print('Teacher test accuracy:', acc)\n",
        "\n",
        "    if use_proba is True:\n",
        "      train_probas = teacher.predict(x_train)\n",
        "    else:\n",
        "      train_probas = teacher.predict(x_train)\n",
        "      print(train_probas[0, :])\n",
        "      train_probas = np.argmax(train_probas, axis=1)\n",
        "      train_probas = tf.one_hot(train_probas, depth=num_classes)\n",
        "      print(train_probas[0, :])\n",
        "\n",
        "    if white_noise_input is True:\n",
        "      x_train = np.random.normal(size=x_train.shape)\n",
        "\n",
        "    print(\"Training student model\")\n",
        "    student = train_model(x_train, train_probas, input_shape, num_classes, temperature=temperature)\n",
        "    \n",
        "    _, acc = student.evaluate(x_test, y_test, batch_size=512)\n",
        "    print('Student test accuracy:', acc)\n",
        "\n",
        "    print('Attacking teacher model')\n",
        "    attack_model(x_test, y_test, teacher, eps=0.3)\n",
        "    print('Attacking student model')\n",
        "    attack_model(x_test, y_test, student, eps=0.3)\n",
        "\n",
        "    return teacher, student"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "997p2hd1rIZb",
        "outputId": "74ae89cb-e7df-40da-cbbd-eb06affaed0e"
      },
      "source": [
        "teacher, student = train_distillation(temperature=10, use_proba=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training teacher model\n",
            "Epoch 1/15\n",
            "211/211 [==============================] - 4s 14ms/step - loss: 1.3754 - accuracy: 0.6046 - val_loss: 0.7729 - val_accuracy: 0.9508\n",
            "Epoch 2/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.2553 - accuracy: 0.9234 - val_loss: 0.5619 - val_accuracy: 0.9682\n",
            "Epoch 3/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1807 - accuracy: 0.9463 - val_loss: 0.4765 - val_accuracy: 0.9753\n",
            "Epoch 4/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.1482 - accuracy: 0.9556 - val_loss: 0.4378 - val_accuracy: 0.9778\n",
            "Epoch 5/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.1249 - accuracy: 0.9623 - val_loss: 0.4146 - val_accuracy: 0.9787\n",
            "Epoch 6/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.1166 - accuracy: 0.9633 - val_loss: 0.3762 - val_accuracy: 0.9808\n",
            "Epoch 7/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.1009 - accuracy: 0.9705 - val_loss: 0.3380 - val_accuracy: 0.9827\n",
            "Epoch 8/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0915 - accuracy: 0.9722 - val_loss: 0.2989 - val_accuracy: 0.9858\n",
            "Epoch 9/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0858 - accuracy: 0.9728 - val_loss: 0.3006 - val_accuracy: 0.9865\n",
            "Epoch 10/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0817 - accuracy: 0.9753 - val_loss: 0.2995 - val_accuracy: 0.9862\n",
            "Epoch 11/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0788 - accuracy: 0.9754 - val_loss: 0.2934 - val_accuracy: 0.9880\n",
            "Epoch 12/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0784 - accuracy: 0.9755 - val_loss: 0.2598 - val_accuracy: 0.9880\n",
            "Epoch 13/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0699 - accuracy: 0.9784 - val_loss: 0.2941 - val_accuracy: 0.9862\n",
            "Epoch 14/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0664 - accuracy: 0.9790 - val_loss: 0.2507 - val_accuracy: 0.9897\n",
            "Epoch 15/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0645 - accuracy: 0.9805 - val_loss: 0.2647 - val_accuracy: 0.9878\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2330 - accuracy: 0.9860\n",
            "Teacher test accuracy: 0.9860000014305115\n",
            "Training student model\n",
            "Epoch 1/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 1.3529 - accuracy: 0.6044 - val_loss: 0.5452 - val_accuracy: 0.9527\n",
            "Epoch 2/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.2235 - accuracy: 0.9286 - val_loss: 0.3314 - val_accuracy: 0.9735\n",
            "Epoch 3/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.1498 - accuracy: 0.9503 - val_loss: 0.2312 - val_accuracy: 0.9798\n",
            "Epoch 4/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.1136 - accuracy: 0.9630 - val_loss: 0.1934 - val_accuracy: 0.9837\n",
            "Epoch 5/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0911 - accuracy: 0.9696 - val_loss: 0.1649 - val_accuracy: 0.9857\n",
            "Epoch 6/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0766 - accuracy: 0.9751 - val_loss: 0.1362 - val_accuracy: 0.9873\n",
            "Epoch 7/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0689 - accuracy: 0.9770 - val_loss: 0.1040 - val_accuracy: 0.9897\n",
            "Epoch 8/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0588 - accuracy: 0.9804 - val_loss: 0.1020 - val_accuracy: 0.9895\n",
            "Epoch 9/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0523 - accuracy: 0.9822 - val_loss: 0.0730 - val_accuracy: 0.9913\n",
            "Epoch 10/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0479 - accuracy: 0.9840 - val_loss: 0.0650 - val_accuracy: 0.9915\n",
            "Epoch 11/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0461 - accuracy: 0.9835 - val_loss: 0.0554 - val_accuracy: 0.9922\n",
            "Epoch 12/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0442 - accuracy: 0.9843 - val_loss: 0.0618 - val_accuracy: 0.9935\n",
            "Epoch 13/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0409 - accuracy: 0.9858 - val_loss: 0.0530 - val_accuracy: 0.9928\n",
            "Epoch 14/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0387 - accuracy: 0.9865 - val_loss: 0.0499 - val_accuracy: 0.9937\n",
            "Epoch 15/15\n",
            "211/211 [==============================] - 3s 12ms/step - loss: 0.0374 - accuracy: 0.9859 - val_loss: 0.0585 - val_accuracy: 0.9938\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3443 - accuracy: 0.9851\n",
            "Student test accuracy: 0.9850999712944031\n",
            "Attacking teacher model\n",
            "Test loss: 23.87\n",
            "Successfully moved out of source class: 0.36\n",
            "Mean perturbation distance: 0.11\n",
            "Attacking student model\n",
            "Test loss: 20.95\n",
            "Successfully moved out of source class: 0.22\n",
            "Mean perturbation distance: 0.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0wlPjoC-1L8",
        "outputId": "be6d443b-486e-47d8-f67e-763eeb456a75"
      },
      "source": [
        "teacher, student = train_distillation(temperature=10, use_proba=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training teacher model\n",
            "Epoch 1/15\n",
            "211/211 [==============================] - 3s 14ms/step - loss: 1.4108 - accuracy: 0.5956 - val_loss: 0.7853 - val_accuracy: 0.9508\n",
            "Epoch 2/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.2501 - accuracy: 0.9278 - val_loss: 0.5431 - val_accuracy: 0.9707\n",
            "Epoch 3/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1689 - accuracy: 0.9509 - val_loss: 0.4625 - val_accuracy: 0.9770\n",
            "Epoch 4/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1375 - accuracy: 0.9594 - val_loss: 0.4091 - val_accuracy: 0.9792\n",
            "Epoch 5/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1164 - accuracy: 0.9648 - val_loss: 0.3756 - val_accuracy: 0.9822\n",
            "Epoch 6/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1067 - accuracy: 0.9686 - val_loss: 0.3469 - val_accuracy: 0.9830\n",
            "Epoch 7/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0944 - accuracy: 0.9714 - val_loss: 0.2977 - val_accuracy: 0.9855\n",
            "Epoch 8/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0808 - accuracy: 0.9760 - val_loss: 0.3095 - val_accuracy: 0.9860\n",
            "Epoch 9/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0788 - accuracy: 0.9763 - val_loss: 0.2833 - val_accuracy: 0.9873\n",
            "Epoch 10/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0771 - accuracy: 0.9768 - val_loss: 0.2690 - val_accuracy: 0.9878\n",
            "Epoch 11/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0664 - accuracy: 0.9798 - val_loss: 0.2838 - val_accuracy: 0.9873\n",
            "Epoch 12/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 0.2450 - val_accuracy: 0.9890\n",
            "Epoch 13/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0620 - accuracy: 0.9812 - val_loss: 0.2512 - val_accuracy: 0.9888\n",
            "Epoch 14/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0592 - accuracy: 0.9822 - val_loss: 0.2261 - val_accuracy: 0.9903\n",
            "Epoch 15/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0577 - accuracy: 0.9816 - val_loss: 0.2280 - val_accuracy: 0.9903\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1843 - accuracy: 0.9878\n",
            "Teacher test accuracy: 0.9878000020980835\n",
            "[0.000000e+00 0.000000e+00 0.000000e+00 4.972901e-16 0.000000e+00\n",
            " 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
            "tf.Tensor([0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n",
            "Training student model\n",
            "Epoch 1/15\n",
            "211/211 [==============================] - 3s 14ms/step - loss: 1.3997 - accuracy: 0.5620 - val_loss: 0.5761 - val_accuracy: 0.9520\n",
            "Epoch 2/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.2290 - accuracy: 0.9287 - val_loss: 0.3932 - val_accuracy: 0.9727\n",
            "Epoch 3/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1525 - accuracy: 0.9522 - val_loss: 0.2792 - val_accuracy: 0.9787\n",
            "Epoch 4/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1160 - accuracy: 0.9615 - val_loss: 0.2192 - val_accuracy: 0.9830\n",
            "Epoch 5/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0986 - accuracy: 0.9677 - val_loss: 0.1959 - val_accuracy: 0.9845\n",
            "Epoch 6/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0876 - accuracy: 0.9706 - val_loss: 0.1569 - val_accuracy: 0.9863\n",
            "Epoch 7/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0740 - accuracy: 0.9751 - val_loss: 0.1316 - val_accuracy: 0.9880\n",
            "Epoch 8/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0690 - accuracy: 0.9761 - val_loss: 0.1325 - val_accuracy: 0.9872\n",
            "Epoch 9/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0623 - accuracy: 0.9793 - val_loss: 0.0989 - val_accuracy: 0.9902\n",
            "Epoch 10/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0557 - accuracy: 0.9813 - val_loss: 0.1170 - val_accuracy: 0.9900\n",
            "Epoch 11/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0511 - accuracy: 0.9825 - val_loss: 0.1015 - val_accuracy: 0.9912\n",
            "Epoch 12/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.1056 - val_accuracy: 0.9917\n",
            "Epoch 13/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0482 - accuracy: 0.9834 - val_loss: 0.0932 - val_accuracy: 0.9923\n",
            "Epoch 14/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0427 - accuracy: 0.9845 - val_loss: 0.0688 - val_accuracy: 0.9933\n",
            "Epoch 15/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0412 - accuracy: 0.9855 - val_loss: 0.0626 - val_accuracy: 0.9937\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3313 - accuracy: 0.9840\n",
            "Student test accuracy: 0.984000027179718\n",
            "Attacking teacher model\n",
            "Test loss: 24.21\n",
            "Successfully moved out of source class: 0.34\n",
            "Mean perturbation distance: 0.11\n",
            "Attacking student model\n",
            "Test loss: 21.91\n",
            "Successfully moved out of source class: 0.24\n",
            "Mean perturbation distance: 0.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1ApDN5iBpln",
        "outputId": "3e64264a-7ae6-4ca2-bcea-2219b20324ce"
      },
      "source": [
        "teacher, student = train_distillation(temperature=10, use_proba=True, white_noise_input=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training teacher model\n",
            "Epoch 1/15\n",
            "211/211 [==============================] - 4s 14ms/step - loss: 1.4245 - accuracy: 0.5812 - val_loss: 0.8294 - val_accuracy: 0.9455\n",
            "Epoch 2/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.2729 - accuracy: 0.9210 - val_loss: 0.6061 - val_accuracy: 0.9662\n",
            "Epoch 3/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1865 - accuracy: 0.9436 - val_loss: 0.5152 - val_accuracy: 0.9720\n",
            "Epoch 4/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1464 - accuracy: 0.9571 - val_loss: 0.4520 - val_accuracy: 0.9765\n",
            "Epoch 5/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1251 - accuracy: 0.9621 - val_loss: 0.4054 - val_accuracy: 0.9808\n",
            "Epoch 6/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1143 - accuracy: 0.9661 - val_loss: 0.3943 - val_accuracy: 0.9805\n",
            "Epoch 7/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0993 - accuracy: 0.9699 - val_loss: 0.3717 - val_accuracy: 0.9828\n",
            "Epoch 8/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0944 - accuracy: 0.9717 - val_loss: 0.3438 - val_accuracy: 0.9843\n",
            "Epoch 9/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0859 - accuracy: 0.9738 - val_loss: 0.3269 - val_accuracy: 0.9862\n",
            "Epoch 10/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0843 - accuracy: 0.9734 - val_loss: 0.3127 - val_accuracy: 0.9860\n",
            "Epoch 11/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0747 - accuracy: 0.9777 - val_loss: 0.2765 - val_accuracy: 0.9863\n",
            "Epoch 12/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0724 - accuracy: 0.9779 - val_loss: 0.2809 - val_accuracy: 0.9873\n",
            "Epoch 13/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.2731 - val_accuracy: 0.9883\n",
            "Epoch 14/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0623 - accuracy: 0.9812 - val_loss: 0.2671 - val_accuracy: 0.9883\n",
            "Epoch 15/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.0630 - accuracy: 0.9801 - val_loss: 0.2659 - val_accuracy: 0.9883\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2276 - accuracy: 0.9863\n",
            "Teacher test accuracy: 0.986299991607666\n",
            "before (60000, 28, 28, 1)\n",
            "after (60000, 28, 28, 1)\n",
            "Training student model\n",
            "Epoch 1/15\n",
            "211/211 [==============================] - 3s 14ms/step - loss: 2.3020 - accuracy: 0.1083 - val_loss: 2.3246 - val_accuracy: 0.1065\n",
            "Epoch 2/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 2.3011 - accuracy: 0.1131 - val_loss: 2.3306 - val_accuracy: 0.1083\n",
            "Epoch 3/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 2.3009 - accuracy: 0.1132 - val_loss: 2.3165 - val_accuracy: 0.1065\n",
            "Epoch 4/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 2.3009 - accuracy: 0.1138 - val_loss: 2.3268 - val_accuracy: 0.1065\n",
            "Epoch 5/15\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 2.3007 - accuracy: 0.1138 - val_loss: 2.3528 - val_accuracy: 0.1065\n",
            "Epoch 00005: early stopping\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 2.3383 - accuracy: 0.1135\n",
            "Student test accuracy: 0.11349999904632568\n",
            "Attacking teacher model\n",
            "Test loss: 23.40\n",
            "Successfully moved out of source class: 0.37\n",
            "Mean perturbation distance: 0.12\n",
            "Attacking student model\n",
            "Test loss: 2.51\n",
            "Successfully moved out of source class: 0.89\n",
            "Mean perturbation distance: 0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzyY4lx1qpBZ"
      },
      "source": [
        "```Distilling using only white noise is a though question. Why is it? Think about the concepts of distribution and out-of-distribution in when answering this.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXBSUcNHqpBa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "solution": "hidden",
        "solution_first": true,
        "id": "FnC14lF9qpBa"
      },
      "source": [
        "```Assuming conjecture #2 and regarding experiment #1, the distillation isn't happening on white noise, but not on real images either. Think of an experiment that will help you decide if the phenomenon the authors encountered is indeed just network distillation. Think how to measure your success. Open the hint only if you can't think of a way do to it.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "solution": "hidden",
        "id": "ZBmBIcIoqpBa"
      },
      "source": [
        "```You can, for example, take a dataset such as CIFAR10, train a network on 5 classes and try to distill it using the other 5 classes. That way you are using real images, but from different distributions.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JIEJxDQqpBa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ataDtPbuqpBb"
      },
      "source": [
        "```Let's focus now on conjecture #1. Conduct an experiment that will demonstrate that two different networks indeed use the same features. Do it gradually: Start with two copies of the same architecture and on the same data, and move to different architectures and different subsets of the data (but from the same distribution). Think how to measure your success.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyOB6OXWqpBb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}