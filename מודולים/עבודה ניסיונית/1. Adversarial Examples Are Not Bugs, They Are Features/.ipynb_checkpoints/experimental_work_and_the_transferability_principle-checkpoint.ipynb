{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxAra_wOqpBQ"
   },
   "source": [
    "## Experimental Work and The Transferability Principle\n",
    "```In this exercise you will experience with real experimental work and will meet a very interesting issue - adversarial AI and the transferability principle. You will define experiments and measures for success, and will execute those experiments. It is of great importance that you will discuss this exercise with your tutor, even during the work on the exercise.```\n",
    "\n",
    "```~Ohad Amosi & Ittai Haran```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsROUXTNqpBV"
   },
   "source": [
    "```Read the paper \"Adversarial Examples Are Not Bugs, They Are Features\", which you can find in this exercise directory. Read it thoroughly. Make sure you understand how the datasets of experiments #1 and #2 were generated. As you might tell, the paper reports very interesting results.```\n",
    "\n",
    "```If you are not familiar with the concept of model distillation, read about it. There arise the question, how can we be sure that the \"non-robust\" features described in the paper are real? Could it be that the effect the paper measures is a model distillation? Think about it: In both the experiments the authors used a trained robust network to label images, on which another network was trained normally. Could it be that the robust network was somehow \"leaked\"? Think about this possibility, and how it could have happen in experiment #1 and experiment #2.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmNWt916qpBW"
   },
   "source": [
    "## Answer\n",
    "\n",
    "In distillation training, one model is trained to predict the output probabilities of another model that was trained on an earlier, baseline standard to emphasize accuracy.\n",
    "\n",
    "In the case of the first experiment, robust model is used to create a robust dataset.\n",
    "We want to replace each sample x in the original dataset by a sample x_r. x and x_r must be very close in the latent space of a robust model.\n",
    "We use only the representation layer of the robust model, not the output probabilities and then it is not a distillation training. Furthermore, the new sample x_r does not lie in the latent sapce, but in the input space\n",
    "\n",
    "Experiment #2 does not use a robust model, but creates advresarial example to show that good classification can be achieved only based on non-robust features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMNkNZMiqpBW"
   },
   "source": [
    "```All in all, we will examine two conjectures:```\n",
    "1. ```The paper is great, non-robust features are the real thing and different networks use the same features.```\n",
    "2. ```The paper is wrong, it's only a fancy way of network distilling.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrjOUvtPqpBX"
   },
   "source": [
    "```Let's first explore the concept of distilling networks. Can we do it in any case? Can you distill a network using only, for example, white noise? Can you do it using only the predictions? Or do you maybe need to use the logits of the network? Answer this question. MNIST might help you with that.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O_98iH7IqpBX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Layer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hf4uG7lzqpBY"
   },
   "outputs": [],
   "source": [
    "class DivideLayer(Layer):\n",
    "    def __init__(self, temperature=1, **kwargs):\n",
    "        self.temperature = temperature\n",
    "        super(DivideLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return  inputs / self.temperature\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qf2le7lGqpBY"
   },
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, input_shape, num_classes, temperature=1):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=input_shape),\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            DivideLayer(temperature),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=3)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "#     model.summary()\n",
    "    model.fit(x_train, y_train, batch_size=256, epochs=15, validation_split=0.1, callbacks=[es])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nPDFh26XqyDS"
   },
   "outputs": [],
   "source": [
    "def TestAttack(model, adv_images, orig_images, true_labels, target_labels=None, targeted=False):\n",
    "    adv_images = adv_images.numpy()\n",
    "    score = model.evaluate(adv_images, true_labels, verbose=0)\n",
    "    \n",
    "    print('Test loss: {:.2f}'.format(score[0]))\n",
    "    print('Successfully moved out of source class: {:.2f}'.format( 1 - score[1]))\n",
    "    \n",
    "    if targeted:\n",
    "        score = model.evaluate(adv_images, target, verbose=0)\n",
    "        print('Test loss: {:.2f}'.format(score[0]))\n",
    "        print('Successfully perturbed to target class: {:.2f}'.format(score[1]))\n",
    "    \n",
    "    dist = np.mean(np.sqrt(np.mean(np.square(adv_images - orig_images), axis=(1,2,3))))\n",
    "    print('Mean perturbation distance: {:.2f}'.format(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rnoBkO4Pq0xC"
   },
   "outputs": [],
   "source": [
    "def FastGradientSignMethod(model, input_image, input_label, eps=0.3):\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = loss_object(input_label, prediction)\n",
    "\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adv_x = input_image + eps*signed_grad\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tK5lG8riqvKL"
   },
   "outputs": [],
   "source": [
    "def attack_model(test_images, test_labels, model, eps=0.3):\n",
    "  test_images = tf.convert_to_tensor(test_images)\n",
    "\n",
    "  adv_images = FastGradientSignMethod(model, test_images, test_labels, eps=eps)\n",
    "  TestAttack(model, adv_images, test_images.numpy(), test_labels, targeted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "atp3ixO5rQvJ"
   },
   "outputs": [],
   "source": [
    "def train_distillation(temperature=1, use_proba=True, white_noise_input=False, \n",
    "                       number_white_noise=60000):\n",
    "    num_classes = 10\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    print('x_train', x_train.shape)\n",
    "    print(\"Training teacher model\")\n",
    "    teacher = train_model(x_train, y_train, input_shape, num_classes, temperature=temperature)\n",
    "    \n",
    "    _, acc = teacher.evaluate(x_test, y_test, batch_size=512)\n",
    "    print('Teacher test accuracy:', acc)\n",
    "\n",
    "    if use_proba is True:\n",
    "        train_probas = teacher.predict(x_train)\n",
    "    else:\n",
    "        train_probas = teacher.predict(x_train)\n",
    "        print(train_probas[0, :])\n",
    "        train_probas = np.argmax(train_probas, axis=1)\n",
    "        train_probas = tf.one_hot(train_probas, depth=num_classes)\n",
    "        print(train_probas[0, :])\n",
    "\n",
    "    if white_noise_input is True:\n",
    "#         x_train = np.random.normal(size=(60000, 28, 28, 1))\n",
    "        x_train = np.random.randint(0, 2, size=(number_white_noise, 28, 28, 1))\n",
    "        train_probas = teacher.predict(x_train)\n",
    "\n",
    "    print(\"Training student model\")\n",
    "    student = train_model(x_train, train_probas, input_shape, num_classes, temperature=temperature)\n",
    "    \n",
    "    _, acc = student.evaluate(x_test, y_test, batch_size=512)\n",
    "    print('Student test accuracy:', acc)\n",
    "\n",
    "    print('Attacking teacher model')\n",
    "    attack_model(x_test, y_test, teacher, eps=0.3)\n",
    "    print('Attacking student model')\n",
    "    attack_model(x_test, y_test, student, eps=0.3)\n",
    "\n",
    "    return teacher, student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "997p2hd1rIZb",
    "outputId": "74ae89cb-e7df-40da-cbbd-eb06affaed0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training teacher model\n",
      "Epoch 1/15\n",
      "211/211 [==============================] - 4s 14ms/step - loss: 1.3754 - accuracy: 0.6046 - val_loss: 0.7729 - val_accuracy: 0.9508\n",
      "Epoch 2/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.2553 - accuracy: 0.9234 - val_loss: 0.5619 - val_accuracy: 0.9682\n",
      "Epoch 3/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.1807 - accuracy: 0.9463 - val_loss: 0.4765 - val_accuracy: 0.9753\n",
      "Epoch 4/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.1482 - accuracy: 0.9556 - val_loss: 0.4378 - val_accuracy: 0.9778\n",
      "Epoch 5/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.1249 - accuracy: 0.9623 - val_loss: 0.4146 - val_accuracy: 0.9787\n",
      "Epoch 6/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.1166 - accuracy: 0.9633 - val_loss: 0.3762 - val_accuracy: 0.9808\n",
      "Epoch 7/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.1009 - accuracy: 0.9705 - val_loss: 0.3380 - val_accuracy: 0.9827\n",
      "Epoch 8/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0915 - accuracy: 0.9722 - val_loss: 0.2989 - val_accuracy: 0.9858\n",
      "Epoch 9/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0858 - accuracy: 0.9728 - val_loss: 0.3006 - val_accuracy: 0.9865\n",
      "Epoch 10/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0817 - accuracy: 0.9753 - val_loss: 0.2995 - val_accuracy: 0.9862\n",
      "Epoch 11/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0788 - accuracy: 0.9754 - val_loss: 0.2934 - val_accuracy: 0.9880\n",
      "Epoch 12/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0784 - accuracy: 0.9755 - val_loss: 0.2598 - val_accuracy: 0.9880\n",
      "Epoch 13/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0699 - accuracy: 0.9784 - val_loss: 0.2941 - val_accuracy: 0.9862\n",
      "Epoch 14/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0664 - accuracy: 0.9790 - val_loss: 0.2507 - val_accuracy: 0.9897\n",
      "Epoch 15/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0645 - accuracy: 0.9805 - val_loss: 0.2647 - val_accuracy: 0.9878\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2330 - accuracy: 0.9860\n",
      "Teacher test accuracy: 0.9860000014305115\n",
      "Training student model\n",
      "Epoch 1/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 1.3529 - accuracy: 0.6044 - val_loss: 0.5452 - val_accuracy: 0.9527\n",
      "Epoch 2/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.2235 - accuracy: 0.9286 - val_loss: 0.3314 - val_accuracy: 0.9735\n",
      "Epoch 3/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.1498 - accuracy: 0.9503 - val_loss: 0.2312 - val_accuracy: 0.9798\n",
      "Epoch 4/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.1136 - accuracy: 0.9630 - val_loss: 0.1934 - val_accuracy: 0.9837\n",
      "Epoch 5/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0911 - accuracy: 0.9696 - val_loss: 0.1649 - val_accuracy: 0.9857\n",
      "Epoch 6/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0766 - accuracy: 0.9751 - val_loss: 0.1362 - val_accuracy: 0.9873\n",
      "Epoch 7/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0689 - accuracy: 0.9770 - val_loss: 0.1040 - val_accuracy: 0.9897\n",
      "Epoch 8/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0588 - accuracy: 0.9804 - val_loss: 0.1020 - val_accuracy: 0.9895\n",
      "Epoch 9/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0523 - accuracy: 0.9822 - val_loss: 0.0730 - val_accuracy: 0.9913\n",
      "Epoch 10/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0479 - accuracy: 0.9840 - val_loss: 0.0650 - val_accuracy: 0.9915\n",
      "Epoch 11/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0461 - accuracy: 0.9835 - val_loss: 0.0554 - val_accuracy: 0.9922\n",
      "Epoch 12/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0442 - accuracy: 0.9843 - val_loss: 0.0618 - val_accuracy: 0.9935\n",
      "Epoch 13/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0409 - accuracy: 0.9858 - val_loss: 0.0530 - val_accuracy: 0.9928\n",
      "Epoch 14/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0387 - accuracy: 0.9865 - val_loss: 0.0499 - val_accuracy: 0.9937\n",
      "Epoch 15/15\n",
      "211/211 [==============================] - 3s 12ms/step - loss: 0.0374 - accuracy: 0.9859 - val_loss: 0.0585 - val_accuracy: 0.9938\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3443 - accuracy: 0.9851\n",
      "Student test accuracy: 0.9850999712944031\n",
      "Attacking teacher model\n",
      "Test loss: 23.87\n",
      "Successfully moved out of source class: 0.36\n",
      "Mean perturbation distance: 0.11\n",
      "Attacking student model\n",
      "Test loss: 20.95\n",
      "Successfully moved out of source class: 0.22\n",
      "Mean perturbation distance: 0.07\n"
     ]
    }
   ],
   "source": [
    "teacher, student = train_distillation(temperature=10, use_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0wlPjoC-1L8",
    "outputId": "be6d443b-486e-47d8-f67e-763eeb456a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training teacher model\n",
      "Epoch 1/15\n",
      "211/211 [==============================] - 3s 14ms/step - loss: 1.4108 - accuracy: 0.5956 - val_loss: 0.7853 - val_accuracy: 0.9508\n",
      "Epoch 2/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.2501 - accuracy: 0.9278 - val_loss: 0.5431 - val_accuracy: 0.9707\n",
      "Epoch 3/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.1689 - accuracy: 0.9509 - val_loss: 0.4625 - val_accuracy: 0.9770\n",
      "Epoch 4/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.1375 - accuracy: 0.9594 - val_loss: 0.4091 - val_accuracy: 0.9792\n",
      "Epoch 5/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.1164 - accuracy: 0.9648 - val_loss: 0.3756 - val_accuracy: 0.9822\n",
      "Epoch 6/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.1067 - accuracy: 0.9686 - val_loss: 0.3469 - val_accuracy: 0.9830\n",
      "Epoch 7/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0944 - accuracy: 0.9714 - val_loss: 0.2977 - val_accuracy: 0.9855\n",
      "Epoch 8/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0808 - accuracy: 0.9760 - val_loss: 0.3095 - val_accuracy: 0.9860\n",
      "Epoch 9/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0788 - accuracy: 0.9763 - val_loss: 0.2833 - val_accuracy: 0.9873\n",
      "Epoch 10/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0771 - accuracy: 0.9768 - val_loss: 0.2690 - val_accuracy: 0.9878\n",
      "Epoch 11/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0664 - accuracy: 0.9798 - val_loss: 0.2838 - val_accuracy: 0.9873\n",
      "Epoch 12/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 0.2450 - val_accuracy: 0.9890\n",
      "Epoch 13/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0620 - accuracy: 0.9812 - val_loss: 0.2512 - val_accuracy: 0.9888\n",
      "Epoch 14/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0592 - accuracy: 0.9822 - val_loss: 0.2261 - val_accuracy: 0.9903\n",
      "Epoch 15/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0577 - accuracy: 0.9816 - val_loss: 0.2280 - val_accuracy: 0.9903\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1843 - accuracy: 0.9878\n",
      "Teacher test accuracy: 0.9878000020980835\n",
      "[0.000000e+00 0.000000e+00 0.000000e+00 4.972901e-16 0.000000e+00\n",
      " 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      "tf.Tensor([0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n",
      "Training student model\n",
      "Epoch 1/15\n",
      "211/211 [==============================] - 3s 14ms/step - loss: 1.3997 - accuracy: 0.5620 - val_loss: 0.5761 - val_accuracy: 0.9520\n",
      "Epoch 2/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.2290 - accuracy: 0.9287 - val_loss: 0.3932 - val_accuracy: 0.9727\n",
      "Epoch 3/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.1525 - accuracy: 0.9522 - val_loss: 0.2792 - val_accuracy: 0.9787\n",
      "Epoch 4/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.1160 - accuracy: 0.9615 - val_loss: 0.2192 - val_accuracy: 0.9830\n",
      "Epoch 5/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0986 - accuracy: 0.9677 - val_loss: 0.1959 - val_accuracy: 0.9845\n",
      "Epoch 6/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0876 - accuracy: 0.9706 - val_loss: 0.1569 - val_accuracy: 0.9863\n",
      "Epoch 7/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0740 - accuracy: 0.9751 - val_loss: 0.1316 - val_accuracy: 0.9880\n",
      "Epoch 8/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0690 - accuracy: 0.9761 - val_loss: 0.1325 - val_accuracy: 0.9872\n",
      "Epoch 9/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0623 - accuracy: 0.9793 - val_loss: 0.0989 - val_accuracy: 0.9902\n",
      "Epoch 10/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0557 - accuracy: 0.9813 - val_loss: 0.1170 - val_accuracy: 0.9900\n",
      "Epoch 11/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0511 - accuracy: 0.9825 - val_loss: 0.1015 - val_accuracy: 0.9912\n",
      "Epoch 12/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.1056 - val_accuracy: 0.9917\n",
      "Epoch 13/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0482 - accuracy: 0.9834 - val_loss: 0.0932 - val_accuracy: 0.9923\n",
      "Epoch 14/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0427 - accuracy: 0.9845 - val_loss: 0.0688 - val_accuracy: 0.9933\n",
      "Epoch 15/15\n",
      "211/211 [==============================] - 3s 13ms/step - loss: 0.0412 - accuracy: 0.9855 - val_loss: 0.0626 - val_accuracy: 0.9937\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3313 - accuracy: 0.9840\n",
      "Student test accuracy: 0.984000027179718\n",
      "Attacking teacher model\n",
      "Test loss: 24.21\n",
      "Successfully moved out of source class: 0.34\n",
      "Mean perturbation distance: 0.11\n",
      "Attacking student model\n",
      "Test loss: 21.91\n",
      "Successfully moved out of source class: 0.24\n",
      "Mean perturbation distance: 0.07\n"
     ]
    }
   ],
   "source": [
    "teacher, student = train_distillation(temperature=10, use_proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1ApDN5iBpln",
    "outputId": "3e64264a-7ae6-4ca2-bcea-2219b20324ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (60000, 28, 28, 1)\n",
      "Training teacher model\n",
      "Epoch 1/15\n",
      "211/211 [==============================] - 22s 103ms/step - loss: 0.7902 - accuracy: 0.7815 - val_loss: 0.7622 - val_accuracy: 0.9507\n",
      "Epoch 2/15\n",
      "211/211 [==============================] - 21s 99ms/step - loss: 0.2169 - accuracy: 0.9352 - val_loss: 0.5494 - val_accuracy: 0.9715\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a4749d0c294d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m teacher, student = train_distillation(temperature=10, use_proba=True, white_noise_input=True,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                      number_white_noise=100000)\n",
      "\u001b[1;32m<ipython-input-12-ea827ffbcb8f>\u001b[0m in \u001b[0;36mtrain_distillation\u001b[1;34m(temperature, use_proba, white_noise_input, number_white_noise)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training teacher model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mteacher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ba21c45afa3e>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(x_train, y_train, input_shape, num_classes, temperature)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#     model.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "teacher, student = train_distillation(temperature=10, use_proba=True, white_noise_input=True,\n",
    "                                     number_white_noise=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzyY4lx1qpBZ"
   },
   "source": [
    "```Distilling using only white noise is a though question. Why is it? Think about the concepts of distribution and out-of-distribution in when answering this.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXBSUcNHqpBa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnC14lF9qpBa",
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "```Assuming conjecture #2 and regarding experiment #1, the distillation isn't happening on white noise, but not on real images either. Think of an experiment that will help you decide if the phenomenon the authors encountered is indeed just network distillation. Think how to measure your success. Open the hint only if you can't think of a way do to it.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBmBIcIoqpBa",
    "solution": "hidden"
   },
   "source": [
    "```You can, for example, take a dataset such as CIFAR10, train a network on 5 classes and try to distill it using the other 5 classes. That way you are using real images, but from different distributions.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JIEJxDQqpBa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ataDtPbuqpBb"
   },
   "source": [
    "```Let's focus now on conjecture #1. Conduct an experiment that will demonstrate that two different networks indeed use the same features. Do it gradually: Start with two copies of the same architecture and on the same data, and move to different architectures and different subsets of the data (but from the same distribution). Think how to measure your success.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyOB6OXWqpBb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experimental work and the transferability principle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
