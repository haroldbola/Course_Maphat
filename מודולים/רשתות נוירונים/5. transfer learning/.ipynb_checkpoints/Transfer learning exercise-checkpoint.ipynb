{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to save this as a new notebook before you begin solving!!\n",
    "## Also remember to open the notebook through a virtual env that works well with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise is meant to teach you how to a powerful, existing neural network for a new problem in a similar domain. This exercise should come after you have some experience with NN (not neccessarily extensive experience)\n",
    "\n",
    "The core of the exercise is downloading a trained NN for face classification, and using transfer learning techniques to solve a new problem. The new problem you will solve is classification of faces - automatic labeling of the different members of your family.\n",
    "\n",
    "### Authors: Philip Tannor, Gal Eyal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will get your hands dirty and create your own data:\n",
    "1. Start by creating a folder with many photos of yourself (between 100 and 300).\n",
    "2. Create similar folders for a number of roommates or family members (preferably people who live with you). Try to make sure the folders have similar amounts of photos in them.\n",
    "3. Use the python program cropall.py (*works on linux only!!*), which should be in the folder of this exercise, to crop out the faces from the different photos. You should create a x/y ratio of 3/4, don't demand a perfect pixel ratio, and change the x/y ratio to 4/3 when you have a rotated photo. I you're working correctly, you should only be using the mouse click, the mouse scroll, and Spacebar.\n",
    "4. Make sure to save the cropped photos of each person in a different folder. Notice that by default the crops will all go to the same folder, so find a way to deal with this manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and parameters\n",
    "\n",
    "We left this section almost exactly as it was were when we created the solution, just to save some time. Don't feel bonded by these libraries (or parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#These next 2 imports depend on the 'keras-vggface' folder in https://github.com/rcmalli/keras-vggface\n",
    "#Just in case the git won't be around, we put this folder in the directory you're working in.\n",
    "#Notice this if you move the notebook around.\n",
    "# from keras_vggface import VGGFace\n",
    "# from keras_vggface import utils\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "import unittest\n",
    "import pandas as pd\n",
    "from os import listdir,path\n",
    "from sklearn.model_selection import train_test_split\n",
    "#This next import will help you with augmentation - generating augmented photos from your originals.\n",
    "#Read about this general teqnique, and also about \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=(224, 224)\n",
    "n_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'set_image_dim_ordering'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-89aff35f274b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_image_dim_ordering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'set_image_dim_ordering'"
     ]
    }
   ],
   "source": [
    "keras.backend.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try testing one of the existing models.\n",
    "1. Create an instance of the VGGFace class you imported, and chose which model it will be using.\n",
    "2. Use keras.preprocessing.image to read the only picture in the 'image' file (fix the target size).\n",
    "3. Use vgg-keras.utils.preprocess_input for preprocessing the photo. You may have to expand the dimensions before this.\n",
    "4. Use the model to create a prediction for the image.\n",
    "5. Check how the model did. The guy you looked at should be indexed first.\n",
    "6. Restart the kernel and next time skip this section (so that you aren't wasting RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 34s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# vggface = VGGFace(model ='vgg16')\n",
    "\n",
    "vggface = keras.applications.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggface.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 2us/step\n",
      "suit (12.38%)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "image = load_img('image/ajb.jpg', target_size=input_size)\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "\n",
    "yhat = vggface.predict(image)\n",
    "\n",
    "label = decode_predictions(yhat)\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preprocessing the data you created\n",
    "create a DataFrame with 2 columns:\n",
    "1. The exact path of each image\n",
    "2. The label of each image\n",
    "\n",
    "Split this data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['image_path', 'label'])\n",
    "for folder in listdir('./data'):\n",
    "    for im in listdir(path.join('./data', folder)):\n",
    "        df = df.append({'image_path': path.join('./data', folder, im),\n",
    "                  'label': folder}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data\\gal\\20150221_112656_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data\\gal\\20150221_124226_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data\\gal\\20150221_133047_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data\\gal\\20150221_135339_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data\\gal\\20150227_162125_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_path label\n",
       "0  ./data\\gal\\20150221_112656_HDR.jpg   gal\n",
       "1  ./data\\gal\\20150221_124226_HDR.jpg   gal\n",
       "2  ./data\\gal\\20150221_133047_HDR.jpg   gal\n",
       "3  ./data\\gal\\20150221_135339_HDR.jpg   gal\n",
       "4  ./data\\gal\\20150227_162125_HDR.jpg   gal"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use iterate through the dataframes and read the images corresponding to each row (using keras.preprocessing.image). Save the images as numpy arrays, and once again use utils.preprocess_input to preprocess the arrays.\n",
    "Save the labels in 2 different ways:\n",
    "1. Onehot encoding - for using a NN\n",
    "2. Serial number of the corresponding class (one number per label) - for using an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((len(data_train), input_size[0], input_size[1], n_channels))\n",
    "\n",
    "for i, row in enumerate(data_train.itertuples()):\n",
    "    image_path = row.image_path\n",
    "    \n",
    "    image = load_img(image_path, target_size=input_size)\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    # Shape: 1 x 224 x 224 x 3\n",
    "    \n",
    "    train_x[i, :, :, :] = image\n",
    "\n",
    "train_y_NN = pd.get_dummies(data_train['label'])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "train_y_SVM = encoder.fit_transform(data_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (212, 224, 224, 3)\n",
      "train_y_NN (212, 2)\n",
      "train_y_SVM (212,)\n"
     ]
    }
   ],
   "source": [
    "print('train_x', train_x.shape)\n",
    "print('train_y_NN', train_y_NN.shape)\n",
    "print('train_y_SVM', train_y_SVM.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of keras.preprocessing.image.ImageDataGenerator, which will define how you will create augmentations of each original image you've created. Chose all the parameters on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rotation_range=90, width_shift_range=0.1, \n",
    "                               height_shift_range=0.1,zoom_range=0.2, \n",
    "                               horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally - transfer learning. \n",
    "### This should be done with the keras functional API (not Sequential)\n",
    "1. Create an instance of VGGFace with the model of your choice. \n",
    "2. Examine the model's architecture using .summary(). Understand what it means to replace the last 2 dense layers (including the final softmax layer). \n",
    "3. Use .get_layer() to retrieve the last layer which you want to keep in your new NN.\n",
    "4. Create the 2 new Dense layers which continue the previous pretrained layers (the last layer should have a softmax activation).\n",
    "5. Create a new model which the input of the original model as input, and outputs the new dense-softmax layer. You can use .input on the old model for this.\n",
    "6. Freeze all of the layers except the last 2 using .layers on the new model, and .trainable = False. This will stop you from training those layers.\n",
    "7. Compile the model ('adam' worked ok for us as an optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_class = 2\n",
    "hidden_dim = 512\n",
    "\n",
    "vgg_model = keras.applications.VGG16()\n",
    "dense_1 = vgg_model.get_layer('fc6/relu').output\n",
    "#new_dense_1 = Dense(hidden_dim, activation='relu', name='fc6')(flatten)\n",
    "new_dense_1 = Dense(hidden_dim, activation='relu', name='fc7')(dense_1)\n",
    "out = Dense(nb_class, activation='softmax', name='fc8')(new_dense_1)\n",
    "custom_vgg_model = Model(vgg_model.input, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to train the model:\n",
    "1. Use .fit_generator() and not .fit(), since you'll be using the augmentor you created.\n",
    "2. Use .flow() on the instance of ImageDataGenerator as the first input. \n",
    "3. Choose a combination on batch_size(within .flow) and steps_per_epoch which will create a total number of images that you want per each epoch. \n",
    "4. Use validation data which isn't augmented.\n",
    "\n",
    "How well is it doing? We got an accuracy of 90%-95% percent (validation data) on photos on the 2 of us (just 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar exercise with SVM\n",
    "1. Now create a new model which outputs the data from the last frozen layer. Use this model to create feature for each image.\n",
    "2. Create a SVM classifier using these features. We used sklearn.svm.SVC.\n",
    "3. Did this work better or less good? Evaluate using sklearn.metrics.classification_report or sklearn.metrics.accuracy_score.\n",
    "4. Try the same thing with the one-before-last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats! You completed the exercise. \n",
    "# Bonus - use one of these models and a webcam to created a personalized security system for your house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
