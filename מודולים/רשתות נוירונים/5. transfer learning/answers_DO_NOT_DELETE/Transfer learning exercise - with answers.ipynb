{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to save this as a new notebook before you begin solving!!\n",
    "## Also remember to open the notebook through a virtual env that works well with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise is meant to teach you how to a powerful, existing neural network for a new problem in a similar domain. This exercise should come after you have some experience with NN (not necessarily extensive experience)\n",
    "\n",
    "The core of the exercise is downloading a trained NN for face classification, and using transfer learning techniques to solve a new problem. The new problem you will solve is classification of faces - automatic labeling of the different members of your family.\n",
    "\n",
    "### Authors: Philip Tannor, Gal Eyal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will get your hands dirty and create your own data:\n",
    "1. Start by creating a folder with many photos of yourself (between 100 and 300).\n",
    "2. Create similar folders for a number of roommates or family members (preferably people who live with you). Try to make sure the folders have similar amounts of photos in them.\n",
    "3. Use the python program cropall.py (*works on linux only!!*), which should be in the folder of this exercise, to crop out the faces from the different photos. You should create a x/y ratio of 3/4, don't demand a perfect pixel ratio, and change the x/y ratio to 4/3 when you have a rotated photo. I you're working correctly, you should only be using the mouse click, the mouse scroll, and Spacebar.\n",
    "4. Make sure to save the cropped photos of each person in a different folder. Notice that by default the crops will all go to the same folder, so find a way to deal with this manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and parameters\n",
    "\n",
    "We left this section almost exactly as it was were when we created the solution, just to save some time. Don't feel bonded by these libraries (or parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#These next 2 imports depend on the 'keras-vggface' folder in https://github.com/rcmalli/keras-vggface\n",
    "#Just in case the git won't be around, we put this folder in the directory you're working in.\n",
    "#Notice this if you move the notebook around.\n",
    "from keras_vggface import VGGFace\n",
    "from keras_vggface import utils\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "import unittest\n",
    "import pandas as pd\n",
    "from os import listdir,path\n",
    "from sklearn.model_selection import train_test_split\n",
    "#This next import will help you with augmentation - generating augmented photos from your originals.\n",
    "#Read about this general teqnique, and also about \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size=(224, 224)\n",
    "n_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.backend.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try testing one of the existing models.\n",
    "1. Create an instance of the VGGFace class you imported, and chose which model it will be using.\n",
    "2. Use keras.preprocessing.image to read the only picture in the 'image' file (fix the target size).\n",
    "3. Use vgg-keras.utils.preprocess_input for preprocessing the photo. You may have to expand the dimensions before this.\n",
    "4. Use the model to create a prediction for the image.\n",
    "5. Check how the model did. The guy you looked at should be indexed first.\n",
    "6. Restart the kernel and next time skip this section (so that you aren't wasting RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VGGFace(model='vgg16')\n",
    "img = image.load_img('image/ajb.jpg', target_size=input_size)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = utils.preprocess_input(x, version=1)\n",
    "preds = model.predict(x)\n",
    "plt.plot(preds[0,:], '.')\n",
    "plt.show()\n",
    "preds[0,:]\n",
    "utils.decode_predictions(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preprocessing the data you created\n",
    "create a DataFrame with 2 columns:\n",
    "1. The exact path of each image\n",
    "2. The label of each image\n",
    "\n",
    "Split this data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['image_path', 'label'])\n",
    "for folder in listdir('./data'):\n",
    "    for im in listdir(path.join('./data', folder)):\n",
    "        df = df.append({'image_path': path.join('./data', folder, im),\n",
    "                  'label': folder}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use iterate through the dataframes and read the images corresponding to each row (using keras.preprocessing.image). Save the images as numpy arrays, and once again use utils.preprocess_input to preprocess the arrays.\n",
    "Save the labels in 2 different ways:\n",
    "1. Onehot encoding - for using a NN\n",
    "2. Serial number of the corresponding class (one number per label) - for using an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.zeros((len(train), input_size[0], input_size[1], n_channels))\n",
    "for i, row in enumerate(train.itertuples()):\n",
    "    img = image.load_img(row.image_path, target_size=input_size)\n",
    "    x = image.img_to_array(img)\n",
    "    train_x[i, :] = x\n",
    "train_x = utils.preprocess_input(train_x, version=1)\n",
    "train_y = pd.get_dummies(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.zeros((len(test), input_size[0], input_size[1], n_channels))\n",
    "for i, row in enumerate(test.itertuples()):\n",
    "    img = image.load_img(row.image_path, target_size=input_size)\n",
    "    x = image.img_to_array(img)\n",
    "    test_x[i, :] = x\n",
    "test_x = utils.preprocess_input(test_x, version=1)\n",
    "test_y = pd.get_dummies(test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of keras.preprocessing.image.ImageDataGenerator, which will define how you will create augmentations of each original image you've created. Chose all the parameters on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rotation_range=90, width_shift_range=0.1, height_shift_range=0.1,zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally - transfer learning. \n",
    "### This should be done with the keras functional API (not Sequential)\n",
    "1. Create an instance of VGGFace with the model of your choice. \n",
    "2. Examine the model's architechture using .summary(). Understand what it means to replace the last 2 dense layers (including the final softmax layer). \n",
    "3. Use .get_layer() to retrieve the last layer which you want to keep in your new NN.\n",
    "4. Create the 2 new Dense layers which continue the previous pretrained layers (the last layer should have a softmax activation).\n",
    "5. Create a new model which the input of the original model as input, and outputs the new dense-softmax layer. You can use .input on the old model for this.\n",
    "6. Freeze all of the layers except the last 2 using .layers on the new model, and .trainable = False. This will stop you from training those layers.\n",
    "7. Compile the model ('adam' worked ok for us as an optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom parameters\n",
    "nb_class = 2\n",
    "hidden_dim = 512\n",
    "\n",
    "vgg_model = VGGFace(input_shape=(224, 224, 3))\n",
    "dense_1 = vgg_model.get_layer('fc6/relu').output\n",
    "#new_dense_1 = Dense(hidden_dim, activation='relu', name='fc6')(flatten)\n",
    "new_dense_1 = Dense(hidden_dim, activation='relu', name='fc7')(dense_1)\n",
    "out = Dense(nb_class, activation='softmax', name='fc8')(new_dense_1)\n",
    "custom_vgg_model = Model(vgg_model.input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in custom_vgg_model.layers[:-2]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_vgg_model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to train the model:\n",
    "1. Use .fit_generator() and not .fit(), since you'll be using the augmentor you created.\n",
    "2. Use .flow() on the instance of ImageDataGenerator as the first input. \n",
    "3. Choose a combination on batch_size(within .flow) and steps_per_epoch which will create a total number of images that you want per each epoch. \n",
    "4. Use validation data which isn't augmented.\n",
    "\n",
    "How well is it doing? We got an accuracy of 90%-95% percent (validation data) on photos on the 2 of us (just 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 36s 3s/step - loss: 3.4984 - acc: 0.7247 - val_loss: 1.0593 - val_acc: 0.9091\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.1545 - acc: 0.8305 - val_loss: 0.9808 - val_acc: 0.9273\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.8975 - acc: 0.8538 - val_loss: 1.0515 - val_acc: 0.9273\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.8186 - acc: 0.8731 - val_loss: 1.6155 - val_acc: 0.8909\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.1879 - acc: 0.8486 - val_loss: 1.4573 - val_acc: 0.9091\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.1815 - acc: 0.8498 - val_loss: 1.1658 - val_acc: 0.9273\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.6433 - acc: 0.8844 - val_loss: 1.1775 - val_acc: 0.9273\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.8150 - acc: 0.8113 - val_loss: 1.4067 - val_acc: 0.9091\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.7380 - acc: 0.8607 - val_loss: 1.4524 - val_acc: 0.8909\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.5193 - acc: 0.8952 - val_loss: 1.0369 - val_acc: 0.9273\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.7788 - acc: 0.8029 - val_loss: 1.7992 - val_acc: 0.8727\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.6521 - acc: 0.8767 - val_loss: 1.0495 - val_acc: 0.9273\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.6446 - acc: 0.8919 - val_loss: 1.7488 - val_acc: 0.8909\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.0003 - acc: 0.8724 - val_loss: 1.4368 - val_acc: 0.8909\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.7904 - acc: 0.8686 - val_loss: 2.3317 - val_acc: 0.8545\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.4144 - acc: 0.8342 - val_loss: 2.3317 - val_acc: 0.8545\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 4.2952 - acc: 0.7217 - val_loss: 1.4573 - val_acc: 0.9091\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.7017 - acc: 0.8802 - val_loss: 2.6267 - val_acc: 0.8364\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.1697 - acc: 0.8498 - val_loss: 1.7488 - val_acc: 0.8909\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.3804 - acc: 0.9110 - val_loss: 1.1658 - val_acc: 0.9273\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.9212 - acc: 0.8759 - val_loss: 2.0402 - val_acc: 0.8727\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 1.8225 - acc: 0.8802 - val_loss: 1.1915 - val_acc: 0.9091\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 15s 1s/step - loss: 2.6571 - acc: 0.8227 - val_loss: 2.6235 - val_acc: 0.8364\n",
      "Epoch 24/100\n",
      " 1/13 [=>............................] - ETA: 11s - loss: 1.6030 - acc: 0.9000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-61707ae8943a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m custom_vgg_model.fit_generator(train_gen.flow(train_x, train_y, batch_size=20, seed=77), steps_per_epoch=13, epochs=100, \n\u001b[1;32m----> 2\u001b[1;33m                                validation_data=(test_x, test_y), workers=50)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "custom_vgg_model.fit_generator(train_gen.flow(train_x, train_y, batch_size=20, seed=77), \\\n",
    "        steps_per_epoch=13, epochs=100, validation_data=(test_x, test_y), workers=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar exercise with SVM\n",
    "1. Now create a new model which outputs the data from the last frozen layer. Use this model to create feature for each image.\n",
    "2. Create a SVM classifier using these features. We used sklearn.svm.SVC.\n",
    "3. Did this work better or less good? Evaluate using sklearn.metrics.classification_report or sklearn.metrics.accuracy_score.\n",
    "4. Try the same thing with the one-before-last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_dense = vgg_model.get_layer('fc7/relu').output\n",
    "cnn_code_model = Model(vgg_model.input, final_dense)\n",
    "train_f = cnn_code_model.predict(train_x)\n",
    "test_f = cnn_code_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model = SVC()\n",
    "svm_model.fit(train_f,train_y.gal)\n",
    "test_p=svm_model.predict(test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.96      0.91        27\n",
      "          1       0.96      0.86      0.91        28\n",
      "\n",
      "avg / total       0.91      0.91      0.91        55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test_y.gal, test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y.gal, test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats! You completed the exercise. \n",
    "# Bonus - use one of these models and a webcam to created a personalized security system for your house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = {'open_cargo_area': 0.76805175603205811, 'sunroof': 0.86970924721930554, 'yellow': 0.89589529966763537, 'ac_vents': 0.96354515221579118, 'large vehicle': 0.967633060324956, 'soft_shell_box': 0.12349743597877082, 'small vehicle': 0.99923983599401911, 'flatbed': 0.41886243386243383, 'blue': 0.60881424782853055, 'van': 0.74821960084759509, 'wrecked': 0.30877492983981797, 'crane truck': 0.73290598290598297, 'harnessed_to_a_cart': 0.11041046723608668, 'pickup': 0.63983575751303001, 'other': 0.10624473746628872, 'black': 0.87419993496329729, 'enclosed_cab': 0.29070354292695971, 'light truck': 0.46831705714546018, 'green': 0.047688623423157162, 'white': 0.93120701947835438, 'ladder': 0.020864457138074614, 'red': 0.83955850703367685, 'prime mover': 0.66474348026280405, 'bus': 0.96542111400274411, 'dedicated agricultural vehicle': 0.53234126984126984, 'enclosed_box': 0.64633525361595334, 'cement mixer': 0.17111823555768785, 'sedan': 0.96088493835027711, 'hatchback': 0.81490439599803766, 'minibus': 0.36037590386145602, 'jeep': 0.12514606086779878, 'silver/grey': 0.806923012930991, 'spare_wheel': 0.079828318098639134, 'truck': 0.55628477043330549, 'minivan': 0.44816745345990783, 'luggage_carrier': 0.80062166303241111, 'tanker': 0.08553744730215318}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ac_vents': 0.9635451522157912,\n",
       " 'black': 0.8741999349632973,\n",
       " 'blue': 0.6088142478285306,\n",
       " 'bus': 0.9654211140027441,\n",
       " 'cement mixer': 0.17111823555768785,\n",
       " 'crane truck': 0.732905982905983,\n",
       " 'dedicated agricultural vehicle': 0.5323412698412698,\n",
       " 'enclosed_box': 0.6463352536159533,\n",
       " 'enclosed_cab': 0.2907035429269597,\n",
       " 'flatbed': 0.4188624338624338,\n",
       " 'green': 0.04768862342315716,\n",
       " 'harnessed_to_a_cart': 0.11041046723608668,\n",
       " 'hatchback': 0.8149043959980377,\n",
       " 'jeep': 0.12514606086779878,\n",
       " 'ladder': 0.020864457138074614,\n",
       " 'large vehicle': 0.967633060324956,\n",
       " 'light truck': 0.4683170571454602,\n",
       " 'luggage_carrier': 0.8006216630324111,\n",
       " 'minibus': 0.360375903861456,\n",
       " 'minivan': 0.4481674534599078,\n",
       " 'open_cargo_area': 0.7680517560320581,\n",
       " 'other': 0.10624473746628872,\n",
       " 'pickup': 0.63983575751303,\n",
       " 'prime mover': 0.664743480262804,\n",
       " 'red': 0.8395585070336768,\n",
       " 'sedan': 0.9608849383502771,\n",
       " 'silver/grey': 0.806923012930991,\n",
       " 'small vehicle': 0.9992398359940191,\n",
       " 'soft_shell_box': 0.12349743597877082,\n",
       " 'spare_wheel': 0.07982831809863913,\n",
       " 'sunroof': 0.8697092472193055,\n",
       " 'tanker': 0.08553744730215318,\n",
       " 'truck': 0.5562847704333055,\n",
       " 'van': 0.7482196008475951,\n",
       " 'white': 0.9312070194783544,\n",
       " 'wrecked': 0.30877492983981797,\n",
       " 'yellow': 0.8958952996676354}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('small vehicle', 0.9992398359940191),\n",
       " ('large vehicle', 0.967633060324956),\n",
       " ('bus', 0.9654211140027441),\n",
       " ('ac_vents', 0.9635451522157912),\n",
       " ('sedan', 0.9608849383502771),\n",
       " ('white', 0.9312070194783544),\n",
       " ('yellow', 0.8958952996676354),\n",
       " ('black', 0.8741999349632973),\n",
       " ('sunroof', 0.8697092472193055),\n",
       " ('red', 0.8395585070336768),\n",
       " ('hatchback', 0.8149043959980377),\n",
       " ('silver/grey', 0.806923012930991),\n",
       " ('luggage_carrier', 0.8006216630324111),\n",
       " ('open_cargo_area', 0.7680517560320581),\n",
       " ('van', 0.7482196008475951),\n",
       " ('crane truck', 0.732905982905983),\n",
       " ('prime mover', 0.664743480262804),\n",
       " ('enclosed_box', 0.6463352536159533),\n",
       " ('pickup', 0.63983575751303),\n",
       " ('blue', 0.6088142478285306),\n",
       " ('truck', 0.5562847704333055),\n",
       " ('dedicated agricultural vehicle', 0.5323412698412698),\n",
       " ('light truck', 0.4683170571454602),\n",
       " ('minivan', 0.4481674534599078),\n",
       " ('flatbed', 0.4188624338624338),\n",
       " ('minibus', 0.360375903861456),\n",
       " ('wrecked', 0.30877492983981797),\n",
       " ('enclosed_cab', 0.2907035429269597),\n",
       " ('cement mixer', 0.17111823555768785),\n",
       " ('jeep', 0.12514606086779878),\n",
       " ('soft_shell_box', 0.12349743597877082),\n",
       " ('harnessed_to_a_cart', 0.11041046723608668),\n",
       " ('other', 0.10624473746628872),\n",
       " ('tanker', 0.08553744730215318),\n",
       " ('spare_wheel', 0.07982831809863913),\n",
       " ('green', 0.04768862342315716),\n",
       " ('ladder', 0.020864457138074614)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores.items(), key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
