{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to save this as a new notebook before you begin solving!!\n",
    "## Also remember to open the notebook through a virtual env that works well with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise is meant to teach you how to a powerful, existing neural network for a new problem in a similar domain. This exercise should come after you have some experience with NN (not neccessarily extensive experience)\n",
    "\n",
    "The core of the exercise is downloading a trained NN for face classification, and using transfer learning techniques to solve a new problem. The new problem you will solve is classification of faces - automatic labeling of the different members of your family.\n",
    "\n",
    "### Authors: Philip Tannor, Gal Eyal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will get your hands dirty and create your own data:\n",
    "1. Start by creating a folder with many photos of yourself (between 100 and 300).\n",
    "2. Create similar folders for a number of roommates or family members (preferably people who live with you). Try to make sure the folders have similar amounts of photos in them.\n",
    "3. Use the python program cropall.py (*works on linux only!!*), which should be in the folder of this exercise, to crop out the faces from the different photos. You should create a x/y ratio of 3/4, don't demand a perfect pixel ratio, and change the x/y ratio to 4/3 when you have a rotated photo. I you're working correctly, you should only be using the mouse click, the mouse scroll, and Spacebar.\n",
    "4. Make sure to save the cropped photos of each person in a different folder. Notice that by default the crops will all go to the same folder, so find a way to deal with this manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and parameters\n",
    "\n",
    "We left this section almost exactly as it was were when we created the solution, just to save some time. Don't feel bonded by these libraries (or parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#These next 2 imports depend on the 'keras-vggface' folder in https://github.com/rcmalli/keras-vggface\n",
    "#Just in case the git won't be around, we put this folder in the directory you're working in.\n",
    "#Notice this if you move the notebook around.\n",
    "# from keras_vggface import VGGFace\n",
    "# from keras_vggface import utils\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "import unittest\n",
    "import pandas as pd\n",
    "from os import listdir,path\n",
    "from sklearn.model_selection import train_test_split\n",
    "#This next import will help you with augmentation - generating augmented photos from your originals.\n",
    "#Read about this general teqnique, and also about \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=(224, 224)\n",
    "n_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'set_image_dim_ordering'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-89aff35f274b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_image_dim_ordering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'set_image_dim_ordering'"
     ]
    }
   ],
   "source": [
    "keras.backend.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try testing one of the existing models.\n",
    "1. Create an instance of the VGGFace class you imported, and chose which model it will be using.\n",
    "2. Use keras.preprocessing.image to read the only picture in the 'image' file (fix the target size).\n",
    "3. Use vgg-keras.utils.preprocess_input for preprocessing the photo. You may have to expand the dimensions before this.\n",
    "4. Use the model to create a prediction for the image.\n",
    "5. Check how the model did. The guy you looked at should be indexed first.\n",
    "6. Restart the kernel and next time skip this section (so that you aren't wasting RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 34s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# vggface = VGGFace(model ='vgg16')\n",
    "\n",
    "vggface = keras.applications.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggface.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "suit (12.38%)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "image = load_img('image/ajb.jpg', target_size=input_size)\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "\n",
    "print(image.shape)\n",
    "yhat = vggface.predict(image)\n",
    "\n",
    "label = decode_predictions(yhat)\n",
    "label = label[0][0]\n",
    "\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preprocessing the data you created\n",
    "create a DataFrame with 2 columns:\n",
    "1. The exact path of each image\n",
    "2. The label of each image\n",
    "\n",
    "Split this data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['image_path', 'label'])\n",
    "for folder in listdir('./data'):\n",
    "    for im in listdir(path.join('./data', folder)):\n",
    "        df = df.append({'image_path': path.join('./data', folder, im),\n",
    "                  'label': folder}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data\\gal\\20150221_112656_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data\\gal\\20150221_124226_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data\\gal\\20150221_133047_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data\\gal\\20150221_135339_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data\\gal\\20150227_162125_HDR.jpg</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_path label\n",
       "0  ./data\\gal\\20150221_112656_HDR.jpg   gal\n",
       "1  ./data\\gal\\20150221_124226_HDR.jpg   gal\n",
       "2  ./data\\gal\\20150221_133047_HDR.jpg   gal\n",
       "3  ./data\\gal\\20150221_135339_HDR.jpg   gal\n",
       "4  ./data\\gal\\20150227_162125_HDR.jpg   gal"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use iterate through the dataframes and read the images corresponding to each row (using keras.preprocessing.image). Save the images as numpy arrays, and once again use utils.preprocess_input to preprocess the arrays.\n",
    "Save the labels in 2 different ways:\n",
    "1. Onehot encoding - for using a NN\n",
    "2. Serial number of the corresponding class (one number per label) - for using an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((len(data_train), input_size[0], input_size[1], n_channels))\n",
    "\n",
    "for i, row in enumerate(data_train.itertuples()):\n",
    "    image_path = row.image_path\n",
    "    \n",
    "    image = load_img(image_path, target_size=input_size)\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    # Shape: 1 x 224 x 224 x 3\n",
    "    \n",
    "    train_x[i, :, :, :] = image\n",
    "\n",
    "train_y_NN = pd.get_dummies(data_train['label'])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "train_y_SVM = encoder.fit_transform(data_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (212, 224, 224, 3)\n",
      "train_y_NN (212, 2)\n",
      "train_y_SVM (212,)\n"
     ]
    }
   ],
   "source": [
    "print('train_x', train_x.shape)\n",
    "print('train_y_NN', train_y_NN.shape)\n",
    "print('train_y_SVM', train_y_SVM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.zeros((len(data_test), input_size[0], input_size[1], n_channels))\n",
    "\n",
    "for i, row in enumerate(data_test.itertuples()):\n",
    "    image_path = row.image_path\n",
    "    \n",
    "    image = load_img(image_path, target_size=input_size)\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    # Shape: 1 x 224 x 224 x 3\n",
    "    \n",
    "    test_x[i, :, :, :] = image\n",
    "\n",
    "test_y_NN = pd.get_dummies(data_test['label'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "test_y_SVM = encoder.fit_transform(data_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_x (53, 224, 224, 3)\n",
      "test_y_NN (53, 2)\n",
      "test_y_SVM (53,)\n"
     ]
    }
   ],
   "source": [
    "print('test_x', test_x.shape)\n",
    "print('test_y_NN', test_y_NN.shape)\n",
    "print('test_y_SVM', test_y_SVM.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of keras.preprocessing.image.ImageDataGenerator, which will define how you will create augmentations of each original image you've created. Chose all the parameters on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rotation_range=90, width_shift_range=0.1, \n",
    "                               height_shift_range=0.1,zoom_range=0.2, \n",
    "                               horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally - transfer learning. \n",
    "### This should be done with the keras functional API (not Sequential)\n",
    "1. Create an instance of VGGFace with the model of your choice. \n",
    "2. Examine the model's architecture using .summary(). Understand what it means to replace the last 2 dense layers (including the final softmax layer). \n",
    "3. Use .get_layer() to retrieve the last layer which you want to keep in your new NN.\n",
    "4. Create the 2 new Dense layers which continue the previous pretrained layers (the last layer should have a softmax activation).\n",
    "5. Create a new model which the input of the original model as input, and outputs the new dense-softmax layer. You can use .input on the old model for this.\n",
    "6. Freeze all of the layers except the last 2 using .layers on the new model, and .trainable = False. This will stop you from training those layers.\n",
    "7. Compile the model ('adam' worked ok for us as an optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class = 2\n",
    "hidden_dim = 512\n",
    "\n",
    "vgg_model = keras.applications.VGG16()\n",
    "dense_1 = vgg_model.get_layer('fc2').output\n",
    "\n",
    "new_dense_1 = Dense(hidden_dim, activation='relu', name='fc3')(dense_1)\n",
    "out = Dense(nb_class, activation='softmax', name='fc4')(new_dense_1)\n",
    "custom_vgg_model = Model(vgg_model.input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_4\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block1_pool\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block2_pool\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_pool\n",
      "flatten\n",
      "fc1\n",
      "fc2\n"
     ]
    }
   ],
   "source": [
    "for layer in custom_vgg_model.layers[:-2]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vgg_model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "fc4 (Dense)                  (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 136,359,234\n",
      "Trainable params: 2,098,690\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "custom_vgg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to train the model:\n",
    "1. Use .fit_generator() and not .fit(), since you'll be using the augmentor you created.\n",
    "2. Use .flow() on the instance of ImageDataGenerator as the first input. \n",
    "3. Choose a combination on batch_size(within .flow) and steps_per_epoch which will create a total number of images that you want per each epoch. \n",
    "4. Use validation data which isn't augmented.\n",
    "\n",
    "How well is it doing? We got an accuracy of 90%-95% percent (validation data) on photos on the 2 of us (just 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 38s 3s/step - loss: 1.4762 - accuracy: 0.7951 - val_loss: 0.4824 - val_accuracy: 0.9057\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.6039 - accuracy: 0.8571 - val_loss: 0.0827 - val_accuracy: 0.9623\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 37s 3s/step - loss: 0.2925 - accuracy: 0.8889 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 35s 3s/step - loss: 0.1986 - accuracy: 0.9246 - val_loss: 0.0892 - val_accuracy: 0.9623\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.1483 - accuracy: 0.9524 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.1126 - accuracy: 0.9563 - val_loss: 0.0356 - val_accuracy: 0.9811\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.1271 - accuracy: 0.9444 - val_loss: 0.0637 - val_accuracy: 0.9623\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.1913 - accuracy: 0.9286 - val_loss: 0.0473 - val_accuracy: 0.9811\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0852 - accuracy: 0.9672 - val_loss: 0.0789 - val_accuracy: 0.9623\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 32s 2s/step - loss: 0.0718 - accuracy: 0.9713 - val_loss: 0.0512 - val_accuracy: 0.9623\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0618 - accuracy: 0.9722 - val_loss: 0.0315 - val_accuracy: 0.9811\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0590 - accuracy: 0.9841 - val_loss: 0.0619 - val_accuracy: 0.9623\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0629 - accuracy: 0.9802 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0607 - accuracy: 0.9802 - val_loss: 0.0970 - val_accuracy: 0.9623\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 32s 2s/step - loss: 0.0971 - accuracy: 0.9754 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0849 - accuracy: 0.9683 - val_loss: 0.0543 - val_accuracy: 0.9623\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0340 - accuracy: 0.9881 - val_loss: 0.0360 - val_accuracy: 0.9811\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0604 - accuracy: 0.9643 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0605 - accuracy: 0.9762 - val_loss: 0.0698 - val_accuracy: 0.9623\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0636 - accuracy: 0.9722 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0357 - accuracy: 0.9881 - val_loss: 0.0215 - val_accuracy: 0.9811\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 36s 3s/step - loss: 0.0465 - accuracy: 0.9762 - val_loss: 0.1098 - val_accuracy: 0.9623\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 35s 3s/step - loss: 0.0244 - accuracy: 0.9960 - val_loss: 0.1518 - val_accuracy: 0.9623\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0383 - accuracy: 0.9921 - val_loss: 0.1487 - val_accuracy: 0.9623\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0329 - accuracy: 0.9921 - val_loss: 0.0429 - val_accuracy: 0.9811\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0366 - accuracy: 0.9881 - val_loss: 0.0647 - val_accuracy: 0.9811\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9623\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0096 - accuracy: 0.9959 - val_loss: 0.0954 - val_accuracy: 0.9623\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0206 - accuracy: 0.9918 - val_loss: 0.0572 - val_accuracy: 0.9811\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0264 - accuracy: 0.9960 - val_loss: 0.1837 - val_accuracy: 0.9623\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 0.2644 - val_accuracy: 0.9623\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0373 - accuracy: 0.9881 - val_loss: 0.1528 - val_accuracy: 0.9623\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0625 - accuracy: 0.9802 - val_loss: 0.0490 - val_accuracy: 0.9811\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.1015 - val_accuracy: 0.9811\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0199 - accuracy: 0.9960 - val_loss: 0.1628 - val_accuracy: 0.9623\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0320 - accuracy: 0.9841 - val_loss: 0.1327 - val_accuracy: 0.9623\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0306 - accuracy: 0.9881 - val_loss: 0.0868 - val_accuracy: 0.9811\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0159 - accuracy: 0.9921 - val_loss: 0.2201 - val_accuracy: 0.9623\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0190 - accuracy: 0.9921 - val_loss: 0.2417 - val_accuracy: 0.9623\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0397 - accuracy: 0.9802 - val_loss: 0.1277 - val_accuracy: 0.9623\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0444 - accuracy: 0.9881 - val_loss: 0.2567 - val_accuracy: 0.9623\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0494 - accuracy: 0.9802 - val_loss: 0.0391 - val_accuracy: 0.9811\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0113 - accuracy: 0.9959 - val_loss: 0.0980 - val_accuracy: 0.9623\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 35s 3s/step - loss: 0.0325 - accuracy: 0.9881 - val_loss: 0.0901 - val_accuracy: 0.9623\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 38s 3s/step - loss: 0.0553 - accuracy: 0.9841 - val_loss: 0.1365 - val_accuracy: 0.9623\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0678 - accuracy: 0.9762 - val_loss: 0.1060 - val_accuracy: 0.9623\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0297 - accuracy: 0.9921 - val_loss: 0.0894 - val_accuracy: 0.9623\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0447 - accuracy: 0.9921 - val_loss: 0.1652 - val_accuracy: 0.9623\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0546 - accuracy: 0.9762 - val_loss: 0.1117 - val_accuracy: 0.9434\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 32s 2s/step - loss: 0.0484 - accuracy: 0.9877 - val_loss: 0.3950 - val_accuracy: 0.9623\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0379 - accuracy: 0.9881 - val_loss: 0.3496 - val_accuracy: 0.9623\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0241 - accuracy: 0.9921 - val_loss: 0.1259 - val_accuracy: 0.9623\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0434 - accuracy: 0.9802 - val_loss: 0.3545 - val_accuracy: 0.9623\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0088 - accuracy: 0.9960 - val_loss: 0.4378 - val_accuracy: 0.9434\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 0.9623\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0071 - accuracy: 0.9959 - val_loss: 0.2649 - val_accuracy: 0.9623\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0199 - accuracy: 0.9960 - val_loss: 0.2061 - val_accuracy: 0.9811\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 47s 4s/step - loss: 0.0126 - accuracy: 0.9921 - val_loss: 0.4016 - val_accuracy: 0.9623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 0.2139 - val_accuracy: 0.9623\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 35s 3s/step - loss: 0.0192 - accuracy: 0.9960 - val_loss: 0.3402 - val_accuracy: 0.9623\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0383 - accuracy: 0.9881 - val_loss: 0.1080 - val_accuracy: 0.9811\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0210 - accuracy: 0.9881 - val_loss: 0.2844 - val_accuracy: 0.9623\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0288 - accuracy: 0.9921 - val_loss: 0.3090 - val_accuracy: 0.9623\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 41s 3s/step - loss: 0.0396 - accuracy: 0.9841 - val_loss: 0.3822 - val_accuracy: 0.9623\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0596 - accuracy: 0.9762 - val_loss: 0.0445 - val_accuracy: 0.9811\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0142 - accuracy: 0.9877 - val_loss: 0.3922 - val_accuracy: 0.9623\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0660 - accuracy: 0.9722 - val_loss: 0.1883 - val_accuracy: 0.9623\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 35s 3s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3723 - val_accuracy: 0.9623\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0847 - accuracy: 0.9762 - val_loss: 0.1474 - val_accuracy: 0.9623\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 32s 2s/step - loss: 0.0304 - accuracy: 0.9836 - val_loss: 0.2597 - val_accuracy: 0.9434\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0508 - accuracy: 0.9841 - val_loss: 0.0813 - val_accuracy: 0.9434\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 32s 2s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 0.9623\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0069 - accuracy: 0.9960 - val_loss: 0.0415 - val_accuracy: 0.9811\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0083 - accuracy: 0.9960 - val_loss: 0.0313 - val_accuracy: 0.9811\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.1074 - val_accuracy: 0.9623\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9623\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0201 - accuracy: 0.9921 - val_loss: 0.1153 - val_accuracy: 0.9623\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0196 - accuracy: 0.9921 - val_loss: 0.0382 - val_accuracy: 0.9623\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 32s 2s/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.1640 - val_accuracy: 0.9623\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0096 - accuracy: 0.9921 - val_loss: 0.1334 - val_accuracy: 0.9623\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0371 - accuracy: 0.9921 - val_loss: 0.1487 - val_accuracy: 0.9623\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0213 - accuracy: 0.9881 - val_loss: 0.2219 - val_accuracy: 0.9623\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0606 - accuracy: 0.9881 - val_loss: 0.4589 - val_accuracy: 0.9623\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0300 - accuracy: 0.9921 - val_loss: 0.1770 - val_accuracy: 0.9623\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 35s 3s/step - loss: 0.0218 - accuracy: 0.9881 - val_loss: 0.2802 - val_accuracy: 0.9434\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 6.1793e-04 - accuracy: 1.0000 - val_loss: 0.3008 - val_accuracy: 0.9434\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0342 - accuracy: 0.9921 - val_loss: 0.1756 - val_accuracy: 0.9434\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9623\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9434\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9623\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1299 - val_accuracy: 0.9623\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9623\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1467 - val_accuracy: 0.9623\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2847 - val_accuracy: 0.9623\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 0.9623\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.1539 - val_accuracy: 0.9623\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.3082 - val_accuracy: 0.9623\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 33s 3s/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1478 - val_accuracy: 0.9623\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0453 - accuracy: 0.9881 - val_loss: 0.2321 - val_accuracy: 0.9623\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 34s 3s/step - loss: 0.0943 - accuracy: 0.9762 - val_loss: 0.4950 - val_accuracy: 0.9434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2621acb3be0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_vgg_model.fit_generator(train_gen.flow(train_x, train_y_NN, batch_size=20, seed=77), \n",
    "                               steps_per_epoch=13, epochs=100, validation_data=(test_x, test_y_NN), \n",
    "                               workers=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar exercise with SVM\n",
    "1. Now create a new model which outputs the data from the last frozen layer. Use this model to create feature for each image.\n",
    "2. Create a SVM classifier using these features. We used sklearn.svm.SVC.\n",
    "3. Did this work better or less good? Evaluate using sklearn.metrics.classification_report or sklearn.metrics.accuracy_score.\n",
    "4. Try the same thing with the one-before-last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats! You completed the exercise. \n",
    "# Bonus - use one of these models and a webcam to created a personalized security system for your house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
