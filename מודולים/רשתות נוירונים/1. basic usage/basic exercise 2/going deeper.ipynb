{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoKh0HAYq-ZS"
   },
   "source": [
    "# Basics of Neural networks and other stuff\n",
    "```In this exercise you will experience working with keras, a useful tool for designing and training neural networks.```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lomzfgRq-ZT"
   },
   "source": [
    "## Stage 1- predicting engineered functions\n",
    "```Here you will design a simple fully connected network to predict a few simple functions. You will try different activation functions and different architectures (number of layers, size of layers).```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9EieSkpq-ZV"
   },
   "source": [
    "```The first experiment will be guided:```\n",
    "- ```Read the first dataframe - function_1.csv.```\n",
    "- ```Plot y against x. can you guess the function y(x)?```\n",
    "- ```Split you data to train segment (70%) and test segment (30%).```\n",
    "- ```Write a fully connected neural network with one hidden layer with 3 units. I suggest using the``` [functional API](https://keras.io/getting-started/functional-api-guide/) - ```you can also find there examples for simple working with keras.```\n",
    "- ```Use tanh as an activation function for the hidden layer and a linear activation for the output layer.```\n",
    "- ```Use model.summary() to look at your model's architecture.```\n",
    "- ```Use mean squared error as the loss function and SGD (stochastic gradient descent) as the optimizer.```\n",
    "- ```Try training the network with different batch's sizes (don't be afraid to use many epochs- you don't have a lot of data).```\n",
    "- ```Plot y against x and f(x) against x on the same graph.```\n",
    "- ```Compute the loss on the test segment.```\n",
    "- ```Can you use a smaller hidden layer?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PURF2UDGq-ZW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MjwVu9zq-Ze"
   },
   "source": [
    "```For the second experiment use the dataframe function_2.csv. Here you might need to have more layers. You might want to consider different``` [optimizers](https://keras.io/optimizers/), ```rather than SGD (for example, Adam).```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Be2DgmtIq-Zj"
   },
   "source": [
    "```For the third experiment use the dataframe function_3.csv. Try different activation functions. How many layers you had to take?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOpmc2l3q-Zk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUt2PlCPq-Zn"
   },
   "source": [
    "```As you maybe could've guessed, the function you fitted is``` $y = sin(x)$. ```Hence one might think it would be easy to fit a network with only one neuron, with activation of the sinus function. Try it: define a neural network with a single input and a single output, with no hidden layers, and use a sinus activation, to approximate function_3. Next we will understand why using sinus as an activation function is not a good idea.```\n",
    "\n",
    "```You can use sin_activation provided below as the sinus activation: instead of writing activation='tanh' or activation='sigmoid', use activation=sin_activation.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4zronM5q-Zo"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def sin_activation(vec):\n",
    "    return K.sin(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH-Jecp3q-Zs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6K91Xmdq-Zv"
   },
   "source": [
    "```Let's explore the sinus activation. For this we will simplify the settings. We will generate 3 samples, with ```\n",
    "$x = [0, 0.5, 1]$ ``` and ``` $y=[sin(0), sin(0.5), sin(1)]$. ```We will keep using the simplest neural network, that has one input and one output, with no hidden layers. Hence the possible functions that the network can give us are functions of the form```\n",
    "<center>$y=sin(a\\cdot x+b)$.<center>\n",
    "\n",
    "```We can use the simple setting to compute and visualize the loss surface. As you recall, the loss is given by```\n",
    "<center>$\\sum_i{(sin(a\\cdot x_i+b)-y_i)^2}$<center>\n",
    "\n",
    "```Create a matrix so that in the (i,j) entry there will be the loss for ``` $a = \\frac{2\\pi}{600}\\cdot i$ and $b= \\frac{2\\pi}{600}\\cdot j$. ```Visualize this matrix using plt.imshow.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfo5Z5rUq-Zx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHaqddUWq-Z1"
   },
   "source": [
    "```Do the same for the beloved activation, tanh. Based on the images, what makes sinus a poor activation function and tanh a good activation function? answer in ``` $\\underline{a\\ cell\\ below}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DoYjvKpq-Z3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiu2ibbwq-Z6"
   },
   "source": [
    "### Bonus:\n",
    "```When training neural networks using keras we can use callback functions: There are functions that are called automatically during the training and can be of several usages. For example, we can use a callback function to save our model to a file upon the ending of an epoch.\n",
    "Below you will find a custom callback that keeps the weights for a and b in your sinus model. Use it like this:```\n",
    "\n",
    "``` python\n",
    "wh = WeightsHistory()\n",
    "model.fit(X_train, Y_train, batch_size = 100, epochs = 10, verbose=2, callbacks=[wh])\n",
    "```\n",
    "\n",
    "```After you try training your model using the sinus activation you can plot the path your model took on the a-b plane. Add to your loss surface visualization a plot showing the a-b values during training.```\n",
    "\n",
    "```Note: after using plt.imshow you got an image of the size``` $600\\times 600$. ```You will have to scale your a-b path accordingly to see it on the same graph. Try zooming (by changing the scale of the image, enlarging the size of your image or using any other mean) to get a better view of the a-b path. Can you explain why didn't you get a convergence?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mu-uSUTeq-Z7"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class WeightsHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.a_s = []\n",
    "        self.b_s = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        weights = self.model.get_weights()\n",
    "        self.a_s.append(weights[0][0][0])\n",
    "        self.b_s.append(weights[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIPMphb0q-aC"
   },
   "source": [
    "## Stage 2- predict digits from MNIST\n",
    "```Use your knowledge to create a good prediction for the MNIST dataset. Note that this a classification problem, and you will have to use a different loss: for example, the binary cross entropy (log loss). Furthermore, you might want to use softmax to generate predictions. You can use activation='softmax' in the last layer of your network.\n",
    "good luck!```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znq2DY9eq-aD"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train_images, Y_train_num), (X_test_images, Y_test_num) = mnist.load_data()\n",
    "plt.imshow(X_train_images[0])\n",
    "plt.show()\n",
    "\n",
    "X_train = X_train_images.reshape(-1,784)\n",
    "X_test = X_test_images.reshape(-1,784)\n",
    "Y_train = pd.get_dummies(Y_train_num)\n",
    "Y_test = pd.get_dummies(Y_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ncr7FeEfq-aG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZ-Nk2LEq-aJ"
   },
   "source": [
    "## Stage 3 - exploring deep neural networks\n",
    "```In this part we will explore the problems with training deep networks.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4SwyYEuq-aL"
   },
   "source": [
    "```We will work with the simplest data possible and will try to approximate the identity function, with one feature.\n",
    "Generate 10,000 samples using np.random.random and have ``` $y=x$. ```Create a neural network with a single hidden layer with 3 neurons in it and sigmoid activation. Approximate the identity function.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrbRTjLLq-aM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YU1Wtjtq-aR"
   },
   "source": [
    "```Add 50 hidden layers to your network, again, with 3 neurons in each layer and sigmoid activation. Try approximating the identity function using this network.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TD7g37ooq-aT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5ILOzfqq-aY"
   },
   "source": [
    "```Why isn't it working? Explain it in ``` $\\underline{a\\ cell\\ below}$. ```In your explanation regard the process of back propagation and the formula of gradients found in the first layers of the network. Regard also the possible values of the derivative of the sigmoid function.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6X5Pc0Oq-ac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKhOG2JBq-af"
   },
   "source": [
    "```Run the cell below. In weight_grads you will have a list of the gradients computed in every layer. For each layer take the maximum absolute value of the gradients. Plot a graph of the maximum values against the number of layer. Also plot it in logarithmic scale. Use it to justify your answer from before in ``` $\\underline{a\\ cell\\ below}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqtwBOlCq-ag"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "def get_weight_grad(model, inputs, outputs):\n",
    "    \"\"\" Gets gradient of model for given inputs and outputs for all weights\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad\n",
    "\n",
    "weight_grads = get_weight_grad(model, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcNq8pN2q-al"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - sensitivity of neural networks\n",
    "```In this part we will meet underestimated problems of neural network training.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train a neural network without any hidden layers (that is, only input and output layer) to distinguish between '1' and '0', That is, train a neural network with a sigmoid activation to predict '0' for '0' and '1' for '1'.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Do the same, only this time try to distinguish between 0.4 and 0.41. Is it still an easy task? Try different optimizers, or different architectures.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Time to draw some conclusions: explain to yourself and to your tutor the effect you've just witnessed.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "going deeper.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
