{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remember to save this as a new notebook before you begin solving!!\n",
    "# Also remember to open the notebook through a virtual env that works well with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise is meant to teach you the very beginning of working with neural networks. You will generate your own data for this exercise\n",
    "\n",
    "##### Author: Philip Tannor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll start off with creating the data: \n",
    "### 1) Generate a numpy array named 'X', which contains 10000 random numbers in the range (0,1)\n",
    "### 2) Create a vector named 'y_easy', which contains the vector sqrt (X*3 +5). This is a function that will be very easy for a neural network to \"understand\".\n",
    "### 3) Create a vector named 'y_difficult', which contains the sine (sinus) of the following function (applied to X):\n",
    "###### def weird_change(x):\n",
    "       if x>0.5:\n",
    "            return 1+x\n",
    "        if x<0.3:\n",
    "            return -x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.random(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_easy = np.sqrt(X*3 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weird_change(x):\n",
    "    if x>0.5:\n",
    "        return 1+x\n",
    "    if x<0.3:\n",
    "        return -x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hard = np.sin(np.pi * np.array([weird_change(x) for x in list(X)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we create the simplest possible neural network:\n",
    "##### 1) input layer with 1 unit and output layer (dense) with 1 unit \n",
    "##### 2) no activation function\n",
    "##### Use this NN to try to predict y_easy from X (why does this work? discuss this with your tutor)\n",
    "#### For this you need to import Model from keras.models, and the layers you need from keras.layers. You'll have to decide on the following parameters: which optimizer, which loss, and batch size. It should be fairly straightforward, but this also may be worth discussing with your instructor. Run 100 epochs (make sure you know what this means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape = (1,))\n",
    "output_layer = Dense(units = 1)(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs = [input_layer], outputs = [output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 11.3693\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 10.3774\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 9.4531\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 8.5924\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 7.7918\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 7.0479\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 6.3577: 0s - loss: 6.363\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 5.7187\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 5.1281\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 4.5839\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 4.0834\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 3.6244\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 3.2044\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 2.8215\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2.4735\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 2.1583\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 1.8741\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1.6186\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1.3902\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1.1869\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1.0069\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.8484\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.7097\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.5891\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.4850\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.3958\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.3201\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.2562\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.2030\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.1590\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.1231\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0941\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.0709\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0527\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.0386\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0278\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0197\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0137\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0094\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0063\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0041\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0027\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0017\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0011\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 6.4493e-04\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 3.9335e-04\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 2.4177e-04\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.5299e-04\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.0248e-04\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 7.4446e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19aab73e390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y_easy, batch_size=100, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now take a glimpse at y_easy, and a glimpse at the predictions of your model. Are they similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.27461957, 2.75962714, 2.40047132, ..., 2.24764002, 2.43311996,\n",
       "       2.24985724])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2869031, 2.753409 , 2.399318 , ..., 2.263592 , 2.4294693,\n",
       "       2.2654972], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create another neural network with the exact same architecture. Use it to try to predict y_hard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer_2 = Input(shape = (1,))\n",
    "output_layer_2 = Dense(units = 1)(input_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_2 = Model(inputs = [input_layer_2], outputs = [output_layer_2])\n",
    "model_2.compile(optimizer='Adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.4900\n",
      "Epoch 2/40\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.4854\n",
      "Epoch 3/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4808\n",
      "Epoch 4/40\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.4764\n",
      "Epoch 5/40\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.4720\n",
      "Epoch 6/40\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.4679\n",
      "Epoch 7/40\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.4638\n",
      "Epoch 8/40\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.4600\n",
      "Epoch 9/40\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.4563\n",
      "Epoch 10/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4528\n",
      "Epoch 11/40\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.4495\n",
      "Epoch 12/40\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.4465\n",
      "Epoch 13/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4436\n",
      "Epoch 14/40\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.4409\n",
      "Epoch 15/40\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.4384\n",
      "Epoch 16/40\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.4361\n",
      "Epoch 17/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4340\n",
      "Epoch 18/40\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.4321\n",
      "Epoch 19/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4304\n",
      "Epoch 20/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4289\n",
      "Epoch 21/40\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.4275\n",
      "Epoch 22/40\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.4263\n",
      "Epoch 23/40\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.4252\n",
      "Epoch 24/40\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.4243\n",
      "Epoch 25/40\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.4235\n",
      "Epoch 26/40\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.4227\n",
      "Epoch 27/40\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.4222\n",
      "Epoch 28/40\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.4216\n",
      "Epoch 29/40\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.4212\n",
      "Epoch 30/40\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.4209\n",
      "Epoch 31/40\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.4205\n",
      "Epoch 32/40\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.4203\n",
      "Epoch 33/40\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.4201\n",
      "Epoch 34/40\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.4200\n",
      "Epoch 35/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4198\n",
      "Epoch 36/40\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.4198\n",
      "Epoch 37/40\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4196\n",
      "Epoch 38/40\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 0.4196\n",
      "Epoch 39/40\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.4195\n",
      "Epoch 40/40\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 0.4195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19b91b29c88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x=X, y=y_hard, batch_size=100, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why didn't  this work? Discuss this matter with your Instructor, and then proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create a new NN with the same input and output layers, but add 3 Dense layers between them with 5 units each. For these layers you should choose a non-linear activation function. Does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer_3 = Input(shape = (1,))\n",
    "dense_1 = Dense(units = 5, activation='tanh')(input_layer_3)\n",
    "dense_2 = Dense(units = 5, activation='tanh')(dense_1)\n",
    "dense_3 = Dense(units = 5, activation='tanh')(dense_2)\n",
    "output_layer_3 = Dense(units = 1)(dense_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_3 = Model(inputs = [input_layer_3], outputs = [output_layer_3])\n",
    "model_3.compile(optimizer='Adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 80us/step - loss: 0.4336\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.4215\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.4203\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.4199\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.4198\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.4199\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.4192\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.4192\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.4187\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.4185\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.4184\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.4182\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.4181\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.4181\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.4174\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.4168\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.4165\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 0.4163\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.4153\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.4149\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.4147\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 0.4134\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.4119\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.4101\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.4071\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 50us/step - loss: 0.4004\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.3825: 0s - loss\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.3268\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.2445\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 50us/step - loss: 0.1879\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.1572\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1369\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.1231\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1138\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.1071\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 0.1026\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.1001\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0980\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.0964\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.0951\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0948\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0934\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0923\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0920\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0911\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0896\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0890\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0873\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0852\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.0835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19adea7f588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X, y_hard, epochs = 50, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well done! \n",
    "### Now I want you to test the limits of this NN. Try to find a function which won't be learned well by this NN, and show that your NN doesn't manage to train on it. Don't use the old model (create a new, identical one - in new cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer_4 = Input(shape = (1,))\n",
    "dense_1 = Dense(units = 5, activation='tanh')(input_layer_4)\n",
    "dense_2 = Dense(units = 5, activation='tanh')(dense_1)\n",
    "dense_3 = Dense(units = 5, activation='tanh')(dense_2)\n",
    "output_layer_4 = Dense(units = 1)(dense_3)\n",
    "model_4 = Model(inputs = [input_layer_4], outputs = [output_layer_4])\n",
    "model_4.compile(optimizer='Adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this should be hard for a NN to learn, since NN's aren't good at learning periodic functions. Sine was learned well before \n",
    "#(in y_hard) \"by chance\", since we only took a small area of the sine (in which it wasn't periodic)\n",
    "y_hardest = np.sin(X*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 0.6266\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.5230\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5137\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.5104\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 0.5090\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 0.5081\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 0.5072\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.5070\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s 79us/step - loss: 0.5063\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.5065\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5061\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.5063\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.5061\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.5060\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.5054\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.5058\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.5053\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.5052\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.5055\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.5051\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.5053\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.5052\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.5051\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.5050\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.5050\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.5052\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5050: 0s - loss\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.5047\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.5051\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.5050\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5052\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.5048\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5053\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.5049\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.5049\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.5049\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.5050\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.5051\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.5049\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.5053\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5048\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.5047\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5051\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.5050\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.5049\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.5047\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.5053\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.5052\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.5049\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.5049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19b91a8ca90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X, y_hardest, epochs = 50, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### You're basically finished, but if you're still curious about the architecture of the NN, try playing with it - and see how the changes affect the learning process (on y_hard). You can change the # of hidden layers, # of units in each layer, activation function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forum4",
   "language": "python",
   "name": "forum4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
