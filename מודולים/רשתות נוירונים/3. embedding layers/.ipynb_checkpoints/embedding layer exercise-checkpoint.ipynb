{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember to save this as a new notebook before you begin solving!! \n",
    "# Also remember to open the notebook through a virtual env that works well with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise is meant to teach you how to use embedding layers, and how to create a recommendation system. The data we'll use is the data from the Netflix Prize. This exercise should come after you have some experience with NN (not necessarily extensive experience)\n",
    "\n",
    "### Author: Philip Tannor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description: The first line of each 'batch of movies' contains the movie_id followed by a colon. Each subsequent line in the file corresponds to a rating from a customer and its date in the following format:\n",
    "CustomerID,Rating,Date\n",
    "MovieIDs range from 1 to 17770 sequentially. \n",
    "CustomerIDs range from 1 to 2649429, with gaps. \n",
    "There are 480189 users. \n",
    "Ratings are on a five star (integral) scale from 1 to 5. \n",
    "Dates have the format YYYY-MM-DD.\n",
    "\n",
    "#### Note that originally the data was stored with one movie per file. This is a new, easier, format created by a kaggler (DLao @ Hong Kong) with a later touch of Ittai Haran.\n",
    "Since I don't think that arranging the data teaches very much, I left the basic lines of preprocessing that I did. Feel free to delete it and start over - just make sure to split the data on your own to train and test (randomly). \n",
    "If you're more hard-working than I am - used the dates in the original data to split in a more realistic manner (regarding time). Notice there is an apparent leakage - since we create the dictionaries base on all of the data. This is intentional, and also realistic (we can't use these ML techniques for new users or movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'grade', 'date', 'num'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"resources/combined_data.csv\")#, nrows = 100000)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100480502</th>\n",
       "      <td>1790158</td>\n",
       "      <td>17770</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100480503</th>\n",
       "      <td>1608708</td>\n",
       "      <td>17770</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100480504</th>\n",
       "      <td>234275</td>\n",
       "      <td>17770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100480505</th>\n",
       "      <td>255278</td>\n",
       "      <td>17770</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100480506</th>\n",
       "      <td>453585</td>\n",
       "      <td>17770</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id  movie_id  rating\n",
       "100480502  1790158     17770       4\n",
       "100480503  1608708     17770       3\n",
       "100480504   234275     17770       1\n",
       "100480505   255278     17770       4\n",
       "100480506   453585     17770       2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we drop the date since this isn't used for the embedding model\n",
    "df = df.rename(columns = {'num': 'movie_id', 'grade': 'rating'}).drop('date', axis = 1)[['user_id', 'movie_id', 'rating']]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100480507, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Dataset shape: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('rating', axis = 1), df.rating, test_size = 0.3, random_state = 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell creates dictionaries which help with changing the unique ID's into series of int's. \n",
    "#This is neccessary for embedding layers in keras (so use the transformed columns later on), \n",
    "#especially if you only use part of the data.\n",
    "\n",
    "list_unique = list(set(df.user_id))\n",
    "transforming_user = {v:k for k,v in zip(range(len(list_unique)), list_unique)}\n",
    "list_unique = list(set(df.movie_id))\n",
    "transforming_movie = {v:k for k,v in zip(range(len(list_unique)), list_unique)}\n",
    "\n",
    "X_train['user_id_transformed'] = X_train['user_id'].apply(lambda x: transforming_user[x])\n",
    "X_train['movie_id_transformed'] = X_train['movie_id'].apply(lambda x: transforming_movie[x])\n",
    "\n",
    "X_test['user_id_transformed'] = X_test['user_id'].apply(lambda x: transforming_user[x])\n",
    "X_test['movie_id_transformed'] = X_test['movie_id'].apply(lambda x: transforming_movie[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK, now go through the instructions - and then you'll be on your own for a while. \n",
    "\n",
    "1. Read about embedding layers here: https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12. The explanation isn't detailed enough, but you should try to think of the layer as a change of representation - from a very sparse one-hot vector (which is equivalent), to a dense vector of much lower dimension. In the new representation, each element really means something - and sometimes these elements may be translated into a scale which correlated with some intuitive feature (age, connection holocaust content, spendable income, etc.). \n",
    "2. Pay attention that this function which creates the change of representation (the embedding layer) is learned as the network trains. This means that at the beginning - the elements in the new representation won't mean much, but as the learning progresses the representation will be more and more meaningful.\n",
    "3. Create a neural network in Keras (use functional API - *NOT SEQUENTIAL*. This will become important later on). The NN should have 2 different embedding layers - one which get the user_id as inputs, and one that gets the movie_id as input. I won't tell you the dimension of the 2 new representations - play around with these 2 numbers. Intuitively - try to think how many number you would need to describe a user (same thing for a movie). Notice that 'input_dim' is the size of the vocabulary (and not '1').\n",
    "4. These embedding layers should be merged, and then flattened. After this, add a few dense layers. If you get the hang of the training and don't want to wait too long, or if you want to compare to my model, read the model in the hidden answers folder using keras.model.load_model. TL;DR - the saved model reaches an MSE of a bit less than 0.72.\n",
    "5. The output should be a single number - this can be treated as a regression problem, or as an ordinal classification problem. Originally, it was treated as a regression problem by Netflix, so your output should be a floating number between 1 and 5 and you should minimize the MSE.\n",
    "6. Bonus: treat this as an ordinal classification problem, and use the loss from this paper (squared EMD): https://arxiv.org/abs/1611.05916. This bonus shouldn't be attempted if this is the first time you've dealt with custom losses or ordinal classification. Also, notice there is more work to be done later on in the notebook (which isn't \"a bonus\")!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480189\n",
      "17770\n"
     ]
    }
   ],
   "source": [
    "list_unique_user_id = list(set(df.user_id))\n",
    "print(len(list_unique_user_id))\n",
    "\n",
    "list_unique_movie_id = list(set(df.movie_id))\n",
    "print(len(list_unique_movie_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 20)        9603780     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 60)        1066200     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 80)        0           embedding_4[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 80)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 80)           6480        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 80)           6480        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 80)           6480        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            81          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,689,501\n",
      "Trainable params: 10,689,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, concatenate, Flatten\n",
    "\n",
    "user_dim = 20\n",
    "movie_dim = 60\n",
    "\n",
    "x_user = Input(shape = (1,))\n",
    "embedding_user = Embedding(input_dim = len(list_unique_user_id), output_dim = user_dim, dtype='float32', input_length=1)(x_user)\n",
    "\n",
    "x_movie = Input(shape = (1,))\n",
    "embedding_movie = Embedding(input_dim = len(list_unique_movie_id), output_dim = movie_dim, dtype='float32', input_length=1)(x_movie)\n",
    "\n",
    "concat = concatenate([embedding_user, embedding_movie], axis = -1)\n",
    "\n",
    "flatten = Flatten()(concat)\n",
    "dense1 = Dense(units = (user_dim + movie_dim), activation = 'tanh')(flatten)\n",
    "dense2 = Dense(units = (user_dim + movie_dim), activation = 'tanh')(dense1)\n",
    "dense3 = Dense(units = (user_dim + movie_dim), activation = 'tanh')(dense2)\n",
    "output_layer = Dense(units = 1, activation = 'linear')(dense3)\n",
    "\n",
    "model = Model(inputs = [x_user, x_movie], outputs = output_layer)\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b0580ef24c82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit(x=[X_train['user_id_transformed'], X_train['movie_id_transformed']], y=y_train, \n\u001b[0m\u001b[0;32m      2\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id_transformed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movie_id_transformed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m          epochs=10, verbose=2, batch_size = 1000)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=[X_train['user_id_transformed'], X_train['movie_id_transformed']], y=y_train, \n",
    "         validation_data=[[X_test['user_id_transformed'], X_test['movie_id_transformed']], y_test],\n",
    "         epochs=10, verbose=2, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! If you've reached here, you manged to create a neural network which uses embeddings and can predict movie rankings using only very basic information (unique ID's). \n",
    "Now we want to turn this into a recommendation system. Write a function which gets one specific user_id as an input, and outputs 10 names of movies with the highest expected ratings. Read the file named movie_titles.csv to connect between the movie id's and the names of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('resources/data/movie_titles.csv', header = None)#, names = ['movie_id', 'year', 'name'])\n",
    "movies_dict = {v: k for v, k in zip(movies_df[0], movies_df[2])}\n",
    "untransforming_movie = {v: k for k, v in transforming_movie.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_10(user):\n",
    "    recommend_df = pd.DataFrame(list(set(X_test.movie_id_transformed)), columns = ['movie_id_transformed'])\n",
    "    recommend_df['user_id_transformed'] = [transforming_user[user]] * len(recommend_df)\n",
    "    recommend_df['predictions'] = model.predict([recommend_df['user_id_transformed'], recommend_df['movie_id_transformed']])\n",
    "    top_10 = recommend_df.sort_values('predictions', ascending = False).head(10)\n",
    "    top_10['movie_id'] = top_10.movie_id_transformed.apply(lambda x: untransforming_movie[x])\n",
    "    return top_10.movie_id.apply(lambda x: movies_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_10(2242835)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So is the system we built any good? We can check this at least somewhat by checking if the embeddings are good. This is how we'll do it: \n",
    "1. Extract the embeddings of the different movies from the network. How, you may ask? I hope you remembered to use the functional api. This will allow you to create a new model - using the trained layers that you already used. Create a new Model where the input is the movie_id, and the output is the movie embedding layer (before the merge).\n",
    "2. Do not train this model. Only use the .predict of this model, and the output should be the embedding vectors which represent the different movie id's. Notice you may have to reshape the matrix of the predictions for the next steps.\n",
    "3. Now use sklearn.cluster.KMeans to cluster these vectors. Use k=1000. Make sure to save the cluster number for each movie.\n",
    "4. Check manually if the clusters make sense (can you find connections between movies in the same cluster?). \n",
    "5. Try to visualize the clustering by looking at only some of the clusters as one time. You can use PCA with n_components = 2 to help you visualize (use a different color for each cluster by using 'c = ...' in plt.scatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = Model(inputs = [x_movie], outputs = [embedding_movie])\n",
    "vectors = model_embedding.predict(movies_for_pred).reshape(-1,1)\n",
    "vectors = vectors.reshape(-1,movie_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clf = KMeans(n_clusters=1000)\n",
    "clusters = clf.fit_predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_transformed_to_cluster = dict(zip(movies_for_pred, clusters))\n",
    "clusters_to_take = range(10)\n",
    "\n",
    "vectors_chosen = vectors[map(lambda x: x in clusters_to_take, clusters)]\n",
    "pca = PCA(n_components = 2)\n",
    "pcaed_vectored = pca.fit_transform(vectors_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pcaed_vectored[:,0], pcaed_vectored[:,1], c=filter(lambda x: x in clusters_to_take, clusters))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! Now just one little sophistication. \n",
    "Write another function which gives me the top 10 movies while no 2 of them are from the same cluster. This can be used for a more sophisticated type of recommendation system.\n",
    "\n",
    "This will be more exciting if you go back and reduce the amount of clusters you allow to be not much higher than 10 (and check how the previous system you created will react)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_10_from_different_clusters(user):\n",
    "    recommend_df = pd.DataFrame(list(set(X_test.movie_id_transformed)), columns = ['movie_id_transformed'])\n",
    "    recommend_df['user_id_transformed'] = [transforming_user[user]] * len(recommend_df)\n",
    "    recommend_df['predictions'] = model.predict([recommend_df['user_id_transformed'], recommend_df['movie_id_transformed']])\n",
    "    recommend_df['cluster'] = recommend_df.movie_id_transformed.apply(lambda x: movie_id_transformed_to_cluster[x])\n",
    "    top_10 = recommend_df.sort_values('predictions', ascending = False).groupby('cluster').head(1).head(10)\n",
    "    top_10['movie_id'] = top_10.movie_id_transformed.apply(lambda x: untransforming_movie[x])\n",
    "    return top_10.movie_id.apply(lambda x: movies_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_10_from_different_clusters(2242835)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These cells should be used for telling yourself when your code finishes running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from MMMUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()\n",
    "\n",
    "body = 'Yo Phil, my man - your code (embedding layer exercise) finished running at: ' + str(datetime.datetime.now()) \\\n",
    "    + '\\n\\n\\nThis was an automated email'\n",
    "send_email('ptannor@gmail.com', body = body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
