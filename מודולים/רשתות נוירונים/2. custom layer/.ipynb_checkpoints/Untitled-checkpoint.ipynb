{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parental-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "lesbian-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subsequence(sequence, subsequence):\n",
    "    idx = 0\n",
    "    for char in sequence:\n",
    "        if char == subsequence[idx]:\n",
    "            idx += 1\n",
    "        if idx == len(subsequence):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "minor-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [0, 1, 2, 3, 4, 5, 6]\n",
    "padding_length = 25\n",
    "\n",
    "def create_seq(baseline_in, padding_length, p=0.5, padding_symbol=2):\n",
    "    baseline_length_in = len(baseline_in)\n",
    "    baseline_places = []\n",
    "    seq = []\n",
    "    count = 0\n",
    "    while count<baseline_length_in:\n",
    "        if np.random.random()<p:\n",
    "            digit_copy = digits.copy()\n",
    "            digit_copy.remove(baseline_in[count])\n",
    "            seq.append(np.random.choice(digit_copy, 1)[0])\n",
    "        else:\n",
    "            baseline_places.append(len(seq))\n",
    "            seq.append(baseline_in[count])\n",
    "            count += 1\n",
    "\n",
    "    if len(seq) < padding_length:\n",
    "        seq += [padding_symbol] * (padding_length-len(seq))\n",
    "        \n",
    "    return seq, baseline_places\n",
    "\n",
    "def create_wrong_seq(padding_length, baseline_in, padding_symbol=2):\n",
    "    do_again=True\n",
    "    while do_again is True:\n",
    "        len_seq = np.random.randint(1, 10)\n",
    "\n",
    "        seq = np.random.choice(digits, len_seq).tolist()\n",
    "        seq += [padding_symbol] * (padding_length-len(seq))\n",
    "        \n",
    "        do_again = find_subsequence(seq, baseline_in)\n",
    "    return seq, []\n",
    "    \n",
    "def create_sequences(count, baseline_in):\n",
    "    return map(lambda x: create_seq(baseline_in, padding_length=padding_length), range(count))\n",
    "\n",
    "\n",
    "def create_wrong_sequences(count, baseline_in):\n",
    "    return map(lambda x: create_wrong_seq(padding_length=padding_length, \n",
    "                                          baseline_in=baseline_in), range(count))\n",
    "\n",
    "baseline_length = 3\n",
    "right_baseline = [1,3,2]\n",
    "\n",
    "good_list = list(create_sequences(10000, right_baseline))\n",
    "wrong_list = list(create_wrong_sequences(10000, right_baseline))\n",
    "\n",
    "data = good_list + wrong_list\n",
    "data, places = list(map(lambda x: x[0], data)), list(map(lambda x: x[1], data))\n",
    "target = [True]*10000+[False]*10000\n",
    "\n",
    "good_data = good_list\n",
    "good_data, places = list(map(lambda x: x[0], good_data)), list(map(lambda x: x[1], good_data))\n",
    "        \n",
    "data = np.array(data)\n",
    "good_data = np.array(good_data)\n",
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "special-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 3 5 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(good_data[1946])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "dimensional-principle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(data[17541])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "exact-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (20000, 25, 1)\n",
      "good_data (10000, 25, 1)\n",
      "target (20000,)\n"
     ]
    }
   ],
   "source": [
    "data = data.reshape(data.shape[0], data.shape[1], 1)\n",
    "print('data', data.shape)\n",
    "\n",
    "good_data = good_data.reshape(good_data.shape[0], good_data.shape[1], 1)\n",
    "print('good_data', good_data.shape)\n",
    "\n",
    "print('target', target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "spread-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "data_train, data_val, target_train, target_val = train_test_split(data, target,\n",
    "                                                                  test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "necessary-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(data_train)\n",
    "BATCH_SIZE = 200\n",
    "steps_per_epoch = len(data_train)//BATCH_SIZE\n",
    "units = 64\n",
    "\n",
    "data_train = tf.convert_to_tensor(data_train, dtype=tf.dtypes.float32)\n",
    "good_data = tf.convert_to_tensor(good_data, dtype=tf.dtypes.float32)\n",
    "data = tf.convert_to_tensor(data, dtype=tf.dtypes.float32)\n",
    "target_train = tf.convert_to_tensor(target_train, dtype=tf.dtypes.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_train, target_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "special-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units), dtype=tf.dtypes.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "enhanced-vault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([200, 25, 1]), TensorShape([200]))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "photographic-mortality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (200, 25, 64)\n",
      "Encoder Hidden state shape: (batch size, units) (200, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "contemporary-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, sequence_length, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, sequence_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "variable-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (200, 64)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (200, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "organized-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        #     self.fc = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([context_vector, x], axis=-1)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        x = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "mobile-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(tf.keras.Model):\n",
    "    def __init__(self, batch_sz):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation='tanh')\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='tanh')\n",
    "        self.fc3 = tf.keras.layers.Dense(64, activation='tanh')\n",
    "        self.fc4 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "assured-prisoner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (200, 64)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "european-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "computational-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"binary_crossentropy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "extraordinary-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        all_out_dec = []\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(0, inp.shape[1]):\n",
    "            dec_input = inp[:, t, :]\n",
    "            out_dec, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            all_out_dec.append(out_dec)\n",
    "            \n",
    "        all_out_dec = tf.concat(all_out_dec, axis=-1)\n",
    "        print(all_out_dec.shape)\n",
    "        prediction = classifier(all_out_dec)\n",
    "\n",
    "        loss = loss_fn(prediction, targ)\n",
    "        batch_loss = (loss / int(inp.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables + classifier.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "juvenile-homework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n",
      "(200, 1600)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-331-ce82cc6d74f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-330-2399013f787b>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(inp, targ, enc_hidden)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mdec_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mout_dec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mall_out_dec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_dec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-323-815b103235cd>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# enc_output shape == (batch_size, max_length, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mcontext_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-321-e9e3c226a934>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, query, values)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# context_vector shape after sum == (batch_size, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mcontext_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcontext_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   1981\u001b[0m   \"\"\"\n\u001b[0;32m   1982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1983\u001b[1;33m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[0;32m   1984\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[0;32m   1985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[1;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[0;32m   1993\u001b[0m   return _may_reduce_to_scalar(\n\u001b[0;32m   1994\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[0;32m   1996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m  10515\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10517\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m  10518\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10519\u001b[0m         input, axis, \"keep_dims\", keep_dims)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print('Epoch {} Loss {}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "traditional-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sequence):\n",
    "    attention_plot = np.zeros((padding_length, padding_length))\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(sequence, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    all_out_dec = []\n",
    "\n",
    "    for t in range(0, padding_length):\n",
    "        dec_input = sequence[:, t, :]\n",
    "        out_dec, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        all_out_dec.append(out_dec)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    all_out_dec = tf.concat(all_out_dec, axis=-1)\n",
    "    result = classifier(all_out_dec)\n",
    "\n",
    "    return result, sequence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "resident-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels(np.roll(sentence, 1), fontdict=fontdict)\n",
    "    ax.set_yticklabels(np.roll(sentence, 1), fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "light-intake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 25, 1)\n",
      "(10000, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(good_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "several-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(idx):\n",
    "    sequence = tf.expand_dims(data[idx], 0)\n",
    "\n",
    "    result, sentence, attention_plot = evaluate(sequence)\n",
    "    seq_numpy = sequence.numpy()\n",
    "    seq_numpy = seq_numpy.reshape(-1)\n",
    "    \n",
    "    plot_attention(attention_plot, seq_numpy, seq_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "canadian-master",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-298-02e7a5da5daf>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(np.roll(sentence, 1), fontdict=fontdict)\n",
      "<ipython-input-298-02e7a5da5daf>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(np.roll(sentence, 1), fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_attention(idx=11234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-shore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
