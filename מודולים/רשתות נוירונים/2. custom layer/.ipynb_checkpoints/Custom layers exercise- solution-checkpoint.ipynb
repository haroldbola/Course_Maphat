{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras- writing custom layers\n",
    "```Here you will experience with writing custom keras layers. We will have two stages: in the first stage we will implement a simple layer. In the second you will implement a more complicated layer.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "```Implement an unpooling layer, that acts on matrices as follow:```\n",
    "```\n",
    "A = array([[0, 1, 3, 1, 0],\n",
    "           [2, 0, 1, 2, 4],\n",
    "           [3, 2, 1, 4, 3],\n",
    "           [4, 0, 3, 2, 0],\n",
    "           [4, 1, 2, 0, 2]])\n",
    "       \n",
    "unpooling(A) = array([[0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2]])\n",
    "```\n",
    "```Use the following example to do so, which is taken from https://keras.io/layers/writing-your-own-keras-layers/.```\n",
    "\n",
    "```Note: you can't use numpy's functions in your layer's logic. You will have to use functions that are accessed through the backend you use (Theano or Tensorflow).```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='weight_variable_name', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "class Unpool(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Unpool, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(Unpool, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.repeat_elements(K.repeat_elements(x,2,2), 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2\n",
    "```Consider the following simple attention mechanism:```\n",
    "\n",
    "```Given a vector compute Dense(v), while Dense(v).shape = v.shape\n",
    "Multiply v and Dense(v) element-wise\n",
    "Return the result```\n",
    "\n",
    "```What is the purpose of this mechanism? Can you think what can be achieved using this kind of mechanism?```\n",
    "\n",
    "```Implement the attention mechanism as a keras layer.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmm\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "class SimpleAttention(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], input_shape[1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(SimpleAttention, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3\n",
    "```Here you will try solving a problem I once struggled with. The problem is the following:\n",
    "You are given a set of sequences of symbols. All sequences contain the same \"core sequence\", but have extra noise in the form of other symbols between the symbols of the core sequence. For example, the sequences could be```\n",
    "\n",
    "\n",
    "**1**-**3**-2-**4**-3-**2**-4-**1**-3-2-4\n",
    "\n",
    "**1**-2-**3**-3-**4**-1-2-**2**-**1**-3-4-2-1-1\n",
    "\n",
    "**1**-4-4-4-**3**-**4**-1-1-**2**-**1**-1-2\n",
    "\n",
    "```while the core sequence is 1-3-4-2-1```\n",
    "```Your task is, given a dataset of such sequences, to find the core sequence. You may speak to me to learn about the context of this question and the reasons led to facing it.```\n",
    "\n",
    "```Generate a dataset that will simulate this problem. Follow the instructions:```\n",
    "- ```Use a 4-letter alphabet.```\n",
    "- ```Generate a core sequence with 10 symbols.```\n",
    "- ```Create a new sequence symbol by symbol: for each symbol you add to the sequence, put the next letter of the sequence with probability p and a random symbol with a probability 1-p. choose p to be 0.5.```\n",
    "- ```Generate a 10,000 examples dataset.```\n",
    "\n",
    "```Try solving the problem with simple means.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = [0,1,2,3]\n",
    "baseline_length = 10\n",
    "baseline = np.random.choice(digits, baseline_length)\n",
    "\n",
    "def create_seq(p, baseline_in):\n",
    "    baseline_length_in = len(baseline_in)\n",
    "    baseline_places = []\n",
    "    seq = [baseline_in[0]]\n",
    "    count = 1\n",
    "    while count<baseline_length_in:\n",
    "        if np.random.random()<p:\n",
    "            seq.append(np.random.choice(digits, 1)[0])\n",
    "        else:\n",
    "            baseline_places.append(len(seq))\n",
    "            seq.append(baseline_in[count])\n",
    "            count += 1\n",
    "    return seq, baseline_places\n",
    "\n",
    "def create_sequences(count, p, baseline_in=None):\n",
    "    if baseline_in is None:\n",
    "        return map(lambda x: create_seq(p, np.random.choice(digits, baseline_length)), range(count))\n",
    "    return map(lambda x: create_seq(p, baseline_in), range(count))\n",
    "\n",
    "baseline\n",
    "\n",
    "true_count = 10000\n",
    "false_count = 10000\n",
    "first_symbol = 1\n",
    "seq_len = 15\n",
    "data = list(create_sequences(true_count,0.3, baseline))+list(create_sequences(false_count,0.3, None))\n",
    "data, places = list(map(lambda x: x[0], data)), list(map(lambda x: x[1], data))\n",
    "data = list(map(lambda x: [first_symbol]+x, data))\n",
    "target = [True]*true_count+[False]*false_count\n",
    "data, target = shuffle(data, target)\n",
    "trim = list(filter(lambda x: len(x[0])<seq_len, zip(data, target)))\n",
    "data, target = map(lambda x: x[0], trim), np.array(list(map(lambda x: x[1], trim)))\n",
    "data = list(map(lambda x: x+[0]*(seq_len-len(x)), list(data)))\n",
    "data = np.array(list(map(lambda x: np.eye(4)[x], data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4\n",
    "```A possible solution for the problem could be done as follow:```\n",
    "- ```Given a dataset of sequences as such, generate a new dataset of random sequences.```\n",
    "- ```Train a classifier that will determine whether a sequence belongs to the original dataset or the generated dataset. Make sure that this problem is solvable.```\n",
    "- ```Now train a specific model, containing an attention layer. We can hope that the attention mechanism will learn to use the core sequence when classifying.```\n",
    "- ```Use the attention visualization to find the symbols of the core sequence.```\n",
    "\n",
    "```What are the advantages of this solution? Do you think you can make it work? You certainly will need a different kind of attention mechanism for the task, rather than the simple one you already have.```\n",
    "\n",
    "```Read the paper Neural Machine Translation by Jointly Learning to Align and Translate by Bahanau, Cho and Bengio. The paper concerns with an attention mechanism implemented in the context of machine translation. Implement the attention mechanism the authors suggest as a keras layer. Use the source code of the keras.layers.recurrent class. You can find the paper and the class source code in the current directory.```\n",
    "\n",
    "```Basic instructions:```\n",
    "- ```Use your tutor. A lot. This is a hard exercise.```\n",
    "- ```Open the source code of recurrent neural networks. You would like to implement a layer that inherits from Recurrent.```\n",
    "- ```Understand the code's flow and the functions you would like to write.```\n",
    "- ```Start by writing a mechanism that would be a little bit simpler: don't return a sequence, but rather return a single vector.```\n",
    "- ```Try solving the above problem using your attention mechanism. What problems do you encouter?```\n",
    "- ```Complete the full mechanism. Assuming Yoshua Bengio didn't lie in his paper, how do you think their architecture overcomes the problem you found?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Recurrent\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "class AttentionLayer(Recurrent):\n",
    "    def __init__(self, units, computation_length,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.computation_length = computation_length\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.alphas = []\n",
    "        \n",
    "    def get_constants(self, inputs, training=None):\n",
    "        return [inputs, K.l2_normalize(inputs, axis = -1)]\n",
    "    \n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        computation_length = self.computation_length\n",
    "        to_ret = K.tile(np.ones((1)), K.shape(inputs)[0:1])\n",
    "        to_ret = K.expand_dims(K.expand_dims(to_ret, -1), -1)\n",
    "        to_ret = K.repeat_elements(to_ret,computation_length,1)\n",
    "        return to_ret\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        batch_size = input_shape[0] if self.stateful else None\n",
    "        self.input_time_length = input_shape[1]\n",
    "        self.input_dim = input_shape[2]\n",
    "        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n",
    "        self.state_spec = InputSpec(shape=(batch_size, self.units))\n",
    "\n",
    "        self.states = [None]\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "\n",
    "        self.kernel = self.add_weight((self.input_dim, self.units * 6),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "\n",
    "        self.U_z = self.kernel[:, :self.units]\n",
    "        self.C_z = self.kernel[:, self.units*1:self.units*2]\n",
    "        self.U_r = self.kernel[:, self.units*2:self.units*3]\n",
    "        self.C_r = self.kernel[:, self.units*3:self.units*4]\n",
    "        self.U = self.kernel[:, self.units*4:self.units*5]\n",
    "        self.C = self.kernel[:, self.units*5:self.units*6]\n",
    "\n",
    "        self.built = True\n",
    "    \n",
    "    \n",
    "    def step(self, inputs, states):\n",
    "        s_previous = states[0]+1e-8  # previous memory, 1e-8 to prevent 0-normed vectors\n",
    "        h_matrix = states[1]  # dropout matrices for recurrent units\n",
    "        h_matrix_normalized = states[2]\n",
    "        s_previous_normalized = K.l2_normalize(s_previous, axis = -1)\n",
    "        s_previous_normalized = K.repeat(s_previous_normalized, self.input_time_length)\n",
    "        e_i_j = K.sum(s_previous_normalized*h_matrix_normalized, axis = -1)\n",
    "        alpha_i_j = K.softmax(e_i_j)#-1\n",
    "        \n",
    "        alpha_i_j = K.repeat_elements(K.expand_dims(alpha_i_j, axis = -1), rep = self.units, axis = 2)\n",
    "        c_i = K.sum(alpha_i_j*h_matrix, axis = 1)\n",
    "        z_i = K.sigmoid(K.dot(s_previous, self.U_z)+ K.dot(c_i, self.C_z))\n",
    "        z_i = K.cast(z_i>0.5, 'float32')\n",
    "        r_i = K.sigmoid(K.dot(s_previous, self.U_r) + K.dot(c_i, self.C_r))\n",
    "        s_tilde = self.activation(K.dot(r_i*s_previous, self.U) + K.dot(c_i, self.C))\n",
    "        s = (1-z_i)*s_previous +z_i*s_tilde\n",
    "        return s,[s]\n",
    "\n",
    "\n",
    "    \n",
    "def get_attentions(inputs, layer):\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def reLu(x):\n",
    "        return np.maximum(x,0)\n",
    "    \n",
    "    weights = layer.get_weights()[0]\n",
    "    units = layer.units\n",
    "    input_time_length = layer.input_time_length\n",
    "    U_z = weights[:, :units]\n",
    "    C_z = weights[:, units*1:units*2]\n",
    "    U_r = weights[:, units*2:units*3]\n",
    "    C_r = weights[:, units*3:units*4]\n",
    "    U = weights[:, units*4:units*5]\n",
    "    C = weights[:, units*5:units*6]\n",
    "    \n",
    "    alphas_list = []\n",
    "    parts_names = ['c_i', 'z_i', 'r_i', 's_tilde', 's_previous', 's_previous_normalized']\n",
    "    parts = {x:[] for x in parts_names}\n",
    "    \n",
    "    inputs = model_att.predict(inputs)\n",
    "    inputs_normalized = normalize(inputs, axis = -1)\n",
    "    s_previous = np.random.random((inputs.shape[0], inputs.shape[2]))+1e-8\n",
    "    for i in range(layer.computation_length):\n",
    "        s_previous_normalized = normalize(s_previous, axis = -1)\n",
    "        parts['s_previous_normalized'].append(s_previous_normalized)\n",
    "        s_previous_normalized = np.repeat(np.expand_dims(s_previous_normalized, 1), input_time_length, axis = 1)\n",
    "\n",
    "        e_i_j = np.sum(s_previous_normalized*inputs_normalized, axis = -1)\n",
    "        alpha_i_j = softmax(e_i_j)\n",
    "        alphas_list.append(alpha_i_j)\n",
    "\n",
    "        alpha_i_j = np.repeat(np.expand_dims(alpha_i_j, axis = -1), units, -1)\n",
    "        c_i = np.sum(alpha_i_j*inputs, axis = 1)\n",
    "\n",
    "        z_i = sigmoid(np.dot(s_previous,U_z) + np.dot(c_i,C_z))\n",
    "        r_i = sigmoid(np.dot(s_previous,U_r) + np.dot(c_i,C_r))\n",
    "        s_tilde = np.tanh(np.dot(r_i*s_previous, U) + np.dot(c_i,C))\n",
    "        parts['s_previous'].append(s_previous)\n",
    "        z_i = z_i>0.5\n",
    "        s_previous = (1-z_i)*s_previous +z_i*s_tilde\n",
    "        \n",
    "        parts['c_i'].append(c_i)\n",
    "        parts['z_i'].append(z_i)\n",
    "        parts['r_i'].append(r_i)\n",
    "        parts['s_tilde'].append(s_tilde)\n",
    "        \n",
    "        \n",
    "    for alpha in alphas_list:\n",
    "        alpha = alpha.reshape(-1)\n",
    "        plt.bar(range(len(alpha)), alpha)\n",
    "        plt.show()\n",
    "    return alphas_list, parts, inputs, inputs_normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forum4",
   "language": "python",
   "name": "forum4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
