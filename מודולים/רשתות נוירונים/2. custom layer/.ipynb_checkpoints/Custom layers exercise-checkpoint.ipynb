{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras- writing custom layers\n",
    "```Here you will experience with writing custom keras layers. We will have two stages: in the first stage we will implement a simple layer. In the second you will implement a more complicated layer.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "```Implement an unpooling layer, that acts on matrices as follow:```\n",
    "```\n",
    "A = array([[0, 1, 3, 1, 0],\n",
    "           [2, 0, 1, 2, 4],\n",
    "           [3, 2, 1, 4, 3],\n",
    "           [4, 0, 3, 2, 0],\n",
    "           [4, 1, 2, 0, 2]])\n",
    "       \n",
    "unpooling(A) = array([[0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2]])\n",
    "```\n",
    "```Use the following example to do so, which is taken from https://keras.io/layers/writing-your-own-keras-layers/.```\n",
    "\n",
    "```Note: you can't use numpy's functions in your layer's logic. You will have to use functions that are accessed through the backend you use (Theano or Tensorflow).```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='weight_variable_name', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2\n",
    "```Consider the following simple attention mechanism:```\n",
    "\n",
    "```Given a vector compute Dense(v), while Dense(v).shape = v.shape\n",
    "Multiply v and Dense(v) element-wise\n",
    "Return the result```\n",
    "\n",
    "```What is the purpose of this mechanism? Can you think what can be achieved using this kind of mechanism?```\n",
    "\n",
    "```Implement the attention mechanism as a keras layer.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3\n",
    "```Here you will try solving a problem I once  struggled with. The problem is the following:\n",
    "You are given a set of sequences of symbols. All sequences contain the same \"core sequence\", but have extra noise in the form of other symbols between the symbols of the core sequence. For example, the sequences could be```\n",
    "\n",
    "\n",
    "**1**-**3**-2-**4**-3-**2**-4-**1**-3-2-4\n",
    "\n",
    "**1**-2-**3**-3-**4**-1-2-**2**-**1**-3-4-2-1-1\n",
    "\n",
    "**1**-4-4-4-**3**-**4**-1-1-**2**-**1**-1-2\n",
    "\n",
    "```while the core sequence is 1-3-4-2-1```\n",
    "```Your task is, given a dataset of such sequences, to find the core sequence. You may speak to me to learn about the context of this question and the reasons led to facing it.```\n",
    "\n",
    "```Generate a dataset that will simulate this problem. Follow the instructions:```\n",
    "- ```Use a 4-letter alphabet.```\n",
    "- ```Generate a core sequence with 10 symbols.```\n",
    "- ```Create a new sequence symbol by symbol: for each symbol you add to the sequence, put the next letter of the sequence with probability p and a random symbol with a probability 1-p. choose p to be 0.5.```\n",
    "- ```Generate a 10,000 examples dataset.```\n",
    "\n",
    "```Try solving the problem with simple means.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4\n",
    "```A possible solution for the problem could be done as follow:```\n",
    "- ```Given a dataset of sequences as such, generate a new dataset of random sequences.```\n",
    "- ```Train a classifier that will determine whether a sequence belongs to the original dataset or the generated dataset. Make sure that this problem is solvable.```\n",
    "- ```Now train a specific model, containing an attention layer. We can hope that the attention mechanism will learn to use the core sequence when classifying.```\n",
    "- ```Use the attention visualization to find the symbols of the core sequence.```\n",
    "\n",
    "```What are the advantages of this solution? Do you think you can make it work? You certainly will need a different kind of attention mechanism for the task, rather than the simple one you already have.```\n",
    "\n",
    "```Read the paper Neural Machine Translation by Jointly Learning to Align and Translate by Bahanau, Cho and Bengio. The paper concerns with an attention mechanism implemented in the context of machine translation. Implement the attention mechanism the authors suggest as a keras layer. Use the source code of the keras.layers.recurrent class. You can find the paper and the class source code in the current directory.```\n",
    "\n",
    "```Basic instructions:```\n",
    "- ```Use your tutor. A lot. This is a hard exercise.```\n",
    "- ```Open the source code of recurrent neural networks. You would like to implement a layer that inherits from Recurrent.```\n",
    "- ```Understand the code's flow and the functions you would like to write.```\n",
    "- ```Start by writing a mechanism that would be a little bit simpler: don't return a sequence, but rather return a single vector.```\n",
    "- ```Try solving the above problem using your attention mechanism. What problems do you encounter?```\n",
    "- ```Complete the full mechanism. Assuming Yoshua Bengio didn't lie in his paper, how do you think their architecture overcomes the problem you found?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forum4",
   "language": "python",
   "name": "forum4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
