{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Iu8z5YH1RtS"
   },
   "source": [
    "# Keras- writing custom layers\n",
    "```Here you will experience with writing custom keras layers. We will have two stages: in the first stage we will implement a simple layer. In the second you will implement a more complicated layer.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMnNgDTC1RtU"
   },
   "source": [
    "## Stage 1\n",
    "```Implement an unpooling layer, that acts on matrices as follow:```\n",
    "```\n",
    "A = array([[0, 1, 3, 1, 0],\n",
    "           [2, 0, 1, 2, 4],\n",
    "           [3, 2, 1, 4, 3],\n",
    "           [4, 0, 3, 2, 0],\n",
    "           [4, 1, 2, 0, 2]])\n",
    "       \n",
    "unpooling(A) = array([[0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2]])\n",
    "```\n",
    "```Use the following example to do so, which is taken from https://keras.io/layers/writing-your-own-keras-layers/.```\n",
    "\n",
    "```Note: you can't use numpy's functions in your layer's logic. You will have to use functions that are accessed through the backend you use (Theano or Tensorflow).```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gzD4RVdW1RtV"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='weight_variable_name', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cuTmBe8h1RtW"
   },
   "outputs": [],
   "source": [
    "class Unpooling(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Unpooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Unpooling, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        repeat_first_axis = K.repeat_elements(x,2,1)\n",
    "        repeat_scond_axis = K.repeat_elements(repeat_first_axis,2,2)\n",
    "        return repeat_scond_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "mwSCCCqr1RtX",
    "outputId": "f326321d-27cc-42c8-dcab-c26309f3c12e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n",
      "tf.Tensor(\n",
      "[[[ 1  1  2  2  3  3]\n",
      "  [ 1  1  2  2  3  3]\n",
      "  [ 0  0  5  5 10 10]\n",
      "  [ 0  0  5  5 10 10]]], shape=(1, 4, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 1\n",
    "x = tf.convert_to_tensor(np.array([[[1,2,3], [0, 5, 10]]]))\n",
    "print(x.shape)\n",
    "unpool_layer = Unpooling()\n",
    "y = unpool_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoGVXPLD1RtY"
   },
   "source": [
    "## Stage 2\n",
    "```Consider the following simple attention mechanism:```\n",
    "\n",
    "```Given a vector compute Dense(v), while Dense(v).shape = v.shape\n",
    "Multiply v and Dense(v) element-wise\n",
    "Return the result```\n",
    "\n",
    "```What is the purpose of this mechanism? Can you think what can be achieved using this kind of mechanism?```\n",
    "\n",
    "```Implement the attention mechanism as a keras layer.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFB9d0tO1RtY"
   },
   "outputs": [],
   "source": [
    "class AttentionMechanism(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionMechanism, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[0], input_shape[1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(AttentionMechanism, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        print(\"x\", x.shape)\n",
    "        print(\"kernel\", self.kernel.shape)\n",
    "        return tf.keras.layers.Multiply()([x, self.kernel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE6_57EL1RtZ",
    "outputId": "4f91b6bc-f7e8-4c74-b899-6385790215e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "x (2, 3)\n",
      "kernel (2, 3)\n",
      "tf.Tensor(\n",
      "[[ 0.00923688 -0.06304872  0.05575053]\n",
      " [ 0.          0.20001902 -0.136467  ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "x = tf.convert_to_tensor(np.array([[1,2,3], [0, 5, 10]]), dtype='float32')\n",
    "print(x.shape)\n",
    "AttentionMechanism_layer = AttentionMechanism()\n",
    "y = AttentionMechanism_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iz3_Q3ti1RtZ"
   },
   "source": [
    "## Stage 3\n",
    "```Here you will try solving a problem I once  struggled with. The problem is the following:\n",
    "You are given a set of sequences of symbols. All sequences contain the same \"core sequence\", but have extra noise in the form of other symbols between the symbols of the core sequence. For example, the sequences could be```\n",
    "\n",
    "\n",
    "**1**-**3**-2-**4**-3-**2**-4-**1**-3-2-4\n",
    "\n",
    "**1**-2-**3**-3-**4**-1-2-**2**-**1**-3-4-2-1-1\n",
    "\n",
    "**1**-4-4-4-**3**-**4**-1-1-**2**-**1**-1-2\n",
    "\n",
    "```while the core sequence is 1-3-4-2-1```\n",
    "```Your task is, given a dataset of such sequences, to find the core sequence. You may speak to me to learn about the context of this question and the reasons led to facing it.```\n",
    "\n",
    "```Generate a dataset that will simulate this problem. Follow the instructions:```\n",
    "- ```Use a 4-letter alphabet.```\n",
    "- ```Generate a core sequence with 10 symbols.```\n",
    "- ```Create a new sequence symbol by symbol: for each symbol you add to the sequence, put the next letter of the sequence with probability p and a random symbol with a probability 1-p. choose p to be 0.5.```\n",
    "- ```Generate a 10,000 examples dataset.```\n",
    "\n",
    "```Try solving the problem with simple means.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFdiDu7z1Rta",
    "outputId": "97cb670c-3900-42a1-966b-30bdfe6c77ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2]\n"
     ]
    }
   ],
   "source": [
    "digits = [0,1,2,3]\n",
    "baseline_length = 3\n",
    "baseline = np.random.choice(digits, baseline_length)\n",
    "\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6bGmdty1Rta"
   },
   "outputs": [],
   "source": [
    "def create_seq(baseline_in, p=0.5):\n",
    "    baseline_length_in = len(baseline_in)\n",
    "    baseline_places = []\n",
    "    seq = [baseline_in[0]]\n",
    "    count = 1\n",
    "    while count<baseline_length_in:\n",
    "        if np.random.random()<p:\n",
    "            seq.append(np.random.choice(digits, 1)[0])\n",
    "        else:\n",
    "            baseline_places.append(len(seq))\n",
    "            seq.append(baseline_in[count])\n",
    "            count += 1\n",
    "    return seq, baseline_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PoRNr7z1Rta"
   },
   "outputs": [],
   "source": [
    "def create_sequences(count, baseline_in):\n",
    "    return map(lambda x: create_seq(baseline_in), range(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rFHGVJq1Rtb"
   },
   "outputs": [],
   "source": [
    "data = list(create_sequences(10000, baseline))\n",
    "data, places = list(map(lambda x: x[0], data)), list(map(lambda x: x[1], data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXmQKMr1Rtb",
    "outputId": "63b82f3d-cbe5-47d8-ea06-beca3ec14d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(3, 0, 2), (0, 1, 1), (1, 0, 3), (1, 3, 2), (3, 0, 3), (0, 1, 2), (1, 0, 0), (1, 3, 1), (0, 1, 3), (1, 0, 1), (3, 3, 1), (1, 3, 0), (2, 3, 0), (3, 3, 0), (0, 2, 1), (2, 3, 1), (3, 3, 3), (0, 2, 0), (2, 3, 2), (3, 3, 2), (0, 2, 3), (2, 3, 3), (0, 2, 2), (3, 2, 2), (2, 0, 1), (3, 1, 3), (3, 2, 3), (0, 3, 2), (1, 2, 0), (2, 0, 0), (0, 0, 3), (3, 1, 2), (3, 2, 0), (0, 3, 3), (1, 2, 1), (2, 0, 3), (0, 0, 2), (3, 1, 1), (3, 2, 1), (0, 3, 0), (1, 2, 2), (2, 0, 2), (0, 0, 1), (3, 1, 0), (0, 3, 1), (1, 2, 3), (0, 0, 0), (2, 1, 2), (1, 1, 1), (2, 2, 3), (2, 1, 3), (1, 1, 0), (2, 2, 2), (2, 1, 0), (0, 1, 0), (1, 1, 3), (2, 2, 1), (2, 1, 1), (1, 1, 2), (2, 2, 0), (3, 0, 0), (3, 0, 1), (1, 0, 2), (1, 3, 3)}\n"
     ]
    }
   ],
   "source": [
    "subsequences_set = set()\n",
    "\n",
    "for letter1 in digits:\n",
    "    for letter2 in digits:\n",
    "        for letter3 in digits:\n",
    "            subsequences_set.add((letter1, letter2, letter3))\n",
    "\n",
    "print(subsequences_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYLc5iX61Rtb"
   },
   "outputs": [],
   "source": [
    "def find_subsequence(sequence, subsequence):\n",
    "    idx = 0\n",
    "    for char in sequence:\n",
    "        if char == subsequence[idx]:\n",
    "            idx += 1\n",
    "        if idx == len(subsequence):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SfXd2861Rtc",
    "outputId": "bf941acc-0261-4b08-bd1d-91c12886270c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 3812.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "subsequences_set_copy = subsequences_set.copy()\n",
    "for subsequence in tqdm(subsequences_set):\n",
    "    for sequence in data:\n",
    "        if find_subsequence(sequence, subsequence) is False:\n",
    "            subsequences_set_copy.remove(subsequence)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0Hxod2_1Rtc",
    "outputId": "959e7028-1174-4753-9536-9b1d924a56f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(2, 3, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(subsequences_set_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzxQN20K1Rtc"
   },
   "source": [
    "## Stage 4\n",
    "```A possible solution for the problem could be done as follow:```\n",
    "- ```Given a dataset of sequences as such, generate a new dataset of random sequences.```\n",
    "- ```Train a classifier that will determine whether a sequence belongs to the original dataset or the generated dataset. Make sure that this problem is solvable.```\n",
    "- ```Now train a specific model, containing an attention layer. We can hope that the attention mechanism will learn to use the core sequence when classifying.```\n",
    "- ```Use the attention visualization to find the symbols of the core sequence.```\n",
    "\n",
    "```What are the advantages of this solution? Do you think you can make it work? You certainly will need a different kind of attention mechanism for the task, rather than the simple one you already have.```\n",
    "\n",
    "```Read the paper Neural Machine Translation by Jointly Learning to Align and Translate by Bahanau, Cho and Bengio. The paper concerns with an attention mechanism implemented in the context of machine translation. Implement the attention mechanism the authors suggest using PyTorch and try solving the above problem. You can find the paper in the current directory.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [0,1,2,3,4,5,6]\n",
    "padding_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "5Guk0zaE1Rtd"
   },
   "outputs": [],
   "source": [
    "def create_seq(baseline_in, p=0.5, padding_length=25, padding_symbol=8):\n",
    "    baseline_length_in = len(baseline_in)\n",
    "    baseline_places = []\n",
    "    seq = [baseline_in[0]]\n",
    "    count = 1\n",
    "    while count<baseline_length_in:\n",
    "        if np.random.random()<p:\n",
    "            digit_copy = digits.copy()\n",
    "            digit_copy.remove(baseline_in[count])\n",
    "            seq.append(np.random.choice(digits, 1)[0])\n",
    "        else:\n",
    "            baseline_places.append(len(seq))\n",
    "            seq.append(baseline_in[count])\n",
    "            count += 1\n",
    "            \n",
    "    if len(seq) < padding_length:\n",
    "        seq += [padding_symbol] * (padding_length-len(seq))\n",
    "    return seq, baseline_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "0R84fu1w1Rte"
   },
   "outputs": [],
   "source": [
    "def create_sequences(count, baseline_in):\n",
    "    return map(lambda x: create_seq(baseline_in, padding_length=padding_length), range(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecq1wdn71Rte",
    "outputId": "efb44736-1c3f-46d3-92aa-b137d767487c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "[0, 4, 5]\n",
      "data (20000, 25)\n",
      "good_data (10000, 25)\n"
     ]
    }
   ],
   "source": [
    "baseline_length = 3\n",
    "right_baseline = [1,3,2]\n",
    "wrong_baseline = [0,4,5]\n",
    "\n",
    "print(right_baseline)\n",
    "print(wrong_baseline)\n",
    "\n",
    "good_list = list(create_sequences(10000, right_baseline))\n",
    "\n",
    "data = good_list + list(create_sequences(10000, wrong_baseline))\n",
    "data, places = list(map(lambda x: x[0], data)), list(map(lambda x: x[1], data))\n",
    "target = [True]*10000+[False]*10000\n",
    "\n",
    "good_data = good_list\n",
    "good_data, places = list(map(lambda x: x[0], good_data)), list(map(lambda x: x[1], good_data))\n",
    "\n",
    "data = np.array(data)\n",
    "print('data', data.shape)\n",
    "\n",
    "good_data = np.array(good_data)\n",
    "print('good_data', good_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKHLRl8hJ39o",
    "outputId": "98c51e1c-01c0-4eaa-ecef-decb6287ae05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in data:\n",
    "  if len(i) > max_len:\n",
    "    max_len = len(i)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "dKKxO1TU1Rtf"
   },
   "outputs": [],
   "source": [
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLf_tW8l1Rtf",
    "outputId": "ca15557e-b4fa-4968-8bea-a37321770c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yY6cdQwq1Rtg",
    "outputId": "538ab150-1809-4ced-b95b-c4e6b9a3d2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 21)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 21, 10)       100         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder (LSTM)                  [(None, 21), (None,  2688        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 21, 10)       100         encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  (None, 21)           2688        embedding_1[0][0]                \n",
      "                                                                 encoder[0][1]                    \n",
      "                                                                 encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            22          decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,598\n",
      "Trainable params: 5,598\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "encoder_input = layers.Input(shape=(data.shape[1], ))\n",
    "encoder_embedded = layers.Embedding(input_dim=10, output_dim=10)(\n",
    "    encoder_input\n",
    ")\n",
    "# Return states in addition to output\n",
    "output_encoder, state_h, state_c = layers.LSTM(data.shape[1], return_state=True, name=\"encoder\")(\n",
    "    encoder_embedded\n",
    ")\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "decoder_embedded = layers.Embedding(input_dim=10, output_dim=10)(\n",
    "    output_encoder\n",
    ")\n",
    "decoder_output = layers.LSTM(data.shape[1], name=\"decoder\")(\n",
    "    decoder_embedded, initial_state=encoder_state\n",
    ")\n",
    "output = layers.Dense(1)(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vflvQoy1Rtg",
    "outputId": "3186e215-4f20-4fbb-ef19-ec764ae7cf22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 60)\n",
      "(None, 60)\n"
     ]
    }
   ],
   "source": [
    "print(state_h.shape)\n",
    "print(state_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiMsdE5Z1Rtg"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1hpw0Bd1Rth",
    "outputId": "353c9dff-3db0-494f-9d35-69d2fedfac79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "107/107 - 2s - loss: 0.2856 - val_loss: 0.3320\n",
      "Epoch 2/100\n",
      "107/107 - 1s - loss: 0.0217 - val_loss: 0.0066\n",
      "Epoch 3/100\n",
      "107/107 - 1s - loss: 0.0060 - val_loss: 0.0052\n",
      "Epoch 4/100\n",
      "107/107 - 1s - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 5/100\n",
      "107/107 - 1s - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 6/100\n",
      "107/107 - 1s - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 7/100\n",
      "107/107 - 1s - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 8/100\n",
      "107/107 - 1s - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 9/100\n",
      "107/107 - 1s - loss: 6.2105e-04 - val_loss: 0.0018\n",
      "Epoch 10/100\n",
      "107/107 - 1s - loss: 0.0025 - val_loss: 7.1037e-05\n",
      "Epoch 11/100\n",
      "107/107 - 1s - loss: 0.0025 - val_loss: 1.4828e-04\n",
      "Epoch 12/100\n",
      "107/107 - 1s - loss: 6.5019e-04 - val_loss: 3.1732e-07\n",
      "Epoch 13/100\n",
      "107/107 - 1s - loss: 4.5660e-04 - val_loss: 2.1961e-06\n",
      "Epoch 14/100\n",
      "107/107 - 1s - loss: 4.5307e-04 - val_loss: 1.6550e-06\n",
      "Epoch 15/100\n",
      "107/107 - 1s - loss: 2.6263e-04 - val_loss: 1.1047e-06\n",
      "Epoch 16/100\n",
      "107/107 - 1s - loss: 5.7084e-06 - val_loss: 1.8333e-08\n",
      "Epoch 17/100\n",
      "107/107 - 1s - loss: 5.5517e-06 - val_loss: 1.8008e-08\n",
      "Epoch 18/100\n",
      "107/107 - 1s - loss: 5.0502e-06 - val_loss: 1.8111e-08\n",
      "Epoch 19/100\n",
      "107/107 - 1s - loss: 4.7669e-06 - val_loss: 1.7191e-08\n",
      "Epoch 20/100\n",
      "107/107 - 1s - loss: 4.5360e-06 - val_loss: 1.6851e-08\n",
      "Epoch 21/100\n",
      "107/107 - 1s - loss: 4.0359e-06 - val_loss: 1.6639e-08\n",
      "Epoch 22/100\n",
      "107/107 - 1s - loss: 3.9162e-06 - val_loss: 1.7805e-08\n",
      "Epoch 23/100\n",
      "107/107 - 1s - loss: 3.5778e-06 - val_loss: 1.5898e-08\n",
      "Epoch 24/100\n",
      "107/107 - 1s - loss: 3.2840e-06 - val_loss: 1.5469e-08\n",
      "Epoch 25/100\n",
      "107/107 - 1s - loss: 2.9852e-06 - val_loss: 1.5565e-08\n",
      "Epoch 26/100\n",
      "107/107 - 1s - loss: 2.9359e-06 - val_loss: 1.4624e-08\n",
      "Epoch 27/100\n",
      "107/107 - 1s - loss: 2.5059e-06 - val_loss: 1.7089e-08\n",
      "Epoch 28/100\n",
      "107/107 - 1s - loss: 2.3230e-06 - val_loss: 1.4618e-08\n",
      "Epoch 29/100\n",
      "107/107 - 1s - loss: 2.1739e-06 - val_loss: 1.3710e-08\n",
      "Epoch 30/100\n",
      "107/107 - 1s - loss: 2.1027e-06 - val_loss: 1.3630e-08\n",
      "Epoch 31/100\n",
      "107/107 - 1s - loss: 1.8349e-06 - val_loss: 1.4474e-08\n",
      "Epoch 32/100\n",
      "107/107 - 1s - loss: 1.7700e-06 - val_loss: 1.2683e-08\n",
      "Epoch 33/100\n",
      "107/107 - 1s - loss: 1.5670e-06 - val_loss: 1.2528e-08\n",
      "Epoch 34/100\n",
      "107/107 - 1s - loss: 1.5738e-06 - val_loss: 1.3128e-08\n",
      "Epoch 35/100\n",
      "107/107 - 1s - loss: 1.3372e-06 - val_loss: 1.1802e-08\n",
      "Epoch 36/100\n",
      "107/107 - 1s - loss: 1.4210e-06 - val_loss: 1.2639e-08\n",
      "Epoch 37/100\n",
      "107/107 - 1s - loss: 1.2894e-06 - val_loss: 1.2768e-08\n",
      "Epoch 38/100\n",
      "107/107 - 1s - loss: 1.2191e-06 - val_loss: 1.3113e-08\n",
      "Epoch 39/100\n",
      "107/107 - 1s - loss: 1.1414e-06 - val_loss: 1.2591e-08\n",
      "Epoch 40/100\n",
      "107/107 - 1s - loss: 1.0718e-06 - val_loss: 1.4089e-08\n",
      "Epoch 41/100\n",
      "107/107 - 1s - loss: 9.3235e-07 - val_loss: 1.2948e-08\n",
      "Epoch 42/100\n",
      "107/107 - 1s - loss: 9.3320e-07 - val_loss: 1.0400e-08\n",
      "Epoch 43/100\n",
      "107/107 - 1s - loss: 1.0354e-06 - val_loss: 1.0000e-08\n",
      "Epoch 44/100\n",
      "107/107 - 1s - loss: 7.5373e-07 - val_loss: 9.4284e-09\n",
      "Epoch 45/100\n",
      "107/107 - 1s - loss: 9.9593e-07 - val_loss: 9.1758e-09\n",
      "Epoch 46/100\n",
      "107/107 - 1s - loss: 7.9002e-07 - val_loss: 8.9751e-09\n",
      "Epoch 47/100\n",
      "107/107 - 1s - loss: 8.7389e-07 - val_loss: 8.9569e-09\n",
      "Epoch 48/100\n",
      "107/107 - 1s - loss: 8.7948e-07 - val_loss: 8.7996e-09\n",
      "Epoch 49/100\n",
      "107/107 - 1s - loss: 6.5954e-07 - val_loss: 8.4110e-09\n",
      "Epoch 50/100\n",
      "107/107 - 1s - loss: 8.5046e-07 - val_loss: 3.5046e-08\n",
      "Epoch 51/100\n",
      "107/107 - 1s - loss: 2.1623e-06 - val_loss: 3.0492e-08\n",
      "Epoch 52/100\n",
      "107/107 - 1s - loss: 1.2669e-06 - val_loss: 7.7577e-09\n",
      "Epoch 53/100\n",
      "107/107 - 1s - loss: 1.2810e-06 - val_loss: 8.1431e-09\n",
      "Epoch 54/100\n",
      "107/107 - 1s - loss: 1.8270e-06 - val_loss: 1.0442e-08\n",
      "Epoch 55/100\n",
      "107/107 - 1s - loss: 9.9833e-07 - val_loss: 2.0665e-08\n",
      "Epoch 56/100\n",
      "107/107 - 1s - loss: 9.6115e-07 - val_loss: 8.2420e-09\n",
      "Epoch 57/100\n",
      "107/107 - 1s - loss: 1.0520e-06 - val_loss: 5.9554e-09\n",
      "Epoch 58/100\n",
      "107/107 - 1s - loss: 1.7075e-06 - val_loss: 1.7580e-07\n",
      "Epoch 59/100\n",
      "107/107 - 1s - loss: 3.6372e-06 - val_loss: 6.7615e-09\n",
      "Epoch 60/100\n",
      "107/107 - 1s - loss: 3.4522e-07 - val_loss: 4.8343e-09\n",
      "Epoch 61/100\n",
      "107/107 - 1s - loss: 5.5865e-07 - val_loss: 4.8364e-09\n",
      "Epoch 62/100\n",
      "107/107 - 1s - loss: 4.9459e-07 - val_loss: 4.6361e-09\n",
      "Epoch 63/100\n",
      "107/107 - 1s - loss: 1.2274e-06 - val_loss: 5.6330e-09\n",
      "Epoch 64/100\n",
      "107/107 - 1s - loss: 9.3783e-07 - val_loss: 1.7254e-08\n",
      "Epoch 65/100\n",
      "107/107 - 1s - loss: 4.5522e-06 - val_loss: 6.8139e-09\n",
      "Epoch 66/100\n",
      "107/107 - 1s - loss: 4.0751e-07 - val_loss: 5.6006e-09\n",
      "Epoch 67/100\n",
      "107/107 - 1s - loss: 5.1008e-07 - val_loss: 5.5648e-09\n",
      "Epoch 68/100\n",
      "107/107 - 1s - loss: 1.0420e-06 - val_loss: 3.1648e-09\n",
      "Epoch 69/100\n",
      "107/107 - 1s - loss: 2.0272e-06 - val_loss: 3.0915e-09\n",
      "Epoch 70/100\n",
      "107/107 - 1s - loss: 2.5595e-07 - val_loss: 7.6676e-09\n",
      "Epoch 71/100\n",
      "107/107 - 1s - loss: 1.5518e-06 - val_loss: 3.2151e-09\n",
      "Epoch 72/100\n",
      "107/107 - 1s - loss: 3.8941e-07 - val_loss: 2.8699e-09\n",
      "Epoch 73/100\n",
      "107/107 - 1s - loss: 3.4343e-06 - val_loss: 4.4800e-09\n",
      "Epoch 74/100\n",
      "107/107 - 1s - loss: 3.9584e-07 - val_loss: 3.3014e-09\n",
      "Epoch 75/100\n",
      "107/107 - 1s - loss: 2.6871e-07 - val_loss: 2.4763e-09\n",
      "Epoch 76/100\n",
      "107/107 - 1s - loss: 4.6006e-06 - val_loss: 1.6968e-09\n",
      "Epoch 77/100\n",
      "107/107 - 1s - loss: 1.1612e-07 - val_loss: 1.6086e-09\n",
      "Epoch 78/100\n",
      "107/107 - 1s - loss: 1.1091e-07 - val_loss: 1.8654e-09\n",
      "Epoch 79/100\n",
      "107/107 - 1s - loss: 8.9112e-07 - val_loss: 1.6455e-09\n",
      "Epoch 80/100\n",
      "107/107 - 1s - loss: 1.1993e-06 - val_loss: 2.0288e-09\n",
      "Epoch 81/100\n",
      "107/107 - 1s - loss: 3.3378e-07 - val_loss: 1.5569e-09\n",
      "Epoch 82/100\n",
      "107/107 - 1s - loss: 6.4526e-06 - val_loss: 1.0366e-09\n",
      "Epoch 83/100\n",
      "107/107 - 1s - loss: 3.7680e-07 - val_loss: 1.2438e-09\n",
      "Epoch 84/100\n",
      "107/107 - 1s - loss: 1.4945e-07 - val_loss: 2.0745e-09\n",
      "Epoch 85/100\n",
      "107/107 - 1s - loss: 3.9182e-07 - val_loss: 2.1160e-09\n",
      "Epoch 86/100\n",
      "107/107 - 1s - loss: 9.9575e-08 - val_loss: 1.0472e-09\n",
      "Epoch 87/100\n",
      "107/107 - 1s - loss: 4.3142e-07 - val_loss: 1.0447e-09\n",
      "Epoch 88/100\n",
      "107/107 - 1s - loss: 3.2763e-07 - val_loss: 1.2741e-09\n",
      "Epoch 89/100\n",
      "107/107 - 1s - loss: 2.2867e-06 - val_loss: 7.9667e-10\n",
      "Epoch 90/100\n",
      "107/107 - 1s - loss: 6.7555e-07 - val_loss: 1.7213e-08\n",
      "Epoch 91/100\n",
      "107/107 - 1s - loss: 6.4168e-07 - val_loss: 8.3777e-10\n",
      "Epoch 92/100\n",
      "107/107 - 1s - loss: 5.0885e-06 - val_loss: 3.4175e-09\n",
      "Epoch 93/100\n",
      "107/107 - 1s - loss: 9.5184e-08 - val_loss: 6.2205e-10\n",
      "Epoch 94/100\n",
      "107/107 - 1s - loss: 6.4619e-08 - val_loss: 1.3588e-09\n",
      "Epoch 95/100\n",
      "107/107 - 1s - loss: 1.2060e-07 - val_loss: 2.0950e-09\n",
      "Epoch 96/100\n",
      "107/107 - 1s - loss: 5.9221e-07 - val_loss: 1.2552e-07\n",
      "Epoch 97/100\n",
      "107/107 - 1s - loss: 9.4479e-07 - val_loss: 1.3826e-08\n",
      "Epoch 98/100\n",
      "107/107 - 1s - loss: 4.0466e-06 - val_loss: 1.9309e-07\n",
      "Epoch 99/100\n",
      "107/107 - 1s - loss: 1.6396e-07 - val_loss: 2.8071e-09\n",
      "Epoch 100/100\n",
      "107/107 - 1s - loss: 6.8865e-08 - val_loss: 6.0248e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b6cc1ca460>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, target, batch_size=150, epochs=100, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NwbB9_W61Rth"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][1], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, encoder_state_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, encoder_state_seq = inputs\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        encoder_state_seq = tf.keras.layers.Reshape(target_shape=(encoder_state_seq.shape[1], 1))(encoder_state_seq)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, encoder_state_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][1])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bcx9OVFFBEHA"
   },
   "outputs": [],
   "source": [
    "class PersoAttentionLayer_v2(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PersoAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][1], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(PersoAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, encoder_state_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, encoder_state_seq = inputs\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        encoder_state_seq = tf.keras.layers.Reshape(target_shape=(encoder_state_seq.shape[1], 1))(encoder_state_seq)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, encoder_state_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][1])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sExeUdn2DTtW"
   },
   "outputs": [],
   "source": [
    "class attention_v3(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention_v3,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention_v3, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_v4(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention_v4,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        # W is Hx1\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[2],1),\n",
    "                               initializer=\"normal\")\n",
    "        # B is Sx1\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")        \n",
    "        super(attention_v4, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        dot_product = K.dot(x,self.W)+self.b # Sx1\n",
    "        et = K.squeeze(K.tanh(dot_product),axis=-1) # Sx1\n",
    "        at = K.softmax(et) # Sx1\n",
    "        at = K.expand_dims(at,axis=-1) # Sx1\n",
    "        output = x*at\n",
    "        sum_final = K.sum(output,axis=2)\n",
    "#         sum_final = K.expand_dims(sum_final,axis=-1)\n",
    "        return sum_final\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Reshape\n",
    "\n",
    "class attention_v5(Layer):\n",
    "    def __init__(self, seq_size, hidden_size, **kwargs):\n",
    "        super(attention_v5,self).__init__(**kwargs)\n",
    "        # W is 2Hx1\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(2*hidden_size,2*hidden_size),\n",
    "                               initializer=\"normal\")\n",
    "        # B is Sx1\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(seq_size,1),\n",
    "                               initializer=\"zeros\") \n",
    "        \n",
    "        # V: 2Hx1\n",
    "        self.v = self.add_weight(name=\"v_no_bias\",shape=(2*hidden_size,hidden_size),\n",
    "                               initializer=\"normal\") \n",
    "\n",
    "    def call(self, hidden, encoder_outputs):\n",
    "        # Hidden: BxH\n",
    "        # encoder_outputs: BxSxH\n",
    "        \n",
    "        # repeat_encoder_state: BxSxH\n",
    "        repeat_encoder_state = layers.RepeatVector(seq_size)(encoder_state)\n",
    "\n",
    "        # Concat: BxSx2H\n",
    "        concat = layers.Concatenate()([encoder_out, repeat_encoder_state])\n",
    "\n",
    "        # dot_product: BxSx2H\n",
    "        dot_product = K.tanh(K.dot(concat,self.W)+self.b)\n",
    "\n",
    "        # output: BxSxH\n",
    "        output = K.dot(dot_product, self.v)\n",
    "#         output = K.squeeze(output, axis=-1)\n",
    "        \n",
    "        # softmax_tensor: BxSxH\n",
    "        softmax_tensor = K.softmax(output) \n",
    "\n",
    "#         # sum_final: B\n",
    "#         sum_final = K.sum(softmax_tensor,axis=1)\n",
    "        \n",
    "        # sum_final: Bx1xH\n",
    "        sum_final = K.sum(softmax_tensor,axis=1)\n",
    "        sum_final = layers.Reshape(target_shape=(1, sum_final.shape[1]))(sum_final)\n",
    "        \n",
    "        return sum_final\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(Layer):\n",
    "    def __init__(self, seq_size, hidden_size, attention_module, **kwargs):\n",
    "        super(decoder,self).__init__(**kwargs)\n",
    "        \n",
    "        self.decoder_gru = layers.GRU(hidden_size, \n",
    "                                      return_sequences=True, return_state=True, name='decoder_gru')\n",
    "        self.attention = attention_module\n",
    "\n",
    "    def call(self, input_tensor, hidden, encoder_outputs):\n",
    "        # input_tensor: Bx1\n",
    "        # Hidden: BxH\n",
    "        # encoder_outputs: BxSxH\n",
    "        \n",
    "        # sum_final: Bx1xH\n",
    "        sum_final = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        # input_tensor: Bx1x1\n",
    "        input_tensor = layers.Reshape(target_shape=(1, 1))(input_tensor)\n",
    "\n",
    "        # concat: Bx1x(H+1)\n",
    "        concat = layers.Concatenate()([sum_final, input_tensor])\n",
    "        \n",
    "        # decoder_out: Bx1xH\n",
    "        # decoder_state: BxH\n",
    "        decoder_out, decoder_state = self.decoder_gru(concat)\n",
    "        \n",
    "        return decoder_out, decoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "id": "jUfJFRt21Rth"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "batch_size = 200\n",
    "timestep = 100\n",
    "seq_size = padding_length\n",
    "hidden_size = 16\n",
    "\n",
    "encoder_inputs = layers.Input(batch_shape=(batch_size, seq_size, 1), name='encoder_inputs')\n",
    "\n",
    "encoder_gru = layers.GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n",
    "encoder_out, hidden_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "# attn_layer = tf.keras.layers.Attention(name='attention_layer', use_scale=True)\n",
    "# attn_out = attn_layer([encoder_out, encoder_out])\n",
    "\n",
    "# attn_layer = AttentionLayer(name='attention_layer')\n",
    "# attn_out = attn_layer([encoder_out, encoder_out])\n",
    "\n",
    "# attn_out=attention_v3()(encoder_out)\n",
    "\n",
    "# attn_out=attention_v4(name='attention_layer')(encoder_out)\n",
    "\n",
    "# decoder_gru = layers.GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "# decoder_out, decoder_state = decoder_gru(attn_out)\n",
    "\n",
    "# y = tf.zeros(shape=(batch_size, seq_size, hidden_size))\n",
    "\n",
    "# for i in range(seq_size):\n",
    "#     attn_out = attention_v5(name='attention_layer', \n",
    "#                             seq_size=seq_size, hidden_size=hidden_size)(encoder_out, s)\n",
    "#     decoder_input = Concatenate()([attn_out, y])\n",
    "    \n",
    "#     decoder_gru = layers.GRU(hidden_size, return_sequences=True, return_state=True, \n",
    "#                              name='decoder_gru')\n",
    "#     decoder_out, decoder_state = decoder_gru(decoder_input)\n",
    "    \n",
    "#     s = attn_out\n",
    "#     y = decoder_out\n",
    "\n",
    "attn_out = attention_v5(name='attention_layer', seq_size=seq_size, hidden_size=hidden_size)\n",
    "decoder_GRU = decoder(seq_size, hidden_size, attn_out)\n",
    "    \n",
    "decoder_outputs = []\n",
    "input_tensor = encoder_inputs[:,0]\n",
    "for t in range(1, seq_size):\n",
    "    decoder_out, hidden_state = decoder_GRU(input_tensor, hidden_state, encoder_out)\n",
    "    # decoder_out: BxH\n",
    "    decoder_out = layers.Reshape(target_shape=(decoder_out.shape[2],))(decoder_out)\n",
    "    decoder_outputs.append(decoder_out)\n",
    "    \n",
    "    input_tensor = encoder_inputs[:,t]\n",
    "\n",
    "    \n",
    "decoder_outputs_tensor = tf.stack(decoder_outputs, axis=1)\n",
    "output_dense = layers.Dense(1, activation='sigmoid', name='classification')(decoder_outputs_tensor)\n",
    "\n",
    "full_model = keras.Model(inputs=[encoder_inputs], outputs=output_dense)\n",
    "full_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# attention_model = keras.Model(inputs=full_model.input, \n",
    "#                               outputs=full_model.get_layer('attention_layer').output)\n",
    "# attention_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out (200, 25, 16)\n",
      "encoder_state (200, 16)\n",
      "repeat_encoder_state (200, 25, 16)\n",
      "concat (200, 25, 32)\n"
     ]
    }
   ],
   "source": [
    "print('encoder_out', encoder_out.shape)\n",
    "print('encoder_state', encoder_state.shape)\n",
    "\n",
    "repeat_encoder_state = layers.RepeatVector(seq_size)(encoder_state)\n",
    "print('repeat_encoder_state', repeat_encoder_state.shape)\n",
    "\n",
    "concat = layers.Concatenate()([encoder_out, repeat_encoder_state])\n",
    "print('concat', concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_hJd34gBP2P",
    "outputId": "59dde4a5-afdd-4cd9-aa56-9f78df49da64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 25, 16)\n",
      "(200, 16)\n",
      "(200, 25)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_out.shape)\n",
    "print(encoder_state.shape)\n",
    "print(attn_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JkiT_WPP1Rti",
    "outputId": "2f74dc93-6c28-4c53-f22a-85ad3f1273bd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_53\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(200, 25, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_267 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru (GRU)               [(200, 25, 16), (200 912         encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_226 (decoder)           ((200, 1, 16), (200, 3241        tf_op_layer_strided_slice_267[0][\n",
      "                                                                 encoder_gru[0][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_268[0][\n",
      "                                                                 decoder_226[0][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_269[0][\n",
      "                                                                 decoder_226[1][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_270[0][\n",
      "                                                                 decoder_226[2][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_271[0][\n",
      "                                                                 decoder_226[3][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_272[0][\n",
      "                                                                 decoder_226[4][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_273[0][\n",
      "                                                                 decoder_226[5][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_274[0][\n",
      "                                                                 decoder_226[6][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_275[0][\n",
      "                                                                 decoder_226[7][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_276[0][\n",
      "                                                                 decoder_226[8][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_277[0][\n",
      "                                                                 decoder_226[9][1]                \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_278[0][\n",
      "                                                                 decoder_226[10][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_279[0][\n",
      "                                                                 decoder_226[11][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_280[0][\n",
      "                                                                 decoder_226[12][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_281[0][\n",
      "                                                                 decoder_226[13][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_282[0][\n",
      "                                                                 decoder_226[14][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_283[0][\n",
      "                                                                 decoder_226[15][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_284[0][\n",
      "                                                                 decoder_226[16][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_285[0][\n",
      "                                                                 decoder_226[17][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_286[0][\n",
      "                                                                 decoder_226[18][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_287[0][\n",
      "                                                                 decoder_226[19][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_288[0][\n",
      "                                                                 decoder_226[20][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_289[0][\n",
      "                                                                 decoder_226[21][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "                                                                 tf_op_layer_strided_slice_290[0][\n",
      "                                                                 decoder_226[22][1]               \n",
      "                                                                 encoder_gru[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_268 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_269 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_270 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_271 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_272 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_273 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_274 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_275 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_276 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_277 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_278 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_279 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_280 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_281 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_282 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_283 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_284 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_285 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_286 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_287 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_288 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_289 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_290 ( [(200, 1)]           0           encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_130 (Reshape)           (200, 16)            0           decoder_226[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_131 (Reshape)           (200, 16)            0           decoder_226[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_132 (Reshape)           (200, 16)            0           decoder_226[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_133 (Reshape)           (200, 16)            0           decoder_226[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_134 (Reshape)           (200, 16)            0           decoder_226[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_135 (Reshape)           (200, 16)            0           decoder_226[5][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_136 (Reshape)           (200, 16)            0           decoder_226[6][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_137 (Reshape)           (200, 16)            0           decoder_226[7][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_138 (Reshape)           (200, 16)            0           decoder_226[8][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_139 (Reshape)           (200, 16)            0           decoder_226[9][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_140 (Reshape)           (200, 16)            0           decoder_226[10][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_141 (Reshape)           (200, 16)            0           decoder_226[11][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_142 (Reshape)           (200, 16)            0           decoder_226[12][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_143 (Reshape)           (200, 16)            0           decoder_226[13][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_144 (Reshape)           (200, 16)            0           decoder_226[14][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_145 (Reshape)           (200, 16)            0           decoder_226[15][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_146 (Reshape)           (200, 16)            0           decoder_226[16][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_147 (Reshape)           (200, 16)            0           decoder_226[17][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_148 (Reshape)           (200, 16)            0           decoder_226[18][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_149 (Reshape)           (200, 16)            0           decoder_226[19][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_150 (Reshape)           (200, 16)            0           decoder_226[20][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_151 (Reshape)           (200, 16)            0           decoder_226[21][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_152 (Reshape)           (200, 16)            0           decoder_226[22][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_153 (Reshape)           (200, 16)            0           decoder_226[23][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_5 (TensorFlow [(200, 24, 16)]      0           reshape_130[0][0]                \n",
      "                                                                 reshape_131[0][0]                \n",
      "                                                                 reshape_132[0][0]                \n",
      "                                                                 reshape_133[0][0]                \n",
      "                                                                 reshape_134[0][0]                \n",
      "                                                                 reshape_135[0][0]                \n",
      "                                                                 reshape_136[0][0]                \n",
      "                                                                 reshape_137[0][0]                \n",
      "                                                                 reshape_138[0][0]                \n",
      "                                                                 reshape_139[0][0]                \n",
      "                                                                 reshape_140[0][0]                \n",
      "                                                                 reshape_141[0][0]                \n",
      "                                                                 reshape_142[0][0]                \n",
      "                                                                 reshape_143[0][0]                \n",
      "                                                                 reshape_144[0][0]                \n",
      "                                                                 reshape_145[0][0]                \n",
      "                                                                 reshape_146[0][0]                \n",
      "                                                                 reshape_147[0][0]                \n",
      "                                                                 reshape_148[0][0]                \n",
      "                                                                 reshape_149[0][0]                \n",
      "                                                                 reshape_150[0][0]                \n",
      "                                                                 reshape_151[0][0]                \n",
      "                                                                 reshape_152[0][0]                \n",
      "                                                                 reshape_153[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (200, 24, 1)         17          tf_op_layer_stack_5[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 4,170\n",
      "Trainable params: 4,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMZ9HZiq7vAn",
    "outputId": "ee7d6ea0-3d37-4afd-b320-a686a6fea212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  [(200, 25, 1)]            0         \n",
      "_________________________________________________________________\n",
      "encoder_gru (GRU)            [(200, 25, 16), (200, 16) 912       \n",
      "_________________________________________________________________\n",
      "attention_layer (attention_v (200, 25)                 41        \n",
      "=================================================================\n",
      "Total params: 953\n",
      "Trainable params: 953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rz0yHklJ1Rti",
    "outputId": "86eca224-8524-4107-bbcb-8d4d499238f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "data = data.reshape(data.shape[0], data.shape[1], 1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cljq7c8A1Rti",
    "outputId": "8be92bdb-f871-4ac1-ea0b-1bd47f8198b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "target = np.array(target)\n",
    "target = target.reshape(data.shape[0], 1)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VX6ucLxK1Rtj",
    "outputId": "c5471c7d-9ed7-42c1-a132-ff0d9a99c7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder_gru/gru_cell_318/kernel:0', 'encoder_gru/gru_cell_318/recurrent_kernel:0', 'encoder_gru/gru_cell_318/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder_gru/gru_cell_318/kernel:0', 'encoder_gru/gru_cell_318/recurrent_kernel:0', 'encoder_gru/gru_cell_318/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder_gru/gru_cell_318/kernel:0', 'encoder_gru/gru_cell_318/recurrent_kernel:0', 'encoder_gru/gru_cell_318/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder_gru/gru_cell_318/kernel:0', 'encoder_gru/gru_cell_318/recurrent_kernel:0', 'encoder_gru/gru_cell_318/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "ename": "_SymbolicException",
     "evalue": "Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'encoder_gru/PartitionedCall_64:2' shape=(200, 16) dtype=float32>, <tf.Tensor 'encoder_gru/PartitionedCall_80:1' shape=(200, 25, 16) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n",
      "\u001b[1;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: encoder_gru/PartitionedCall_64:2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31m_SymbolicException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-372-4cd720499b12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfull_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     70\u001b[0m     ]\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m       raise core._SymbolicException(\n\u001b[0m\u001b[0;32m     73\u001b[0m           \u001b[1;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n",
      "\u001b[1;31m_SymbolicException\u001b[0m: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'encoder_gru/PartitionedCall_64:2' shape=(200, 16) dtype=float32>, <tf.Tensor 'encoder_gru/PartitionedCall_80:1' shape=(200, 25, 16) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "full_model.fit(data, target, batch_size=batch_size, epochs=100, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "good_data = good_data.reshape(good_data.shape[0], good_data.shape[1], 1)\n",
    "print(good_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "khPfnp5t84Rn"
   },
   "outputs": [],
   "source": [
    "attention_output = attention_model.predict(good_data, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzDpvLqOAJ2_",
    "outputId": "5e9846da-925b-416e-9a49-5cc7b09cf281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 25)\n"
     ]
    }
   ],
   "source": [
    "print(attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6rCJ8qPAQM0",
    "outputId": "4d856a5c-6d17-4562-b73f-b67f20c70306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [3]\n",
      " [4]\n",
      " [2]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]]\n",
      "[-0.02919025 -0.16077662 -0.31713867 -1.1034439  -1.1566602  -0.596961\n",
      " -0.5538951  -0.34042245 -1.202547   -0.34156737 -1.2464143  -0.339229\n",
      " -1.2351216  -0.3061426  -0.33846828 -0.63232154 -1.0343393  -0.30453697\n",
      " -0.3166213  -1.1776497  -0.5362682  -0.26088703 -0.319931   -1.1466461\n",
      " -0.2836511 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 5230\n",
    "\n",
    "print(good_data[idx])\n",
    "print(attention_output[idx])\n",
    "\n",
    "plt.scatter(np.arange(0, len(attention_output[idx])), attention_output[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 25)\n"
     ]
    }
   ],
   "source": [
    "attention_output_all = attention_model.predict(data, batch_size=200)\n",
    "print(attention_output_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [4]\n",
      " [5]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [8]]\n",
      "[0.01409244 0.21296704 0.27607703 0.8357383  0.8954886  0.44541532\n",
      " 0.41425017 0.26083833 0.9454976  0.26196238 0.98535365 0.2603906\n",
      " 0.9751067  0.23720865 0.25984368 0.4746433  0.7989456  0.23602384\n",
      " 0.24446326 0.92319536 0.40257055 0.20554723 0.24673556 0.8956383\n",
      " 0.22132696]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 16549\n",
    "\n",
    "print(data[idx])\n",
    "print(attention_output_all[idx])\n",
    "\n",
    "plt.scatter(np.arange(0, len(attention_output_all[idx])), attention_output_all[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ciaxvade1Rtj",
    "outputId": "4c8992e8-777b-4f46-cecc-8b46c83ab8cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_layer_1 (Dense)       (None, 5)                 110       \n",
      "_________________________________________________________________\n",
      "hidden_layer_2 (Dense)       (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "hidden_layer_3 (Dense)       (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        Input(shape = (21,)),\n",
    "        Dense(5, name=\"hidden_layer_1\", activation='tanh'),\n",
    "        Dense(5, name=\"hidden_layer_2\", activation='tanh'),\n",
    "        Dense(5, name=\"hidden_layer_3\", activation='tanh'),\n",
    "        Dense(1, name=\"output_layer\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_j4uxIw31Rtj",
    "outputId": "6d5820dc-2183-45dc-8325-3a4ab5a03490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "107/107 - 0s - loss: 0.2860 - val_loss: 0.3706\n",
      "Epoch 2/100\n",
      "107/107 - 0s - loss: 0.2262 - val_loss: 0.3403\n",
      "Epoch 3/100\n",
      "107/107 - 0s - loss: 0.1636 - val_loss: 0.2038\n",
      "Epoch 4/100\n",
      "107/107 - 0s - loss: 0.1212 - val_loss: 0.1789\n",
      "Epoch 5/100\n",
      "107/107 - 0s - loss: 0.1054 - val_loss: 0.1621\n",
      "Epoch 6/100\n",
      "107/107 - 0s - loss: 0.0945 - val_loss: 0.1363\n",
      "Epoch 7/100\n",
      "107/107 - 0s - loss: 0.0857 - val_loss: 0.1301\n",
      "Epoch 8/100\n",
      "107/107 - 0s - loss: 0.0779 - val_loss: 0.1106\n",
      "Epoch 9/100\n",
      "107/107 - 0s - loss: 0.0697 - val_loss: 0.0834\n",
      "Epoch 10/100\n",
      "107/107 - 0s - loss: 0.0609 - val_loss: 0.0800\n",
      "Epoch 11/100\n",
      "107/107 - 0s - loss: 0.0518 - val_loss: 0.0662\n",
      "Epoch 12/100\n",
      "107/107 - 0s - loss: 0.0423 - val_loss: 0.0575\n",
      "Epoch 13/100\n",
      "107/107 - 0s - loss: 0.0335 - val_loss: 0.0336\n",
      "Epoch 14/100\n",
      "107/107 - 0s - loss: 0.0262 - val_loss: 0.0276\n",
      "Epoch 15/100\n",
      "107/107 - 0s - loss: 0.0212 - val_loss: 0.0212\n",
      "Epoch 16/100\n",
      "107/107 - 0s - loss: 0.0172 - val_loss: 0.0145\n",
      "Epoch 17/100\n",
      "107/107 - 0s - loss: 0.0146 - val_loss: 0.0122\n",
      "Epoch 18/100\n",
      "107/107 - 0s - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 19/100\n",
      "107/107 - 0s - loss: 0.0106 - val_loss: 0.0093\n",
      "Epoch 20/100\n",
      "107/107 - 0s - loss: 0.0092 - val_loss: 0.0070\n",
      "Epoch 21/100\n",
      "107/107 - 0s - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 22/100\n",
      "107/107 - 0s - loss: 0.0065 - val_loss: 0.0044\n",
      "Epoch 23/100\n",
      "107/107 - 0s - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 24/100\n",
      "107/107 - 0s - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 25/100\n",
      "107/107 - 0s - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 26/100\n",
      "107/107 - 0s - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 27/100\n",
      "107/107 - 0s - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 28/100\n",
      "107/107 - 0s - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 29/100\n",
      "107/107 - 0s - loss: 9.8499e-04 - val_loss: 0.0027\n",
      "Epoch 30/100\n",
      "107/107 - 0s - loss: 8.2395e-04 - val_loss: 0.0026\n",
      "Epoch 31/100\n",
      "107/107 - 0s - loss: 7.0005e-04 - val_loss: 0.0023\n",
      "Epoch 32/100\n",
      "107/107 - 0s - loss: 6.0439e-04 - val_loss: 0.0019\n",
      "Epoch 33/100\n",
      "107/107 - 0s - loss: 4.8947e-04 - val_loss: 0.0018\n",
      "Epoch 34/100\n",
      "107/107 - 0s - loss: 3.8445e-04 - val_loss: 0.0017\n",
      "Epoch 35/100\n",
      "107/107 - 0s - loss: 3.6204e-04 - val_loss: 0.0016\n",
      "Epoch 36/100\n",
      "107/107 - 0s - loss: 2.7798e-04 - val_loss: 0.0015\n",
      "Epoch 37/100\n",
      "107/107 - 0s - loss: 2.4156e-04 - val_loss: 0.0014\n",
      "Epoch 38/100\n",
      "107/107 - 0s - loss: 2.0910e-04 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      "107/107 - 0s - loss: 1.8811e-04 - val_loss: 0.0013\n",
      "Epoch 40/100\n",
      "107/107 - 0s - loss: 1.6723e-04 - val_loss: 0.0013\n",
      "Epoch 41/100\n",
      "107/107 - 0s - loss: 1.5738e-04 - val_loss: 0.0012\n",
      "Epoch 42/100\n",
      "107/107 - 0s - loss: 1.3684e-04 - val_loss: 0.0012\n",
      "Epoch 43/100\n",
      "107/107 - 0s - loss: 1.2115e-04 - val_loss: 0.0011\n",
      "Epoch 44/100\n",
      "107/107 - 0s - loss: 1.1058e-04 - val_loss: 0.0011\n",
      "Epoch 45/100\n",
      "107/107 - 0s - loss: 1.0045e-04 - val_loss: 0.0011\n",
      "Epoch 46/100\n",
      "107/107 - 0s - loss: 9.2075e-05 - val_loss: 0.0011\n",
      "Epoch 47/100\n",
      "107/107 - 0s - loss: 8.4109e-05 - val_loss: 0.0011\n",
      "Epoch 48/100\n",
      "107/107 - 0s - loss: 7.7254e-05 - val_loss: 0.0011\n",
      "Epoch 49/100\n",
      "107/107 - 0s - loss: 7.0380e-05 - val_loss: 9.4331e-04\n",
      "Epoch 50/100\n",
      "107/107 - 0s - loss: 6.5415e-05 - val_loss: 9.5525e-04\n",
      "Epoch 51/100\n",
      "107/107 - 0s - loss: 5.9416e-05 - val_loss: 9.3346e-04\n",
      "Epoch 52/100\n",
      "107/107 - 0s - loss: 5.4911e-05 - val_loss: 8.7304e-04\n",
      "Epoch 53/100\n",
      "107/107 - 0s - loss: 5.3269e-05 - val_loss: 8.5568e-04\n",
      "Epoch 54/100\n",
      "107/107 - 0s - loss: 4.6689e-05 - val_loss: 8.5348e-04\n",
      "Epoch 55/100\n",
      "107/107 - 0s - loss: 4.2417e-05 - val_loss: 7.8012e-04\n",
      "Epoch 56/100\n",
      "107/107 - 0s - loss: 4.1149e-05 - val_loss: 7.9966e-04\n",
      "Epoch 57/100\n",
      "107/107 - 0s - loss: 3.8852e-05 - val_loss: 8.0349e-04\n",
      "Epoch 58/100\n",
      "107/107 - 0s - loss: 3.6167e-05 - val_loss: 7.7679e-04\n",
      "Epoch 59/100\n",
      "107/107 - 0s - loss: 3.2436e-05 - val_loss: 7.7355e-04\n",
      "Epoch 60/100\n",
      "107/107 - 0s - loss: 3.0595e-05 - val_loss: 7.3524e-04\n",
      "Epoch 61/100\n",
      "107/107 - 0s - loss: 2.9332e-05 - val_loss: 7.2461e-04\n",
      "Epoch 62/100\n",
      "107/107 - 0s - loss: 2.7704e-05 - val_loss: 6.8547e-04\n",
      "Epoch 63/100\n",
      "107/107 - 0s - loss: 2.5389e-05 - val_loss: 7.1886e-04\n",
      "Epoch 64/100\n",
      "107/107 - 0s - loss: 2.3077e-05 - val_loss: 6.9539e-04\n",
      "Epoch 65/100\n",
      "107/107 - 0s - loss: 2.1176e-05 - val_loss: 6.7946e-04\n",
      "Epoch 66/100\n",
      "107/107 - 0s - loss: 2.0337e-05 - val_loss: 6.7743e-04\n",
      "Epoch 67/100\n",
      "107/107 - 0s - loss: 1.9284e-05 - val_loss: 6.4063e-04\n",
      "Epoch 68/100\n",
      "107/107 - 0s - loss: 1.7339e-05 - val_loss: 6.5214e-04\n",
      "Epoch 69/100\n",
      "107/107 - 0s - loss: 1.6785e-05 - val_loss: 6.2991e-04\n",
      "Epoch 70/100\n",
      "107/107 - 0s - loss: 1.6264e-05 - val_loss: 6.1209e-04\n",
      "Epoch 71/100\n",
      "107/107 - 0s - loss: 1.5375e-05 - val_loss: 6.0822e-04\n",
      "Epoch 72/100\n",
      "107/107 - 0s - loss: 1.4708e-05 - val_loss: 6.2466e-04\n",
      "Epoch 73/100\n",
      "107/107 - 0s - loss: 1.3252e-05 - val_loss: 6.4375e-04\n",
      "Epoch 74/100\n",
      "107/107 - 0s - loss: 1.2130e-05 - val_loss: 6.1547e-04\n",
      "Epoch 75/100\n",
      "107/107 - 0s - loss: 1.1382e-05 - val_loss: 5.9182e-04\n",
      "Epoch 76/100\n",
      "107/107 - 0s - loss: 1.0749e-05 - val_loss: 5.7741e-04\n",
      "Epoch 77/100\n",
      "107/107 - 0s - loss: 9.7916e-06 - val_loss: 5.8137e-04\n",
      "Epoch 78/100\n",
      "107/107 - 0s - loss: 9.7739e-06 - val_loss: 5.9355e-04\n",
      "Epoch 79/100\n",
      "107/107 - 0s - loss: 8.4226e-06 - val_loss: 5.8484e-04\n",
      "Epoch 80/100\n",
      "107/107 - 0s - loss: 7.8118e-06 - val_loss: 5.7716e-04\n",
      "Epoch 81/100\n",
      "107/107 - 0s - loss: 8.2480e-06 - val_loss: 6.2570e-04\n",
      "Epoch 82/100\n",
      "107/107 - 0s - loss: 8.0747e-06 - val_loss: 5.2807e-04\n",
      "Epoch 83/100\n",
      "107/107 - 0s - loss: 6.7752e-06 - val_loss: 5.3889e-04\n",
      "Epoch 84/100\n",
      "107/107 - 0s - loss: 7.4340e-06 - val_loss: 5.5708e-04\n",
      "Epoch 85/100\n",
      "107/107 - 0s - loss: 6.0641e-06 - val_loss: 6.4401e-04\n",
      "Epoch 86/100\n",
      "107/107 - 0s - loss: 6.4172e-06 - val_loss: 5.8849e-04\n",
      "Epoch 87/100\n",
      "107/107 - 0s - loss: 5.1067e-06 - val_loss: 5.2043e-04\n",
      "Epoch 88/100\n",
      "107/107 - 0s - loss: 8.7753e-06 - val_loss: 4.5084e-04\n",
      "Epoch 89/100\n",
      "107/107 - 0s - loss: 1.2693e-05 - val_loss: 5.9333e-04\n",
      "Epoch 90/100\n",
      "107/107 - 0s - loss: 6.3095e-06 - val_loss: 5.5728e-04\n",
      "Epoch 91/100\n",
      "107/107 - 0s - loss: 4.5548e-06 - val_loss: 5.5771e-04\n",
      "Epoch 92/100\n",
      "107/107 - 0s - loss: 4.2529e-06 - val_loss: 5.8139e-04\n",
      "Epoch 93/100\n",
      "107/107 - 0s - loss: 4.2785e-06 - val_loss: 5.2379e-04\n",
      "Epoch 94/100\n",
      "107/107 - 0s - loss: 3.6710e-06 - val_loss: 5.5028e-04\n",
      "Epoch 95/100\n",
      "107/107 - 0s - loss: 3.4875e-06 - val_loss: 5.4342e-04\n",
      "Epoch 96/100\n",
      "107/107 - 0s - loss: 3.9116e-06 - val_loss: 5.6374e-04\n",
      "Epoch 97/100\n",
      "107/107 - 0s - loss: 3.0426e-06 - val_loss: 5.3528e-04\n",
      "Epoch 98/100\n",
      "107/107 - 0s - loss: 2.9272e-06 - val_loss: 5.0210e-04\n",
      "Epoch 99/100\n",
      "107/107 - 0s - loss: 3.7221e-06 - val_loss: 5.3808e-04\n",
      "Epoch 100/100\n",
      "107/107 - 0s - loss: 2.9341e-06 - val_loss: 5.6206e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19cc3fb7c40>"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, target, batch_size=150, epochs=100, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex-LK6Kr1Rtj"
   },
   "source": [
    "## Bonus\n",
    "```Now implement the attention mechanism the authors suggest as a keras layer. Use the source code of the keras.layers.recurrent class. You can find the paper and the class source code in the current directory.```\n",
    "\n",
    "```Basic instructions:```\n",
    "- ```Use your tutor. A lot. This is a hard exercise.```\n",
    "- ```Open the source code of recurrent neural networks. You would like to implement a layer that inherits from Recurrent.```\n",
    "- ```Understand the code's flow and the functions you would like to write.```\n",
    "- ```Start by writing a mechanism that would be a little bit simpler: don't return a sequence, but rather return a single vector.```\n",
    "- ```Try solving the above problem using your attention mechanism. What problems do you encouter?```\n",
    "- ```Complete the full mechanism. Assuming Yoshua Bengio didn't lie in his paper, how do you think their architecture overcomes the problem you found?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stWyjNdB1Rtj",
    "outputId": "0cb5e4c6-1d2d-42c3-8d4e-59a3a9a86676"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Recurrent' from 'keras.layers.recurrent' (C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\keras\\layers\\recurrent.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-d641f4166595>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRecurrent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Recurrent' from 'keras.layers.recurrent' (C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\keras\\layers\\recurrent.py)"
     ]
    }
   ],
   "source": [
    "from keras.layers.recurrent import Recurrent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "BUFFER_SIZE = 50\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, target)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x000001D1E3B1CF70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function train_step at 0x000001D1E3B1CF70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-379-d84b3d8301ef>:14 call  *\n        output, state = self.gru(x, initial_state = hidden)\n    C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:720 __call__  **\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:176 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer gru_1 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [200, 25, 1, 16]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-388-7c7053c4079e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-379-d84b3d8301ef>:14 call  *\n        output, state = self.gru(x, initial_state = hidden)\n    C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:720 __call__  **\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:176 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer gru_1 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [200, 25, 1, 16]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "steps_per_epoch = len(data)//BATCH_SIZE\n",
    "\n",
    "encoder = Encoder(vocab_size=7, embedding_dim=16, enc_units=20, batch_sz=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Custom layers exercise.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
