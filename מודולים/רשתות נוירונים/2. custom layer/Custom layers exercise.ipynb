{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras- writing custom layers\n",
    "```Here you will experience with writing custom keras layers. We will have two stages: in the first stage we will implement a simple layer. In the second you will implement a more complicated layer.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "```Implement an unpooling layer, that acts on matrices as follow:```\n",
    "```\n",
    "A = array([[0, 1, 3, 1, 0],\n",
    "           [2, 0, 1, 2, 4],\n",
    "           [3, 2, 1, 4, 3],\n",
    "           [4, 0, 3, 2, 0],\n",
    "           [4, 1, 2, 0, 2]])\n",
    "       \n",
    "unpooling(A) = array([[0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [0, 0, 1, 1, 3, 3, 1, 1, 0, 0],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [2, 2, 0, 0, 1, 1, 2, 2, 4, 4],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [3, 3, 2, 2, 1, 1, 4, 4, 3, 3],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 0, 0, 3, 3, 2, 2, 0, 0],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2],\n",
    "                      [4, 4, 1, 1, 2, 2, 0, 0, 2, 2]])\n",
    "```\n",
    "```Use the following example to do so, which is taken from https://keras.io/layers/writing-your-own-keras-layers/.```\n",
    "\n",
    "```Note: you can't use numpy's functions in your layer's logic. You will have to use functions that are accessed through the backend you use (Theano or Tensorflow).```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='weight_variable_name', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unpooling(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Unpooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Unpooling, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        repeat_first_axis = K.repeat_elements(x,2,1)\n",
    "        repeat_scond_axis = K.repeat_elements(repeat_first_axis,2,2)\n",
    "        return repeat_scond_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n",
      "tf.Tensor(\n",
      "[[[ 1  1  2  2  3  3]\n",
      "  [ 1  1  2  2  3  3]\n",
      "  [ 0  0  5  5 10 10]\n",
      "  [ 0  0  5  5 10 10]]], shape=(1, 4, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 1\n",
    "x = tf.convert_to_tensor(np.array([[[1,2,3], [0, 5, 10]]]))\n",
    "print(x.shape)\n",
    "unpool_layer = Unpooling()\n",
    "y = unpool_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2\n",
    "```Consider the following simple attention mechanism:```\n",
    "\n",
    "```Given a vector compute Dense(v), while Dense(v).shape = v.shape\n",
    "Multiply v and Dense(v) element-wise\n",
    "Return the result```\n",
    "\n",
    "```What is the purpose of this mechanism? Can you think what can be achieved using this kind of mechanism?```\n",
    "\n",
    "```Implement the attention mechanism as a keras layer.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionMechanism, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[0], input_shape[1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(AttentionMechanism, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        print(\"x\", x.shape)\n",
    "        print(\"kernel\", self.kernel.shape)\n",
    "        return tf.keras.layers.Multiply()([x, self.kernel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "x (2, 3)\n",
      "kernel (2, 3)\n",
      "tf.Tensor(\n",
      "[[ 0.00923688 -0.06304872  0.05575053]\n",
      " [ 0.          0.20001902 -0.136467  ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "x = tf.convert_to_tensor(np.array([[1,2,3], [0, 5, 10]]), dtype='float32')\n",
    "print(x.shape)\n",
    "AttentionMechanism_layer = AttentionMechanism()\n",
    "y = AttentionMechanism_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3\n",
    "```Here you will try solving a problem I once  struggled with. The problem is the following:\n",
    "You are given a set of sequences of symbols. All sequences contain the same \"core sequence\", but have extra noise in the form of other symbols between the symbols of the core sequence. For example, the sequences could be```\n",
    "\n",
    "\n",
    "**1**-**3**-2-**4**-3-**2**-4-**1**-3-2-4\n",
    "\n",
    "**1**-2-**3**-3-**4**-1-2-**2**-**1**-3-4-2-1-1\n",
    "\n",
    "**1**-4-4-4-**3**-**4**-1-1-**2**-**1**-1-2\n",
    "\n",
    "```while the core sequence is 1-3-4-2-1```\n",
    "```Your task is, given a dataset of such sequences, to find the core sequence. You may speak to me to learn about the context of this question and the reasons led to facing it.```\n",
    "\n",
    "```Generate a dataset that will simulate this problem. Follow the instructions:```\n",
    "- ```Use a 4-letter alphabet.```\n",
    "- ```Generate a core sequence with 10 symbols.```\n",
    "- ```Create a new sequence symbol by symbol: for each symbol you add to the sequence, put the next letter of the sequence with probability p and a random symbol with a probability 1-p. choose p to be 0.5.```\n",
    "- ```Generate a 10,000 examples dataset.```\n",
    "\n",
    "```Try solving the problem with simple means.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3]\n"
     ]
    }
   ],
   "source": [
    "digits = [0,1,2,3]\n",
    "baseline_length = 3\n",
    "baseline = np.random.choice(digits, baseline_length)\n",
    "\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(baseline_in, p=0.5):\n",
    "    baseline_length_in = len(baseline_in)\n",
    "    baseline_places = []\n",
    "    seq = [baseline_in[0]]\n",
    "    count = 1\n",
    "    while count<baseline_length_in:\n",
    "        if np.random.random()<p:\n",
    "            seq.append(np.random.choice(digits, 1)[0])\n",
    "        else:\n",
    "            baseline_places.append(len(seq))\n",
    "            seq.append(baseline_in[count])\n",
    "            count += 1\n",
    "    return seq, baseline_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(count, baseline_in):\n",
    "    return map(lambda x: create_seq(baseline_in), range(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(create_sequences(10000, baseline))\n",
    "data, places = list(map(lambda x: x[0], data)), list(map(lambda x: x[1], data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(2, 0, 2), (0, 1, 0), (2, 2, 2), (2, 1, 0), (0, 1, 3), (0, 3, 0), (2, 1, 3), (0, 3, 3), (3, 2, 1), (1, 2, 2), (1, 3, 3), (3, 1, 2), (1, 3, 0), (3, 3, 2), (0, 0, 1), (2, 3, 0), (0, 2, 1), (1, 0, 1), (2, 3, 3), (1, 1, 0), (3, 0, 3), (1, 1, 3), (3, 0, 0), (2, 0, 1), (2, 1, 2), (2, 2, 1), (0, 1, 2), (0, 3, 2), (3, 1, 1), (3, 2, 0), (1, 2, 1), (3, 0, 1), (1, 3, 2), (3, 2, 3), (3, 3, 1), (0, 0, 3), (0, 2, 0), (0, 0, 0), (2, 3, 2), (0, 2, 3), (1, 1, 2), (1, 0, 0), (3, 0, 2), (1, 0, 3), (2, 0, 0), (2, 0, 3), (2, 2, 0), (0, 1, 1), (2, 2, 3), (2, 1, 1), (0, 3, 1), (1, 2, 0), (3, 1, 3), (1, 3, 1), (1, 2, 3), (3, 1, 0), (3, 2, 2), (3, 3, 3), (3, 3, 0), (0, 0, 2), (2, 3, 1), (0, 2, 2), (1, 0, 2), (1, 1, 1)}\n"
     ]
    }
   ],
   "source": [
    "subsequences_set = set()\n",
    "\n",
    "for letter1 in digits:\n",
    "    for letter2 in digits:\n",
    "        for letter3 in digits:\n",
    "            subsequences_set.add((letter1, letter2, letter3))\n",
    "\n",
    "print(subsequences_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subsequence(sequence, subsequence):\n",
    "    idx = 0\n",
    "    for char in sequence:\n",
    "        if char == subsequence[idx]:\n",
    "            idx += 1\n",
    "        if idx == len(subsequence):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 64/64 [00:00<00:00, 4009.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "subsequences_set_copy = subsequences_set.copy()\n",
    "for subsequence in tqdm(subsequences_set):\n",
    "    for sequence in data:\n",
    "        if find_subsequence(sequence, subsequence) is False:\n",
    "            subsequences_set_copy.remove(subsequence)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(subsequences_set_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4\n",
    "```A possible solution for the problem could be done as follow:```\n",
    "- ```Given a dataset of sequences as such, generate a new dataset of random sequences.```\n",
    "- ```Train a classifier that will determine whether a sequence belongs to the original dataset or the generated dataset. Make sure that this problem is solvable.```\n",
    "- ```Now train a specific model, containing an attention layer. We can hope that the attention mechanism will learn to use the core sequence when classifying.```\n",
    "- ```Use the attention visualization to find the symbols of the core sequence.```\n",
    "\n",
    "```What are the advantages of this solution? Do you think you can make it work? You certainly will need a different kind of attention mechanism for the task, rather than the simple one you already have.```\n",
    "\n",
    "```Read the paper Neural Machine Translation by Jointly Learning to Align and Translate by Bahanau, Cho and Bengio. The paper concerns with an attention mechanism implemented in the context of machine translation. Implement the attention mechanism the authors suggest using PyTorch and try solving the above problem. You can find the paper in the current directory.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(baseline_in, p=0.5, padding_length=21, padding_symbol=8):\n",
    "    baseline_length_in = len(baseline_in)\n",
    "    baseline_places = []\n",
    "    seq = [baseline_in[0]]\n",
    "    count = 1\n",
    "    while count<baseline_length_in:\n",
    "        if np.random.random()<p:\n",
    "            seq.append(np.random.choice(digits, 1)[0])\n",
    "        else:\n",
    "            baseline_places.append(len(seq))\n",
    "            seq.append(baseline_in[count])\n",
    "            count += 1\n",
    "            \n",
    "    if len(seq) < padding_length:\n",
    "        seq += [padding_symbol] * (padding_length-len(seq))\n",
    "    return seq, baseline_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(count, baseline_in):\n",
    "    return map(lambda x: create_seq(baseline_in), range(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3]\n",
      "[1 3 3]\n"
     ]
    }
   ],
   "source": [
    "digits = [0,1,2,3]\n",
    "baseline_length = 3\n",
    "right_baseline = np.random.choice(digits, baseline_length)\n",
    "wrong_baseline = np.random.choice(digits, baseline_length)\n",
    "\n",
    "print(right_baseline)\n",
    "print(wrong_baseline)\n",
    "\n",
    "data = list(create_sequences(10000, right_baseline)) + list(create_sequences(10000, wrong_baseline))\n",
    "data, places = list(map(lambda x: x[0], data)), list(map(lambda x: x[1], data))\n",
    "target = [True]*10000+[False]*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 21)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_layer_1 (Dense)       (None, 5)                 110       \n",
      "_________________________________________________________________\n",
      "hidden_layer_2 (Dense)       (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "hidden_layer_3 (Dense)       (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        Input(shape = (21,)),\n",
    "        Dense(5, name=\"hidden_layer_1\", activation='tanh'),\n",
    "        Dense(5, name=\"hidden_layer_2\", activation='tanh'),\n",
    "        Dense(5, name=\"hidden_layer_3\", activation='tanh'),\n",
    "        Dense(1, name=\"output_layer\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "107/107 - 0s - loss: 0.2860 - val_loss: 0.3706\n",
      "Epoch 2/100\n",
      "107/107 - 0s - loss: 0.2262 - val_loss: 0.3403\n",
      "Epoch 3/100\n",
      "107/107 - 0s - loss: 0.1636 - val_loss: 0.2038\n",
      "Epoch 4/100\n",
      "107/107 - 0s - loss: 0.1212 - val_loss: 0.1789\n",
      "Epoch 5/100\n",
      "107/107 - 0s - loss: 0.1054 - val_loss: 0.1621\n",
      "Epoch 6/100\n",
      "107/107 - 0s - loss: 0.0945 - val_loss: 0.1363\n",
      "Epoch 7/100\n",
      "107/107 - 0s - loss: 0.0857 - val_loss: 0.1301\n",
      "Epoch 8/100\n",
      "107/107 - 0s - loss: 0.0779 - val_loss: 0.1106\n",
      "Epoch 9/100\n",
      "107/107 - 0s - loss: 0.0697 - val_loss: 0.0834\n",
      "Epoch 10/100\n",
      "107/107 - 0s - loss: 0.0609 - val_loss: 0.0800\n",
      "Epoch 11/100\n",
      "107/107 - 0s - loss: 0.0518 - val_loss: 0.0662\n",
      "Epoch 12/100\n",
      "107/107 - 0s - loss: 0.0423 - val_loss: 0.0575\n",
      "Epoch 13/100\n",
      "107/107 - 0s - loss: 0.0335 - val_loss: 0.0336\n",
      "Epoch 14/100\n",
      "107/107 - 0s - loss: 0.0262 - val_loss: 0.0276\n",
      "Epoch 15/100\n",
      "107/107 - 0s - loss: 0.0212 - val_loss: 0.0212\n",
      "Epoch 16/100\n",
      "107/107 - 0s - loss: 0.0172 - val_loss: 0.0145\n",
      "Epoch 17/100\n",
      "107/107 - 0s - loss: 0.0146 - val_loss: 0.0122\n",
      "Epoch 18/100\n",
      "107/107 - 0s - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 19/100\n",
      "107/107 - 0s - loss: 0.0106 - val_loss: 0.0093\n",
      "Epoch 20/100\n",
      "107/107 - 0s - loss: 0.0092 - val_loss: 0.0070\n",
      "Epoch 21/100\n",
      "107/107 - 0s - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 22/100\n",
      "107/107 - 0s - loss: 0.0065 - val_loss: 0.0044\n",
      "Epoch 23/100\n",
      "107/107 - 0s - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 24/100\n",
      "107/107 - 0s - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 25/100\n",
      "107/107 - 0s - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 26/100\n",
      "107/107 - 0s - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 27/100\n",
      "107/107 - 0s - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 28/100\n",
      "107/107 - 0s - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 29/100\n",
      "107/107 - 0s - loss: 9.8499e-04 - val_loss: 0.0027\n",
      "Epoch 30/100\n",
      "107/107 - 0s - loss: 8.2395e-04 - val_loss: 0.0026\n",
      "Epoch 31/100\n",
      "107/107 - 0s - loss: 7.0005e-04 - val_loss: 0.0023\n",
      "Epoch 32/100\n",
      "107/107 - 0s - loss: 6.0439e-04 - val_loss: 0.0019\n",
      "Epoch 33/100\n",
      "107/107 - 0s - loss: 4.8947e-04 - val_loss: 0.0018\n",
      "Epoch 34/100\n",
      "107/107 - 0s - loss: 3.8445e-04 - val_loss: 0.0017\n",
      "Epoch 35/100\n",
      "107/107 - 0s - loss: 3.6204e-04 - val_loss: 0.0016\n",
      "Epoch 36/100\n",
      "107/107 - 0s - loss: 2.7798e-04 - val_loss: 0.0015\n",
      "Epoch 37/100\n",
      "107/107 - 0s - loss: 2.4156e-04 - val_loss: 0.0014\n",
      "Epoch 38/100\n",
      "107/107 - 0s - loss: 2.0910e-04 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      "107/107 - 0s - loss: 1.8811e-04 - val_loss: 0.0013\n",
      "Epoch 40/100\n",
      "107/107 - 0s - loss: 1.6723e-04 - val_loss: 0.0013\n",
      "Epoch 41/100\n",
      "107/107 - 0s - loss: 1.5738e-04 - val_loss: 0.0012\n",
      "Epoch 42/100\n",
      "107/107 - 0s - loss: 1.3684e-04 - val_loss: 0.0012\n",
      "Epoch 43/100\n",
      "107/107 - 0s - loss: 1.2115e-04 - val_loss: 0.0011\n",
      "Epoch 44/100\n",
      "107/107 - 0s - loss: 1.1058e-04 - val_loss: 0.0011\n",
      "Epoch 45/100\n",
      "107/107 - 0s - loss: 1.0045e-04 - val_loss: 0.0011\n",
      "Epoch 46/100\n",
      "107/107 - 0s - loss: 9.2075e-05 - val_loss: 0.0011\n",
      "Epoch 47/100\n",
      "107/107 - 0s - loss: 8.4109e-05 - val_loss: 0.0011\n",
      "Epoch 48/100\n",
      "107/107 - 0s - loss: 7.7254e-05 - val_loss: 0.0011\n",
      "Epoch 49/100\n",
      "107/107 - 0s - loss: 7.0380e-05 - val_loss: 9.4331e-04\n",
      "Epoch 50/100\n",
      "107/107 - 0s - loss: 6.5415e-05 - val_loss: 9.5525e-04\n",
      "Epoch 51/100\n",
      "107/107 - 0s - loss: 5.9416e-05 - val_loss: 9.3346e-04\n",
      "Epoch 52/100\n",
      "107/107 - 0s - loss: 5.4911e-05 - val_loss: 8.7304e-04\n",
      "Epoch 53/100\n",
      "107/107 - 0s - loss: 5.3269e-05 - val_loss: 8.5568e-04\n",
      "Epoch 54/100\n",
      "107/107 - 0s - loss: 4.6689e-05 - val_loss: 8.5348e-04\n",
      "Epoch 55/100\n",
      "107/107 - 0s - loss: 4.2417e-05 - val_loss: 7.8012e-04\n",
      "Epoch 56/100\n",
      "107/107 - 0s - loss: 4.1149e-05 - val_loss: 7.9966e-04\n",
      "Epoch 57/100\n",
      "107/107 - 0s - loss: 3.8852e-05 - val_loss: 8.0349e-04\n",
      "Epoch 58/100\n",
      "107/107 - 0s - loss: 3.6167e-05 - val_loss: 7.7679e-04\n",
      "Epoch 59/100\n",
      "107/107 - 0s - loss: 3.2436e-05 - val_loss: 7.7355e-04\n",
      "Epoch 60/100\n",
      "107/107 - 0s - loss: 3.0595e-05 - val_loss: 7.3524e-04\n",
      "Epoch 61/100\n",
      "107/107 - 0s - loss: 2.9332e-05 - val_loss: 7.2461e-04\n",
      "Epoch 62/100\n",
      "107/107 - 0s - loss: 2.7704e-05 - val_loss: 6.8547e-04\n",
      "Epoch 63/100\n",
      "107/107 - 0s - loss: 2.5389e-05 - val_loss: 7.1886e-04\n",
      "Epoch 64/100\n",
      "107/107 - 0s - loss: 2.3077e-05 - val_loss: 6.9539e-04\n",
      "Epoch 65/100\n",
      "107/107 - 0s - loss: 2.1176e-05 - val_loss: 6.7946e-04\n",
      "Epoch 66/100\n",
      "107/107 - 0s - loss: 2.0337e-05 - val_loss: 6.7743e-04\n",
      "Epoch 67/100\n",
      "107/107 - 0s - loss: 1.9284e-05 - val_loss: 6.4063e-04\n",
      "Epoch 68/100\n",
      "107/107 - 0s - loss: 1.7339e-05 - val_loss: 6.5214e-04\n",
      "Epoch 69/100\n",
      "107/107 - 0s - loss: 1.6785e-05 - val_loss: 6.2991e-04\n",
      "Epoch 70/100\n",
      "107/107 - 0s - loss: 1.6264e-05 - val_loss: 6.1209e-04\n",
      "Epoch 71/100\n",
      "107/107 - 0s - loss: 1.5375e-05 - val_loss: 6.0822e-04\n",
      "Epoch 72/100\n",
      "107/107 - 0s - loss: 1.4708e-05 - val_loss: 6.2466e-04\n",
      "Epoch 73/100\n",
      "107/107 - 0s - loss: 1.3252e-05 - val_loss: 6.4375e-04\n",
      "Epoch 74/100\n",
      "107/107 - 0s - loss: 1.2130e-05 - val_loss: 6.1547e-04\n",
      "Epoch 75/100\n",
      "107/107 - 0s - loss: 1.1382e-05 - val_loss: 5.9182e-04\n",
      "Epoch 76/100\n",
      "107/107 - 0s - loss: 1.0749e-05 - val_loss: 5.7741e-04\n",
      "Epoch 77/100\n",
      "107/107 - 0s - loss: 9.7916e-06 - val_loss: 5.8137e-04\n",
      "Epoch 78/100\n",
      "107/107 - 0s - loss: 9.7739e-06 - val_loss: 5.9355e-04\n",
      "Epoch 79/100\n",
      "107/107 - 0s - loss: 8.4226e-06 - val_loss: 5.8484e-04\n",
      "Epoch 80/100\n",
      "107/107 - 0s - loss: 7.8118e-06 - val_loss: 5.7716e-04\n",
      "Epoch 81/100\n",
      "107/107 - 0s - loss: 8.2480e-06 - val_loss: 6.2570e-04\n",
      "Epoch 82/100\n",
      "107/107 - 0s - loss: 8.0747e-06 - val_loss: 5.2807e-04\n",
      "Epoch 83/100\n",
      "107/107 - 0s - loss: 6.7752e-06 - val_loss: 5.3889e-04\n",
      "Epoch 84/100\n",
      "107/107 - 0s - loss: 7.4340e-06 - val_loss: 5.5708e-04\n",
      "Epoch 85/100\n",
      "107/107 - 0s - loss: 6.0641e-06 - val_loss: 6.4401e-04\n",
      "Epoch 86/100\n",
      "107/107 - 0s - loss: 6.4172e-06 - val_loss: 5.8849e-04\n",
      "Epoch 87/100\n",
      "107/107 - 0s - loss: 5.1067e-06 - val_loss: 5.2043e-04\n",
      "Epoch 88/100\n",
      "107/107 - 0s - loss: 8.7753e-06 - val_loss: 4.5084e-04\n",
      "Epoch 89/100\n",
      "107/107 - 0s - loss: 1.2693e-05 - val_loss: 5.9333e-04\n",
      "Epoch 90/100\n",
      "107/107 - 0s - loss: 6.3095e-06 - val_loss: 5.5728e-04\n",
      "Epoch 91/100\n",
      "107/107 - 0s - loss: 4.5548e-06 - val_loss: 5.5771e-04\n",
      "Epoch 92/100\n",
      "107/107 - 0s - loss: 4.2529e-06 - val_loss: 5.8139e-04\n",
      "Epoch 93/100\n",
      "107/107 - 0s - loss: 4.2785e-06 - val_loss: 5.2379e-04\n",
      "Epoch 94/100\n",
      "107/107 - 0s - loss: 3.6710e-06 - val_loss: 5.5028e-04\n",
      "Epoch 95/100\n",
      "107/107 - 0s - loss: 3.4875e-06 - val_loss: 5.4342e-04\n",
      "Epoch 96/100\n",
      "107/107 - 0s - loss: 3.9116e-06 - val_loss: 5.6374e-04\n",
      "Epoch 97/100\n",
      "107/107 - 0s - loss: 3.0426e-06 - val_loss: 5.3528e-04\n",
      "Epoch 98/100\n",
      "107/107 - 0s - loss: 2.9272e-06 - val_loss: 5.0210e-04\n",
      "Epoch 99/100\n",
      "107/107 - 0s - loss: 3.7221e-06 - val_loss: 5.3808e-04\n",
      "Epoch 100/100\n",
      "107/107 - 0s - loss: 2.9341e-06 - val_loss: 5.6206e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19cc3fb7c40>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, target, batch_size=150, epochs=100, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "```Now implement the attention mechanism the authors suggest as a keras layer. Use the source code of the keras.layers.recurrent class. You can find the paper and the class source code in the current directory.```\n",
    "\n",
    "```Basic instructions:```\n",
    "- ```Use your tutor. A lot. This is a hard exercise.```\n",
    "- ```Open the source code of recurrent neural networks. You would like to implement a layer that inherits from Recurrent.```\n",
    "- ```Understand the code's flow and the functions you would like to write.```\n",
    "- ```Start by writing a mechanism that would be a little bit simpler: don't return a sequence, but rather return a single vector.```\n",
    "- ```Try solving the above problem using your attention mechanism. What problems do you encouter?```\n",
    "- ```Complete the full mechanism. Assuming Yoshua Bengio didn't lie in his paper, how do you think their architecture overcomes the problem you found?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Recurrent' from 'keras.layers.recurrent' (C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\keras\\layers\\recurrent.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-d641f4166595>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRecurrent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Recurrent' from 'keras.layers.recurrent' (C:\\Users\\RONENAH\\Anaconda3\\envs\\formation_env\\lib\\site-packages\\keras\\layers\\recurrent.py)"
     ]
    }
   ],
   "source": [
    "from keras.layers.recurrent import Recurrent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
