{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will focus on some basic Adversarial Attack methods - Fast Gradient Sign Method (FGSM), Targeted Gradient Sign Method (TGSM), Basic Iterative Method (BIM) & Projected Gradient Descent (PGD). Before starting make sure that you feel comfortable with the basic concept of adversarial examples for classification models, and that you have a good understanding of the 4 methods. Your tutor may ask you questions about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQEZETPDG0rq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4g14QXanG0rx"
   },
   "source": [
    "--- Create and Train a Simple MNIST CNN Classifier --- for those of you less familiar with tensorflow and keras, this is an opportunity (notice that we are using the keras API from within tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwU6OYREG0rz"
   },
   "outputs": [],
   "source": [
    "''' Build a simple MNIST classification CNN\n",
    "    The network takes ~3 minutes to train on a normal laptop and reaches roughly 97% of accuracy\n",
    "    Model structure: Conv, Conv, Max pooling, Dropout, Dense, Dense\n",
    "'''\n",
    "def build_mnist_model():\n",
    "    \n",
    "    activation = 'relu'\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols, img_colors = 28, 28, 1\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(8, kernel_size=(3, 3), input_shape=(img_rows, img_cols, img_colors), activation=activation))\n",
    "    model.add(layers.Conv2D(8, (3, 3), activation=activation))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=activation))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes))\n",
    "    model.add(layers.Activation('softmax', name='y_pred'))\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzdMl1BRG0r3"
   },
   "outputs": [],
   "source": [
    "''' Normalize input to the range of [0..1]\n",
    "    Apart from assisting in the convergance of the training process, this \n",
    "    will also make our lives easier during the adversarial attack process\n",
    "'''\n",
    "def normalize(x_train,x_test):\n",
    "    x_train -= x_train.min()\n",
    "    x_train /= x_train.max()\n",
    "    x_test -= x_test.min()\n",
    "    x_test /= x_test.max()\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4HrIXRyG0r7"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the datasets for training\n",
    "num_classes = 10\n",
    "\n",
    "img_rows, img_cols, img_colors = 28, 28, 1\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "train_images, test_images = normalize(train_images, test_images)\n",
    "    \n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the classifier might take a few minutes to train but should reach quite a high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdGg0FJsG0r_"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = 128\n",
    "maxepoches = 12\n",
    "learning_rate = 0.1\n",
    "lr_decay = 1e-6\n",
    "lr_drop = 20\n",
    "\n",
    "# sess = tf.Session()\n",
    "# keras.backend.set_session(sess)\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "model = build_mnist_model()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=[keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "y_pred (Activation)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 149,538\n",
      "Trainable params: 149,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "469/469 [==============================] - 18s 37ms/step - loss: 0.9878 - categorical_accuracy: 0.6825 - val_loss: 0.3087 - val_categorical_accuracy: 0.9148\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 0.4624 - categorical_accuracy: 0.8559 - val_loss: 0.2427 - val_categorical_accuracy: 0.9311\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.3839 - categorical_accuracy: 0.8827 - val_loss: 0.2046 - val_categorical_accuracy: 0.9405\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.3340 - categorical_accuracy: 0.8988 - val_loss: 0.1718 - val_categorical_accuracy: 0.9492\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.2942 - categorical_accuracy: 0.9099 - val_loss: 0.1492 - val_categorical_accuracy: 0.9567\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 0.2618 - categorical_accuracy: 0.9205 - val_loss: 0.1337 - val_categorical_accuracy: 0.9605\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.2379 - categorical_accuracy: 0.9286 - val_loss: 0.1213 - val_categorical_accuracy: 0.9630\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.2236 - categorical_accuracy: 0.9336 - val_loss: 0.1118 - val_categorical_accuracy: 0.9659\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.2079 - categorical_accuracy: 0.9381 - val_loss: 0.1036 - val_categorical_accuracy: 0.9689\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.1959 - categorical_accuracy: 0.9400 - val_loss: 0.0961 - val_categorical_accuracy: 0.9711\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.1820 - categorical_accuracy: 0.9448 - val_loss: 0.0903 - val_categorical_accuracy: 0.9719\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.1722 - categorical_accuracy: 0.9496 - val_loss: 0.0865 - val_categorical_accuracy: 0.9730\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=maxepoches,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJhAkLskG0sE"
   },
   "outputs": [],
   "source": [
    "''' A simple utility funcion for evaluating the success of an attack\n",
    "'''\n",
    "def TestAttack(model, adv_images, orig_images, true_labels, target_labels=None, targeted=False):\n",
    "    adv_images = adv_images.numpy()\n",
    "    score = model.evaluate(adv_images, true_labels, verbose=0)\n",
    "    print('Test loss: {:.2f}'.format(score[0]))\n",
    "    print('Successfully moved out of source class: {:.2f}'.format( 1 - score[1]))\n",
    "    \n",
    "    if targeted:\n",
    "        score = model.evaluate(adv_images, target, verbose=0)\n",
    "        print('Test loss: {:.2f}'.format(score[0]))\n",
    "        print('Successfully perturbed to target class: {:.2f}'.format(score[1]))\n",
    "    \n",
    "    dist = np.mean(np.sqrt(np.mean(np.square(adv_images - orig_images), axis=(1,2,3))))\n",
    "    print('Mean perturbation distance: {:.2f}'.format(dist))\n",
    "    \n",
    "    index = 10\n",
    "    img = adv_images[index].reshape(28, 28)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u71zcvGqG0sC"
   },
   "source": [
    "--- Fast Gradient Sign Method (FGSM) --- here FGSM is implemented for you as an example, make sure you understand the code, in particular look at the functions keras.backend.function and keras.backend.gradients\n",
    "\n",
    "** the solution will also include a more efficient pure tensorflow implementation but you are welcome, to use this approach for your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UY-DDb8EG0sI"
   },
   "outputs": [],
   "source": [
    "''' Fast Gradient Sign Method implementation - perturb all input features by an epsilon sized step in \n",
    "    the direction of loss gradient\n",
    "'''\n",
    "def FastGradientSignMethod(model, images, labels, epsilon=0.3):\n",
    "    true_label_tensor = keras.backend.placeholder(shape=(None, num_classes))\n",
    "    adv_loss = keras.losses.categorical_crossentropy(true_label_tensor, model.output)\n",
    "    get_grads = keras.backend.function([model.input, true_label_tensor], \n",
    "                                       keras.backend.gradients(adv_loss, model.input))\n",
    "    \n",
    "    adv_grads = get_grads([images, labels])[0]\n",
    "    \n",
    "    adv_out = images + epsilon * np.sign(adv_grads)\n",
    "    return adv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FastGradientSignMethod(model, input_image, input_label, eps=0.3):\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = loss_object(input_label, prediction)\n",
    "\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adv_x = input_image + eps*signed_grad\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test out the attack, play around with the parameters and see how they influence the result visually and regarding the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.convert_to_tensor(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHfFFdSqG0sL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.29\n",
      "Successfully moved out of source class: 0.94\n",
      "Mean perturbation distance: 0.24\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "adv_images = FastGradientSignMethod(model, test_images, test_labels, eps=0.3)\n",
    "TestAttack(model, adv_images, test_images.numpy(), test_labels, targeted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jkg_fY7CG0sO"
   },
   "source": [
    "--- Targeted Gradient Sign Method (TGSM) --- implement this method and test out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YNckVZKG0sR"
   },
   "outputs": [],
   "source": [
    "''' Targeted Gradient Sign Method implementation - A targeted variant of the FGSM attack\n",
    "    here we minimize the loss with respect to the target class, as opposed to maximizing the loss with respect\n",
    "    to the source class\n",
    "'''\n",
    "def TargetedGradientSignMethod(model, input_image, input_label, eps=0.3):\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = loss_object(input_label, prediction)\n",
    "\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adv_out = input_image - eps*signed_grad\n",
    "    \n",
    "    return adv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZLThCUWG0sU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.89\n",
      "Successfully moved out of source class: 0.76\n",
      "Test loss: 3.63\n",
      "Successfully perturbed to target class: 0.48\n",
      "Mean perturbation distance: 0.20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = (np.argmax(test_labels, axis=1) + np.random.randint(1, num_classes, size=(test_labels.shape[0]))) % num_classes\n",
    "target = keras.utils.to_categorical(target, num_classes)\n",
    "adv_images = TargetedGradientSignMethod(model, test_images, target, eps=0.3)\n",
    "TestAttack(model, adv_images, test_images, test_labels, target, targeted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awH61CD3G0sY"
   },
   "source": [
    "--- Basic Iterative Method (BIM) --- implement this method which is the iterative upgrade to any of the two previous attacks, like before test the attack out and play around with the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abLag2V2G0sZ"
   },
   "outputs": [],
   "source": [
    "def BasicIterativeMethod(model, input_image, input_label, epsilon=0.1, iter_eps = 0.05, \n",
    "                         iterations=10, min_x=0.0, max_x=1.0, targeted=False):\n",
    "    \n",
    "    def step_targeted_attack(model, input_image, input_label, epsilon, min_x, max_x):\n",
    "        loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_image)\n",
    "            prediction = model(input_image)\n",
    "            loss = loss_object(input_label, prediction)\n",
    "\n",
    "        gradient = tape.gradient(loss, input_image)\n",
    "        signed_grad = tf.sign(gradient)\n",
    "        adv_x = input_image - epsilon*signed_grad\n",
    "        \n",
    "        adv_x = tf.clip_by_value(adv_x, min_x, max_x)\n",
    "        return tf.stop_gradient(adv_x)\n",
    "    \n",
    "\n",
    "    adv_out = input_image\n",
    "    for i in range(iterations):\n",
    "        adv_out = step_targeted_attack(model, adv_out, input_label, \n",
    "                                       iter_eps, min_x, max_x)\n",
    "        \n",
    "    return adv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g02Rq0o3G0sc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 17.71\n",
      "Successfully moved out of source class: 0.87\n",
      "Test loss: 2.50\n",
      "Successfully perturbed to target class: 0.87\n",
      "Mean perturbation distance: 0.34\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = (np.argmax(test_labels, axis=1) + \n",
    "          np.random.randint(1, num_classes, size=(test_labels.shape[0]))) % num_classes\n",
    "\n",
    "target = keras.utils.to_categorical(target, num_classes)\n",
    "adv_images = BasicIterativeMethod(model, test_images, target, \n",
    "                                  iterations = 30, epsilon=4.0, iter_eps = 0.05, targeted=True)\n",
    "TestAttack(model, adv_images, test_images, test_labels, target, targeted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProjectiveGradientDescent(model, input_image, input_label, iter_eps = 0.05,\n",
    "                              max_it=30, min_x=0.0, max_x=1.0, threshold=0.5):\n",
    "    \n",
    "    def step_targeted_attack(model, input_image, input_label, epsilon, min_x, max_x):\n",
    "        loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_image)\n",
    "            prediction = model(input_image)\n",
    "            loss = loss_object(input_label, prediction)\n",
    "\n",
    "        gradient = tape.gradient(loss, input_image)\n",
    "        signed_grad = tf.sign(gradient)\n",
    "        adv_x = input_image + epsilon*signed_grad\n",
    "        \n",
    "        adv_x = tf.clip_by_value(adv_x, min_x, max_x)\n",
    "        return tf.stop_gradient(adv_x)\n",
    "    \n",
    "    adv_in = input_image + np.random.normal(size=input_image.shape) / 10\n",
    "    adv_in = tf.clip_by_value(adv_in, min_x, max_x)\n",
    "    \n",
    "    for i in range(max_it):\n",
    "        adv_out = step_targeted_attack(model, adv_in, input_label, \n",
    "                                       iter_eps, min_x, max_x)\n",
    "        print('norm', np.linalg.norm(adv_out - adv_in))\n",
    "        if np.linalg.norm(adv_out - adv_in) < threshold:\n",
    "            break\n",
    "        adv_in = adv_out\n",
    "        \n",
    "    return adv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm 119.70964\n",
      "norm 120.64694\n",
      "norm 119.16983\n",
      "norm 119.78466\n",
      "norm 119.42634\n",
      "norm 119.78215\n",
      "norm 118.558525\n",
      "norm 117.32954\n",
      "norm 112.46203\n",
      "norm 103.25356\n",
      "norm 88.069565\n",
      "norm 68.16081\n",
      "norm 47.126644\n",
      "norm 29.255121\n",
      "norm 16.083473\n",
      "norm 9.0976715\n",
      "norm 4.5605597\n",
      "norm 2.7541957\n",
      "norm 1.5502679\n",
      "norm 1.0774893\n",
      "norm 0.0\n",
      "Test loss: 17.14\n",
      "Successfully moved out of source class: 1.00\n",
      "Mean perturbation distance: 0.31\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "adv_images = ProjectiveGradientDescent(model, test_images, test_labels)\n",
    "TestAttack(model, adv_images, test_images.numpy(), test_labels, targeted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Projected Gradient Descent (PGD) --- implement this improvement on the BIM basic variant, as usual test it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after you finish the exercise make sure you look at the solutions, they include the TensorFlow only implementation which is more computationally efficient (but more complex to write), and the Cleverhans implementation which is a well known library for Adversarial Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BIM Re-implementation Part I.ipynb",
   "provenance": [
    {
     "file_id": "1Dvp6LXt6xBbNgdHXLi8mqpz9CUqk08pP",
     "timestamp": 1585681256117
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
