{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Good morning, gent!\n",
    "The year is 1945. The British forces are about to wipe out the Nazis, once and for all.\n",
    "You are a young enthusiast meteorologist, and you are about to give the Great British Empire its victory!\n",
    "At your disposal is the Royal Dataset of Measurements from forecasting stations around the globe. Measurements are taken each morning and each noon. Since we are British, we don't believe in surprise attacks. Leave that rubbish for the barbarians. Instead, we spend each morning preparing for the battle, and attack the bloody Nazis by noon, like gentlemen. Hence, it is of utter importance that you can forecast the temperatures at noon each day.\n",
    "Are you ready for the mission? Are you ready to destroy the Nazis?```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Start by reading the Royal Dataset of Measurements. Also at your disposal is the data_weather_station_locations dataset. As a former meteorologist myself, I suggest you merge it into your measurements dataset.\n",
    "Split your data properly (remember that the British army wants to keep its time travel abilities in secret for a little longer). Split it such that you have 70% of the data in your training segment.\n",
    "Do you have missing values in your dataset? or any categorical values? deal with them with care.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I'm told that you need a definition for a loss function. well, I'm not such a technologist myself, but I can tell you that your loss should be linear in the absolute value of your error.\n",
    "A friend from the Royal Navy once told me, \"Aye stairt wi' a guid auld xgboost\". I trust this friend, and you should too. What results did you get?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Heavens in the sky, that's not so good. This bloody scot, I never trusted him.\n",
    "Let's try a different approach. Another friend, from the Royal Airforce, told me, \"When you are preplexed and can't find your arms and legs, try training a neural net and look carefully at its loss\".\n",
    "I trust this friend very much. You should try what he suggested. Can you learn something to explains our poor results from earlier? Don't forget - neural networks work better when first normalizing your data.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```As I suspected.. The wanker Nazis must have poisoned our dataset! Intel tells me that they could insert one sample at most. Find it and delete it from our Royal Dataset.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Blinding. Now train your xgboost and neural network again! What results can you get?\n",
    "Let me drop another tip for you: when you have an idea of the mean of the target you try to predict, you can initiate the last layer's bias to be this mean. It can make your network converge so much faster!```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```I've got a new message from Bletchley. Apperantly, our troops are very sensitive to cold weather, but to warm weather? not so much. We should update our loss function accordingly. They gave me this formula, that I myself can't read, but you might:```\n",
    "\n",
    "$\\frac {\\sum_i {|Pred_i-True_i|}}{\\sum_i {Pred_i}}$\n",
    "\n",
    "```Does it do what we want it to do? Are there any visible problems with this loss? List here all the problems that come to your mind, regarding this loss. Assuming you have only one sample in your dataset, draw the loss as a function of your prediction. Also, evaluate your former models using this metric.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```That loss function seems a bit suspicious. Let's have a closer look. Implement it (for batches - why can't you properly train a sgd on this loss?), and see if the network exhibits reasonal behavior.\n",
    "The British people take pride for their meticulous examinations of networks. What happens if the network decides, for some reason, to predict negative values? Try train a network that initializes to predict negative values (maybe change the last bias again). Can you understand the behavior of the loss during training?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Compare neural networks trained with this special loss, with MSE and with MAE. Which one achieves the best MAE loss? which one achieves the best special loss?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Iv'e just got an urgent message from the field marshal! He told me, \"Hoi, gent, my soldiers are cold! I have soldiers all around this damned globe, spread evenly, but you keep my boys worm only in a few damned countries!\". Do you think the field marchel is right? Why is that? Can you change the loss function to take this effect into account (bonus points if you can write it in LaTex)? Train a network using this loss.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Something here smells fhisy as a drunk french on sunday morning. Try comparing the network you trained using the new loss with the other networks you trained so far, using all the metrics we used so far. What results did you get? Can you explain your results?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Train the best model you can to answer for the field marchel's needs and win the war!```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
